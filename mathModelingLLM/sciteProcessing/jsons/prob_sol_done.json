[
  {
    "query": "TOPSIS",
    "title": "Rapid, Low-Cost Detection of Zika Virus Using Programmable Biomolecular Components",
    "doi": "10.1016/j.cell.2016.04.059",
    "problem": "The recent Zika virus outbreak demonstrates a critical need for low-cost, rapidly deployable diagnostic tools that can be used in pandemic regions. Existing molecular diagnostics face practical limitations for field deployment, especially in resource-limited settings. There is also a need for high specificity to distinguish Zika virus from closely related viruses like Dengue.",
    "solution": "The researchers developed a pipeline for the rapid design, assembly, and validation of cell-free, paper-based sensors capable of detecting Zika virus RNA. Their approach links isothermal RNA amplification to toehold switch RNA sensors, and incorporates a CRISPR/Cas9-based module for single-base resolution discrimination between viral strains. The freeze-dried biomolecular platform is field-ready, cost-effective, and addresses key limitations of current molecular diagnostics.",
    "year": 2016,
    "journal": "Cell",
    "citations": {
      "total": 1368,
      "supporting": 9,
      "contradicting": 1,
      "mentioning": 1356,
      "unclassified": 2,
      "citingPublications": 1287
    }
  },
  {
    "query": "TOPSIS",
    "title": "Measurements of the Higgs boson production and decay rates and constraints on its couplings from a combined ATLAS and CMS analysis of the LHC pp collision data at s = 7 $$ \\sqrt{s}=7 $$ and 8 TeV",
    "doi": "10.1007/jhep08(2016)045",
    "problem": "The problem addressed is the need for precise measurements of the Higgs boson production and decay rates, as well as constraints on its couplings to vector bosons and fermions, using data from the ATLAS and CMS experiments at the LHC. There is a need to combine results from multiple production processes and decay modes to test the consistency of these measurements with Standard Model predictions. Accurate determination of these parameters is essential for understanding the properties of the Higgs boson and searching for possible deviations indicating new physics.",
    "solution": "The solution proposed is a combined analysis of ATLAS and CMS measurements of the Higgs boson production and decay rates, using data collected at 7 and 8 TeV. The analysis incorporates five production processes and six decay modes, and applies three generic parameterisations based on cross sections, branching fractions, and coupling modifier ratios. The combined results are interpreted within the Standard Model framework and several model-dependent scenarios, providing improved precision and consistency checks with theoretical predictions.",
    "year": 2016,
    "journal": "Journal of High Energy Physics",
    "citations": {
      "total": 1269,
      "supporting": 65,
      "contradicting": 1,
      "mentioning": 1200,
      "unclassified": 3,
      "citingPublications": 1288
    }
  },
  {
    "query": "TOPSIS",
    "title": "Extension of <strong class=\"highlight\">TOPSIS</strong> to Multiple Criteria Decision Making with Pythagorean Fuzzy Sets",
    "doi": "10.1002/int.21676",
    "problem": "Managing uncertainty in real-world multicriteria decision-making problems is challenging, especially when traditional models like intuitionistic fuzzy sets are insufficient. There is a need for more robust methods to handle such uncertainty effectively within decision-making frameworks.",
    "solution": "The paper introduces novel operational laws for Pythagorean fuzzy sets (PFSs) and extends the TOPSIS method to work with PFSs. The approach involves defining a score function for comparison, identifying positive and negative ideal solutions, and calculating distances to these ideals using a new distance measure. A revised closeness measure is then used to determine the optimal alternative, and the method is demonstrated with a practical example.",
    "year": 2014,
    "journal": "International Journal of Intelligent Systems",
    "citations": {
      "total": 1110,
      "supporting": 5,
      "contradicting": 2,
      "mentioning": 1099,
      "unclassified": 4,
      "citingPublications": 1396
    }
  },
  {
    "query": "TOPSIS",
    "title": "Supply chain risk management: a literature review",
    "doi": "10.1080/00207543.2015.1030467",
    "problem": "Supply chains face a variety of uncertainties that necessitate effective risk management. Despite significant research in supply chain risk management (SCRM), there is a need for a comprehensive synthesis of the literature to better understand definitions, types, factors, and mitigation strategies related to supply chain risks. Additionally, identifying gaps in the existing literature is essential for advancing the field.",
    "solution": "The paper reviews and categorizes SCRM research from 2003 to 2013, providing a comprehensive synthesis of developments in risk definitions, types, factors, and management strategies. It also analyzes the literature to identify potential gaps, thereby offering a detailed overview and direction for future research in supply chain risk management.",
    "year": 2015,
    "journal": "International Journal of Production Research",
    "citations": {
      "total": 1042,
      "supporting": 12,
      "contradicting": 0,
      "mentioning": 959,
      "unclassified": 71,
      "citingPublications": 1178
    }
  },
  {
    "query": "TOPSIS",
    "title": "Combined Measurement of the Higgs Boson Mass inppCollisions ats=7and 8 TeV with the ATLAS and CMS Experiments",
    "doi": "10.1103/physrevlett.114.191803",
    "problem": "The precise mass of the Higgs boson, a fundamental parameter in the Standard Model, is not predicted by theory and must be measured experimentally. Previous measurements by the ATLAS and CMS collaborations using LHC Run 1 data have yielded results, but further precision is needed to improve predictions of other Higgs boson properties and to test the consistency of the Standard Model. There is a need to combine data from both experiments and key decay channels to achieve a more accurate measurement.",
    "solution": "The researchers present a combined measurement of the Higgs boson mass using data from both the ATLAS and CMS experiments at the CERN LHC, focusing on the H → γγ and H → ZZ → 4l decay channels. They perform a simultaneous fit to the reconstructed invariant mass peaks in these channels across both experiments, resulting in an improved and more precise value for the Higgs boson mass. This combined approach enhances the precision of the measurement and provides a foundation for further combined studies of other Higgs boson properties.",
    "year": 2015,
    "journal": "Physical Review Letters",
    "citations": {
      "total": 1040,
      "supporting": 39,
      "contradicting": 0,
      "mentioning": 992,
      "unclassified": 9,
      "citingPublications": 1376
    }
  },
  {
    "query": "GRA",
    "title": "Dye-Sensitized Solar Cells",
    "doi": "10.1021/cr900356p",
    "problem": "The research addresses the need for effective physical chemical characterization of mesoporous electrodes, which are crucial components in various optoelectronic devices, particularly dye-sensitized solar cells. Understanding and improving these materials is essential for enhancing device performance.",
    "solution": "The proposed solution involves conducting in-depth research and characterization of mesoporous electrodes to optimize their properties for use in dye-sensitized solar cells and other optoelectronic devices. This work is supported by extensive scientific publications and patent applications, contributing to advancements in the field.",
    "year": 2010,
    "journal": "Chemical Reviews",
    "citations": {
      "total": 7955,
      "supporting": 73,
      "contradicting": 8,
      "mentioning": 7778,
      "unclassified": 96,
      "citingPublications": 8692
    }
  },
  {
    "query": "GRA",
    "title": "Solar Water Splitting Cells",
    "doi": "10.1021/cr1002326",
    "problem": "The abstract discusses challenges in understanding and improving the electrical characteristics and catalytic properties of inorganic semiconductors, particularly when interfaced with conductive polymers and in the context of solar energy conversion and hydrogen evolution reactions. There is a need for efficient materials and architectures for solar energy conversion, storage, and photoelectrolysis, using earth-abundant elements.",
    "solution": "The researchers are investigating the use of molecular and inorganic semiconductors, porphyrin macrocycles, and mixtures of earth-abundant transition metals to enhance solar energy conversion and catalyze hydrogen evolution reactions. Their approach involves synthesizing and characterizing new material architectures and studying semiconductor-coupled heterogeneous catalysis to design more effective systems for solar energy applications.",
    "year": 2010,
    "journal": "Chemical Reviews",
    "citations": {
      "total": 7554,
      "supporting": 40,
      "contradicting": 2,
      "mentioning": 7489,
      "unclassified": 23,
      "citingPublications": 9000
    }
  },
  {
    "query": "GRA",
    "title": "Bounding the role of black carbon in the climate system: A scientific assessment",
    "doi": "10.1002/jgrd.50171",
    "problem": "Black carbon aerosol significantly impacts Earth's climate system, but its overall climate forcing is complex due to its unique physical properties and the variety of processes involved. There is uncertainty in quantifying the main forcing terms, such as direct solar absorption, cloud interactions, and deposition on snow and ice. Additionally, there is a need for accurate estimates of global black carbon emissions and their atmospheric effects.",
    "solution": "The assessment provides a comprehensive and quantitative evaluation of black-carbon climate forcing by including all known relevant processes and estimating best values and uncertainties for the main forcing terms. Climate models are used to calculate these effects, which are further validated with microphysical measurements and field observations when possible. The study also estimates global black carbon emissions using bottom-up inventory methods.",
    "year": 2013,
    "journal": "Journal of Geophysical Research Atmospheres",
    "citations": {
      "total": 5422,
      "supporting": 147,
      "contradicting": 20,
      "mentioning": 5212,
      "unclassified": 43,
      "citingPublications": 6266
    }
  },
  {
    "query": "GRA",
    "title": "Applications of ionic liquids in the chemical industry",
    "doi": "10.1039/b006677j",
    "problem": "There is a widely cited view that ionic liquids are only recently transitioning from academic research to industrial applications. This perspective overlooks the historical context and ongoing interactions between academia and industry regarding these materials.",
    "solution": "The authors critically review the history of ionic liquids, demonstrating that there have been parallel and collaborative exchanges between academic research and industrial developments since 1914. Their analysis challenges the prevailing narrative by providing evidence of long-standing academic-industrial collaboration.",
    "year": 2008,
    "journal": "Chemical Society Reviews",
    "citations": {
      "total": 3843,
      "supporting": 15,
      "contradicting": 0,
      "mentioning": 3782,
      "unclassified": 46,
      "citingPublications": 5382
    }
  },
  {
    "query": "GRA",
    "title": "Lithium metal anodes for rechargeable batteries",
    "doi": "10.1039/c3ee40795k",
    "problem": "The practical application of lithium metal anodes in rechargeable batteries has been hindered for decades by uncontrollable dendritic lithium growth and limited Coulombic efficiency during lithium deposition and stripping. These issues compromise the safety and efficiency of lithium metal batteries, which are critical for next-generation energy storage systems.",
    "solution": "The paper analyzes various factors influencing the morphology and Coulombic efficiency of lithium metal anodes, reviews technologies for characterizing lithium deposition, and discusses modeling results of lithium dendrite growth. It also highlights recent developments and the urgent need for further advancements in this field.",
    "year": 2014,
    "journal": "Energy & Environmental Science",
    "citations": {
      "total": 3367,
      "supporting": 24,
      "contradicting": 1,
      "mentioning": 3327,
      "unclassified": 15,
      "citingPublications": 4326
    }
  },
  {
    "query": "GRA",
    "title": "Cell Movement Is Guided by the Rigidity of the Substrate",
    "doi": "10.1016/s0006-3495(00)76279-5",
    "problem": "The mechanisms guiding directional cell locomotion in physiological processes such as morphogenesis, immune response, and wound healing are not fully understood, with most attention focused on chemical gradients. It remains unclear whether purely physical interactions at the cell-substrate interface can also direct cell movement.",
    "solution": "The study demonstrates that cell movement can be guided by physical cues such as substrate rigidity and mechanical strain. By introducing rigidity transitions and mechanical strains in flexible polyacrylamide sheets, the researchers show that cells exhibit 'durotaxis'—a preference for stiffer substrates—and that their movement can be directed by manipulating substrate mechanics.",
    "year": 2000,
    "journal": "Biophysical Journal",
    "citations": {
      "total": 3124,
      "supporting": 156,
      "contradicting": 17,
      "mentioning": 2938,
      "unclassified": 13,
      "citingPublications": 3133
    }
  },
  {
    "query": "AHP Weights",
    "title": "Global and regional diabetes prevalence estimates for 2019 and projections for 2030 and 2045: Results from the International Diabetes Federation Diabetes Atlas, 9th edition",
    "doi": "10.1016/j.diabres.2019.107843",
    "problem": "There is a need for accurate global estimates of diabetes prevalence and projections for future years, as well as an understanding of diabetes distribution across different regions and populations. Many countries lack high-quality in-country data on diabetes prevalence. This gap makes it challenging to assess the current and future burden of diabetes worldwide.",
    "solution": "The study identified 255 high-quality data sources from 138 countries and used extrapolation methods for countries lacking such data, matching them by economy, ethnicity, geography, and language. Logistic regression was applied to generate smoothed, age-specific prevalence estimates, including undiagnosed cases, for adults aged 20-79 years. This approach provided global estimates of diabetes prevalence for 2019 and projections for 2030 and 2045.",
    "year": 2019,
    "journal": "Diabetes Research and Clinical Practice",
    "citations": {
      "total": 6165,
      "supporting": 39,
      "contradicting": 10,
      "mentioning": 5923,
      "unclassified": 193,
      "citingPublications": 8885
    }
  },
  {
    "query": "AHP Weights",
    "title": "GABAergic Interneurons in the Neocortex: From Cellular Properties to Circuits",
    "doi": "10.1016/j.neuron.2016.06.033",
    "problem": "Cortical networks contain a minority population of highly heterogeneous GABAergic interneurons, whose diversity and specific roles in network dynamics are not fully understood. This complexity makes it challenging to distinguish among interneuron cell types and to comprehend their contributions to cortical computations.",
    "solution": "The authors review current knowledge on neocortical interneuron diversity and the distinguishing properties of different cell types. They discuss how the involvement of multiple interneuron types enhances the computational power of cortical circuits and highlight recent advances that clarify the mechanisms by which GABAergic inhibition shapes network operations.",
    "year": 2016,
    "journal": "Neuron",
    "citations": {
      "total": 2381,
      "supporting": 111,
      "contradicting": 6,
      "mentioning": 2243,
      "unclassified": 21,
      "citingPublications": 1958
    }
  },
  {
    "query": "AHP Weights",
    "title": "Metabotropic Glutamate Receptors: Physiology, Pharmacology, and Disease",
    "doi": "10.1146/annurev.pharmtox.011008.145533",
    "problem": "Metabotropic glutamate receptors (mGluRs) play a crucial role in modulating synaptic transmission and neuronal excitability in the central nervous system. Understanding the mechanisms of mGluR activation and their interactions with various proteins and ligands is essential, as these receptors are implicated in several neurological and psychiatric disorders. There is a need to explore their potential as drug targets for conditions such as Alzheimer's disease, Parkinson's disease, anxiety, depression, and schizophrenia.",
    "solution": "Recent research has made significant progress in elucidating the activation mechanisms of mGluRs, their protein interactions, and the effects of orthosteric and allosteric ligands on receptor activity. These advances support the continued development and validation of mGluR ligands as therapeutic agents for neurological and psychiatric disorders. The widespread expression and modulatory capacity of mGluRs highlight their therapeutic utility in treating these conditions.",
    "year": 2010,
    "journal": "The Annual Review of Pharmacology and Toxicology",
    "citations": {
      "total": 1715,
      "supporting": 26,
      "contradicting": 1,
      "mentioning": 1682,
      "unclassified": 6,
      "citingPublications": 1691
    }
  },
  {
    "query": "AHP Weights",
    "title": "Molecular Physiology of Low-Voltage-Activated T-type Calcium Channels",
    "doi": "10.1152/physrev.00018.2002",
    "problem": "T-type Ca2+ channels, also known as low-voltage-activated channels, play crucial roles in neuronal activity such as burst firing and intracellular signaling, which are implicated in conditions like absence epilepsy and thalamocortical dysrhythmias. Despite their importance, there is a need for a comprehensive understanding of their molecular diversity, distribution, regulation, and pharmacological properties. The existence of multiple Cav3 subtypes with distinct characteristics further complicates their study.",
    "solution": "This review aims to provide a comprehensive description of T-type currents, including their distribution, regulation, pharmacology, and molecular cloning. By summarizing the electrophysiological properties and expression patterns of Cav3 subtypes, the review seeks to clarify their roles and differences. The review also discusses how these channels can be differentiated from high-voltage-activated channels and from each other.",
    "year": 2003,
    "journal": "Physiological Reviews",
    "citations": {
      "total": 1583,
      "supporting": 67,
      "contradicting": 5,
      "mentioning": 1492,
      "unclassified": 19,
      "citingPublications": 1574
    }
  },
  {
    "query": "AHP Weights",
    "title": "Recombinant protein expression in Escherichia coli: advances and challenges",
    "doi": "10.3389/fmicb.2014.00172",
    "problem": "Efficient production of recombinant proteins is a significant challenge in biotechnology, and Escherichia coli is widely used as a host organism for this purpose. Despite its popularity, optimizing the synthesis of heterologous proteins in E. coli requires navigating a complex array of molecular tools, engineered strains, and cultivation strategies.",
    "solution": "The paper reviews various approaches for synthesizing recombinant proteins in E. coli, highlighting the latest advancements in the field. It discusses the available molecular tools, engineered strains, and cultivation protocols to guide researchers in achieving high-level protein production.",
    "year": 2014,
    "journal": "Frontiers in Microbiology",
    "citations": {
      "total": 1572,
      "supporting": 14,
      "contradicting": 1,
      "mentioning": 1508,
      "unclassified": 49,
      "citingPublications": 2227
    }
  },
  {
    "query": "AHP Weights",
    "title": "Multi-criteria decision making approaches for supplier evaluation and selection: A literature review",
    "doi": "10.1016/j.ejor.2009.05.009",
    "problem": "The supplier evaluation and selection process in supply chain management has traditionally focused on a single factor, such as cost, rather than considering multiple criteria. There is a need to understand which multi-criteria decision making approaches are most commonly used, which evaluation criteria are prioritized, and whether current approaches have any inadequacies. Addressing these questions is essential for improving supplier selection methods.",
    "solution": "The paper reviews and analyzes literature from international journals between 2000 and 2008 on multi-criteria decision making approaches for supplier evaluation and selection. It identifies prevalent approaches, commonly used evaluation criteria, and any inadequacies in existing methods, and provides recommendations for improvements and future research. This review supports the effectiveness of multi-criteria approaches over traditional cost-based methods and offers guidance for researchers and decision makers.",
    "year": 2010,
    "journal": "European Journal of Operational Research",
    "citations": {
      "total": 1153,
      "supporting": 19,
      "contradicting": 4,
      "mentioning": 1069,
      "unclassified": 61,
      "citingPublications": 1939
    }
  },
  {
    "query": "AHP Weights",
    "title": "Reactive oxygen and nitrogen intermediates in the relationship between mammalian hosts and microbial pathogens",
    "doi": "10.1073/pnas.97.16.8841",
    "problem": "The mechanisms by which reactive oxygen intermediates and reactive nitrogen intermediates contribute to mammalian immunity, and how microbes resist these defenses, are not fully understood. There is a paradox regarding the roles of phagocyte oxidase and inducible nitric oxide synthase, as they are both nonredundant and mutually redundant in host defense. Understanding the molecular basis of microbial resistance to reactive nitrogen intermediates, especially in diseases like tuberculosis, is necessary for a comprehensive view of host-pathogen interactions.",
    "solution": "Recent studies using knock-out mice have clarified the individual and combined roles of phagocyte oxidase and inducible nitric oxide synthase in immunity, showing their greater-than-expected contribution. The review highlights advances in identifying microbial genes responsible for resistance to reactive nitrogen intermediates through genetic and biochemical approaches. This molecular characterization of microbial defenses against RNI is analogous to existing knowledge about resistance to reactive oxygen intermediates.",
    "year": 2000,
    "journal": "Proceedings of the National Academy of Sciences",
    "citations": {
      "total": 1118,
      "supporting": 17,
      "contradicting": 0,
      "mentioning": 1077,
      "unclassified": 24,
      "citingPublications": 1358
    }
  },
  {
    "query": "AHP Weights",
    "title": "Supply chain risk management: a literature review",
    "doi": "10.1080/00207543.2015.1030467",
    "problem": "Effectively managing risks in supply chains is challenging due to a variety of uncertainties. Although much research has been conducted in supply chain risk management (SCRM), there is a need for a comprehensive synthesis of the literature to better understand definitions, types, factors, and mitigation strategies related to supply chain risks. Additionally, identifying gaps in the existing research is necessary for advancing the field.",
    "solution": "The paper reviews and categorizes SCRM research published between 2003 and 2013. It provides a detailed analysis of developments in risk definitions, types, factors, and management/mitigation strategies within supply chains. Furthermore, the paper explores and identifies potential gaps in the SCRM literature to guide future research.",
    "year": 2015,
    "journal": "International Journal of Production Research",
    "citations": {
      "total": 1042,
      "supporting": 12,
      "contradicting": 0,
      "mentioning": 959,
      "unclassified": 71,
      "citingPublications": 1178
    }
  },
  {
    "query": "AHP Weights",
    "title": "Therapeutic siRNA: state of the art",
    "doi": "10.1038/s41392-020-0207-x",
    "problem": "The development of siRNA therapeutics faces significant challenges, including efficient and safe delivery of siRNAs to target tissues and cells, as well as improving their activity, stability, specificity, and minimizing off-target effects. Overcoming these obstacles is critical for realizing the therapeutic potential of siRNA in treating human diseases.",
    "solution": "This review comprehensively examines the evolution of siRNA chemical modifications and their impact on biomedical performance. It discusses all clinically explored and commercialized siRNA delivery platforms, such as the GalNAc-siRNA conjugate, and their design principles, while summarizing the latest progress in siRNA therapeutic development to provide a roadmap for researchers in the field.",
    "year": 2020,
    "journal": "Signal Transduction and Targeted Therapy",
    "citations": {
      "total": 1040,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 1029,
      "unclassified": 11,
      "citingPublications": 1192
    }
  },
  {
    "query": "Entropic Weights",
    "title": "The Amber biomolecular simulation programs",
    "doi": "10.1002/jcc.20290",
    "problem": "There is a need for advanced computational tools to perform molecular dynamics and free energy calculations for biomolecules such as proteins, nucleic acids, and carbohydrates. Existing methods in the late 1970s were limited in their capabilities for assisted model building and energy refinement.",
    "solution": "The Amber package of computer programs was developed to address these needs, evolving from an early program for assisted model building with energy refinement. It now offers a suite of powerful tools for modern computational chemistry, with a focus on molecular dynamics and free energy calculations for complex biomolecular systems.",
    "year": 2005,
    "journal": "Journal of Computational Chemistry",
    "citations": {
      "total": 10638,
      "supporting": 33,
      "contradicting": 1,
      "mentioning": 10553,
      "unclassified": 51,
      "citingPublications": 10494
    }
  },
  {
    "query": "Entropic Weights",
    "title": "ff14SB: Improving the Accuracy of Protein Side Chain and Backbone Parameters from ff99SB",
    "doi": "10.1021/acs.jctc.5b00255",
    "problem": "While the Amber ff99SB force field improved protein secondary structure balance and dynamics compared to earlier versions, it still exhibited weaknesses in modeling side chain rotamer and backbone secondary structure preferences. Specifically, the side chain dihedral parameters had not been updated since ff94, and inaccuracies remained in the backbone dihedral parameters, affecting the accuracy of atomistic simulations. These shortcomings limited the force field's ability to accurately reproduce experimental data and quantum mechanical (QM) benchmarks.",
    "solution": "The researchers performed a complete refit of all amino acid side chain dihedral parameters using a training set of multidimensional dihedral scans to enhance parameter transferability. They also empirically adjusted the protein backbone dihedral parameters, particularly the φ and ψ angles, to better match NMR scalar coupling data and secondary structure content. These combined modifications, resulting in the new ff14SB force field, significantly improved agreement with QM data and experimental measurements, and enhanced the accuracy of protein secondary structure and side chain modeling.",
    "year": 2015,
    "journal": "Journal of Chemical Theory and Computation",
    "citations": {
      "total": 8572,
      "supporting": 32,
      "contradicting": 0,
      "mentioning": 8529,
      "unclassified": 11,
      "citingPublications": 9943
    }
  },
  {
    "query": "Entropic Weights",
    "title": "Automated docking using a Lamarckian genetic algorithm and an empirical binding free energy function",
    "doi": "10.1002/(sici)1096-987x(19981115)19:14<1639::aid-jcc10>3.0.co;2-b",
    "problem": "Predicting the bound conformations of flexible ligands to macromolecular targets is challenging, particularly when accounting for ligand flexibility and accurately estimating binding free energies. Existing automated docking methods struggle with efficiency and reliability, especially for ligands with many degrees of freedom. There is also a need for improved scoring functions to better estimate the free energy change upon binding.",
    "solution": "The authors developed a novel automated docking method that incorporates a new scoring function for estimating binding free energy and applies a Lamarckian genetic algorithm, where environmental adaptations are inherited. They compared this approach with Monte Carlo simulated annealing and traditional genetic algorithms, demonstrating that the Lamarckian genetic algorithm is the most efficient and reliable. The empirical free energy function was calibrated using linear regression on a set of protein-ligand complexes with known binding constants, resulting in a robust new energy model.",
    "year": 1998,
    "journal": "Journal of Computational Chemistry",
    "citations": {
      "total": 7888,
      "supporting": 43,
      "contradicting": 1,
      "mentioning": 7776,
      "unclassified": 68,
      "citingPublications": 10570
    }
  },
  {
    "query": "Entropic Weights",
    "title": "ViennaRNA Package 2.0",
    "doi": "10.1186/1748-7188-6-26",
    "problem": "RNA secondary structure prediction relies on thermodynamic models and dynamic programming algorithms, but advances in energy models, computational hardware, and algorithmic diversity have outpaced existing software tools. The widely used RNA secondary structure analysis package required updates to accommodate new energy parameters, multi-core processing, and expanded analysis capabilities.",
    "solution": "A major technical overhaul of the RNA secondary structure analysis package was performed, introducing new features such as expanded tools for RNA-RNA interactions, additional output formats, and support for new input types. The updates maintain computational efficiency and backward compatibility, while enabling concurrent computations and enhanced analysis options.",
    "year": 2011,
    "journal": "Algorithms for Molecular Biology",
    "citations": {
      "total": 4913,
      "supporting": 21,
      "contradicting": 0,
      "mentioning": 4885,
      "unclassified": 7,
      "citingPublications": 4694
    }
  },
  {
    "query": "Entropic Weights",
    "title": "Carbon Dioxide Capture in Metal–Organic Frameworks",
    "doi": "10.1021/cr2003272",
    "problem": "The increasing emission of carbon dioxide (CO2) from anthropogenic sources, particularly at stationary point sources, poses significant environmental challenges. Existing CO2 capture materials, such as aqueous alkanolamine absorbents and solid porous adsorbents, have limitations in terms of efficiency, selectivity, and stability. There is a need for improved materials and methods for effective CO2 capture and sequestration.",
    "solution": "The paper explores the use of metal-organic frameworks (MOFs) as advanced materials for CO2 capture due to their tunable synthesis, structural features, and physical properties. It discusses the adsorption capacity, selectivity, and stability of MOFs, as well as strategies for enhancing CO2/N2 selectivity through surface functionalization and the development of MOF-based membranes. Computational modeling and in situ characterization techniques are also employed to optimize MOFs for both post-combustion and pre-combustion CO2 capture applications.",
    "year": 2011,
    "journal": "Chemical Reviews",
    "citations": {
      "total": 4324,
      "supporting": 70,
      "contradicting": 3,
      "mentioning": 4227,
      "unclassified": 24,
      "citingPublications": 6117
    }
  },
  {
    "query": "Entropic Weights",
    "title": "Emerging applications of stimuli-responsive polymer materials",
    "doi": "10.1038/nmat2614",
    "problem": "There is a need for materials that can adapt to environmental changes and respond to various external stimuli for use in diverse applications such as drug delivery, diagnostics, and smart systems. However, challenges remain in developing and applying stimuli-responsive polymeric materials, particularly those self-assembled from nanostructured building blocks.",
    "solution": "The paper reviews recent advances in the development of stimuli-responsive polymeric materials that self-assemble from nanostructured building blocks. It also critically outlines emerging developments and addresses the challenges faced in moving these materials towards practical applications.",
    "year": 2010,
    "journal": "Nature Materials",
    "citations": {
      "total": 4166,
      "supporting": 13,
      "contradicting": 2,
      "mentioning": 4125,
      "unclassified": 26,
      "citingPublications": 5408
    }
  },
  {
    "query": "Entropic Weights",
    "title": "The Halogen Bond",
    "doi": "10.1021/acs.chemrev.5b00484",
    "problem": "There is a need to understand the nature, current status, and future directions of research on the halogen bond, a specific type of attractive interaction between molecular entities. Additionally, the potential applications and advantages of utilizing halogen bonds in various scientific fields are not fully explored.",
    "solution": "This review provides a comprehensive overview of the history, current research, and future prospects of the halogen bond. It demonstrates the specific advantages of designing systems based on halogen bonds across diverse fields such as material sciences, biomolecular recognition, and drug design.",
    "year": 2016,
    "journal": "Chemical Reviews",
    "citations": {
      "total": 3589,
      "supporting": 88,
      "contradicting": 2,
      "mentioning": 3461,
      "unclassified": 38,
      "citingPublications": 3518
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "BEDTools: a flexible suite of utilities for comparing genomic features",
    "doi": "10.1093/bioinformatics/btq033",
    "problem": "Testing for correlations between different sets of genomic features is fundamental in genomics research, but existing web-based methods struggle with the massive datasets produced by modern sequencing technologies. Efficient and flexible tools are needed to handle complex queries on these large datasets.",
    "solution": "The article introduces BEDTools, a new software suite for the efficient comparison, manipulation, and annotation of genomic features in BED, GFF, and BAM formats. BEDTools allows users to compare large datasets with public and custom genome annotation tracks, and can be combined with UNIX commands to facilitate routine genomics tasks and complex data analysis pipelines.",
    "year": 2010,
    "journal": "Bioinformatics",
    "citations": {
      "total": 23025,
      "supporting": 13,
      "contradicting": 2,
      "mentioning": 22991,
      "unclassified": 19,
      "citingPublications": 25525
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "A power primer.",
    "doi": "10.1037/0033-2909.112.1.155",
    "problem": "Statistical power analysis is often neglected in behavioral science research due to the inaccessibility or complexity of standard materials. This neglect is compounded by the lack of clear guidance in textbooks and curricula, making it difficult for researchers to determine the probability that their studies will yield statistically significant results.",
    "solution": "The paper provides a convenient, though not comprehensive, presentation of required sample sizes for achieving .80 power to detect small, medium, and large effects. It includes effect-size indexes, conventional values, and tables of necessary sample sizes for eight standard statistical tests, including the significance of a product-moment correlation, to make power analysis more accessible to researchers.",
    "year": 1992,
    "journal": "Psychological Bulletin",
    "citations": {
      "total": 21231,
      "supporting": 414,
      "contradicting": 71,
      "mentioning": 19919,
      "unclassified": 827,
      "citingPublications": 37823
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "A Guideline of Selecting and Reporting Intraclass <strong class=\"highlight\">Correlation</strong> Coefficients for Reliability Research",
    "doi": "10.1016/j.jcm.2016.02.012",
    "problem": "Researchers often use the intraclass correlation coefficient (ICC) as a reliability index in various analyses, but there are multiple forms of ICC, each with distinct assumptions and interpretations. This can lead to confusion and misreporting if the appropriate ICC form is not selected or clearly specified. There is a need for clear guidelines on choosing and reporting the correct ICC form in research.",
    "solution": "The article provides a practical guideline for clinical researchers to select the correct form of ICC based on their research design and to report ICC parameters transparently, including software, model, type, and definition. It also offers best practices for reporting ICC and advises readers on how to critically evaluate the use and reporting of ICC in scientific publications.",
    "year": 2016,
    "journal": "Journal of Chiropractic Medicine",
    "citations": {
      "total": 16865,
      "supporting": 288,
      "contradicting": 20,
      "mentioning": 16329,
      "unclassified": 228,
      "citingPublications": 21656
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "Maximum entropy modeling of species geographic distributions",
    "doi": "10.1016/j.ecolmodel.2005.03.026",
    "problem": "Predictive modeling of species' environmental requirements and geographic distributions is often hampered by the lack of absence data for most species. While detailed presence/absence data allow the use of standard statistical techniques, most species only have presence-only occurrence data available. This limits the effectiveness of traditional modeling approaches.",
    "solution": "The paper introduces the use of the maximum entropy method (Maxent) for modeling species geographic distributions using presence-only data. Maxent is a machine learning method that is well-suited for species distribution modeling and was shown, through a case study, to outperform a commonly used presence-only modeling method (GARP). The results indicate that Maxent provides better discrimination of suitable versus unsuitable areas and can be effectively used for many applications with presence-only datasets.",
    "year": 2006,
    "journal": "Ecological Modelling",
    "citations": {
      "total": 16098,
      "supporting": 39,
      "contradicting": 2,
      "mentioning": 15346,
      "unclassified": 711,
      "citingPublications": 15801
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "Comparing the Areas under Two or More <strong class=\"highlight\">Correlated</strong> Receiver Operating Characteristic Curves: A Nonparametric Approach",
    "doi": "10.2307/2531595",
    "problem": "As new diagnostic tests are developed, it is increasingly important to evaluate and compare their performance, especially when test results are measured on a continuous scale. When comparing two or more ROC curves derived from tests performed on the same individuals, the correlated nature of the data complicates statistical analysis of differences between the curves.",
    "solution": "This paper proposes a nonparametric approach for analyzing areas under correlated ROC curves. The method utilizes the theory of generalized U-statistics to generate an estimated covariance matrix, allowing for appropriate statistical analysis of the differences between correlated ROC curves.",
    "year": 1988,
    "journal": "Biometrics",
    "citations": {
      "total": 13881,
      "supporting": 66,
      "contradicting": 12,
      "mentioning": 13698,
      "unclassified": 105,
      "citingPublications": 19685
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets",
    "doi": "10.1093/nar/gky1131",
    "problem": "Understanding biological phenomena requires comprehensive knowledge of protein–protein interactions, but current information on these associations is incomplete and varies in annotation quality and reliability. There is a need for an integrated and reliable resource that consolidates diverse sources of protein interaction data.",
    "solution": "The STRING database addresses this by collecting, scoring, and integrating all publicly available protein–protein interaction information, supplementing it with computational predictions to create a comprehensive global network. The latest version expands organism coverage and introduces features such as genome-wide dataset uploads, network visualization, and gene-set enrichment analysis using established and novel classification systems.",
    "year": 2018,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 13336,
      "supporting": 61,
      "contradicting": 2,
      "mentioning": 13248,
      "unclassified": 25,
      "citingPublications": 15963
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "The Hallmarks of Aging",
    "doi": "10.1016/j.cell.2013.05.039",
    "problem": "Aging leads to a progressive loss of physiological integrity, resulting in impaired function, increased vulnerability to death, and is the primary risk factor for major diseases such as cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Despite advances in aging research, a major challenge remains in understanding the interconnectedness and relative contributions of various biological processes to aging. Identifying these connections is crucial for developing effective interventions.",
    "solution": "The review proposes a framework of nine tentative hallmarks that are common denominators of aging across different organisms, with a focus on mammals. By enumerating these hallmarks—such as genomic instability, telomere attrition, and mitochondrial dysfunction—the review aims to guide future research in dissecting their interactions and contributions. The ultimate goal is to identify pharmaceutical targets that can improve human health during aging with minimal side effects.",
    "year": 2013,
    "journal": "Cell",
    "citations": {
      "total": 12934,
      "supporting": 178,
      "contradicting": 13,
      "mentioning": 12532,
      "unclassified": 211,
      "citingPublications": 13279
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "The Structure and Function of Complex Networks",
    "doi": "10.1137/s003614450342480",
    "problem": "Researchers seek to understand and predict the behavior of complex networked systems such as the Internet, social networks, and biological networks. These systems exhibit intricate structural and dynamical properties that are not easily captured by traditional analytical methods.",
    "solution": "The paper reviews recent developments in modeling and analyzing networked systems, including concepts like the small-world effect, degree distributions, clustering, network correlations, and various random graph models. It also discusses models of network growth, preferential attachment, and the study of dynamical processes on networks.",
    "year": 2003,
    "journal": "Siam Review",
    "citations": {
      "total": 12904,
      "supporting": 148,
      "contradicting": 11,
      "mentioning": 12318,
      "unclassified": 427,
      "citingPublications": 17092
    }
  },
  {
    "query": "Pearson Correlation",
    "title": "Simple Combinations of Lineage-Determining Transcription Factors Prime cis-Regulatory Elements Required for Macrophage and B Cell Identities",
    "doi": "10.1016/j.molcel.2010.05.004",
    "problem": "Genome-scale studies have shown extensive, cell type-specific co-localization of transcription factors, but the mechanisms underlying this phenomenon are not well understood. Specifically, it is unclear how these factors establish cell-specific binding sites and contribute to gene regulation.",
    "solution": "The study demonstrates that in macrophages and B cells, collaborative interactions between the common factor PU.1 and small sets of lineage-determining transcription factors establish cell-specific binding sites associated with promoter-distal H3K4me1-marked regions. PU.1 binding initiates nucleosome remodeling and H3K4 monomethylation, creating sites that attract additional factors and drive cell-specific gene expression and responses. These findings suggest that simple combinations of lineage-determining transcription factors can specify genomic sites responsible for cell identity and type-specific responses.",
    "year": 2010,
    "journal": "Molecular Cell",
    "citations": {
      "total": 12712,
      "supporting": 175,
      "contradicting": 4,
      "mentioning": 12520,
      "unclassified": 13,
      "citingPublications": 12556
    }
  },
  {
    "query": "Granger Causality",
    "title": "Co-Integration and Error Correction: Representation, Estimation, and Testing",
    "doi": "10.2307/1913236",
    "problem": "The paper addresses the challenge of modeling and testing relationships among nonstationary time series that may be co-integrated, meaning that while individual series are nonstationary, certain linear combinations are stationary. Traditional vector autoregressions in differenced variables do not adequately capture the dynamics of co-integrated systems. There is also a need for effective estimation procedures and statistical tests for co-integration, particularly given the complexities of unit roots and unidentified parameters under the null hypothesis.",
    "solution": "The paper extends the relationship between co-integration and error correction models, developing new estimation procedures, tests, and empirical examples. It introduces a representation theorem connecting moving average, autoregressive, and error correction models for co-integrated systems, and proposes a simple, asymptotically efficient two-step estimator. Seven test statistics are formulated and analyzed, with critical values determined via Monte Carlo simulation, and one recommended test procedure is identified based on power properties.",
    "year": 1987,
    "journal": "Econometrica",
    "citations": {
      "total": 14327,
      "supporting": 101,
      "contradicting": 7,
      "mentioning": 13430,
      "unclassified": 789,
      "citingPublications": 26082
    }
  },
  {
    "query": "Granger Causality",
    "title": "FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data",
    "doi": "10.1155/2011/156869",
    "problem": "Experimental neuroscientists require robust and user-friendly tools to analyze complex electrophysiological data such as MEG and EEG. Existing solutions may lack consistency, extensibility, or advanced analysis capabilities. There is a need for software that supports both simple and advanced analyses, as well as the ability to handle large datasets efficiently.",
    "solution": "The authors developed FieldTrip, an open source MATLAB toolbox designed for the analysis of MEG, EEG, and other electrophysiological data. FieldTrip provides a comprehensive set of high-level functions for various analyses, including time-frequency analysis, source reconstruction, connectivity analysis, and statistical testing. Its modular and extensible design enables users and developers to perform structured analyses, extend functionality, and integrate with other software packages.",
    "year": 2011,
    "journal": "Computational Intelligence and Neuroscience",
    "citations": {
      "total": 8065,
      "supporting": 13,
      "contradicting": 2,
      "mentioning": 8035,
      "unclassified": 15,
      "citingPublications": 10212
    }
  },
  {
    "query": "Granger Causality",
    "title": "Causation, Prediction, and Search",
    "doi": "10.7551/mitpress/1754.001.0001",
    "problem": "The problem addressed is how to reliably discover and infer causal structure from data, particularly through the connection between conditional independence and causality. There is a need for general and rigorous procedures for causal inference in complex systems. Previous approaches lacked a unified framework connecting these concepts.",
    "solution": "The solution proposed involves leveraging Judea Pearl's work on probabilistic reasoning and graphical models to establish a general connection between conditional independence and causal structure. This approach provides a foundation for reliable discovery procedures in causal inference, utilizing directed graphical models and integrating insights from related frameworks and researchers.",
    "year": 2001,
    "journal": "",
    "citations": {
      "total": 5352,
      "supporting": 10,
      "contradicting": 1,
      "mentioning": 5291,
      "unclassified": 50,
      "citingPublications": 5525
    }
  },
  {
    "query": "Granger Causality",
    "title": "Saliency, switching, attention and control: a network model of insula function",
    "doi": "10.1007/s00429-010-0262-0",
    "problem": "The insula is implicated in a wide range of cognitive, affective, and regulatory functions, but its precise role in mediating interactions between different brain networks remains unclear. Traditional views have classified the insula as a limbic region, yet recent evidence suggests it may play a more central role in cognitive control and attention. There is a need for a unifying model that explains the diverse functions attributed to the insula.",
    "solution": "The authors propose a network model in which the anterior insula acts as an integral hub within a 'salience network,' mediating dynamic interactions between brain networks responsible for attention and self-related cognition. This model posits that the insula detects salient events, initiates control signals, and facilitates network switching to guide behavior. The framework provides a parsimonious explanation for insula function and offers insights into affective and social cognitive disorders.",
    "year": 2010,
    "journal": "Brain Structure and Function",
    "citations": {
      "total": 4766,
      "supporting": 300,
      "contradicting": 13,
      "mentioning": 4434,
      "unclassified": 19,
      "citingPublications": 5129
    }
  },
  {
    "query": "Granger Causality",
    "title": "A critique of the cross-lagged panel model.",
    "doi": "10.1037/a0038889",
    "problem": "The cross-lagged panel model (CLPM) is commonly used to study causal influences in longitudinal panel data, but it fails to adequately account for trait-like, time-invariant stability in constructs. As a result, the autoregressive relationships estimated by the CLPM do not accurately represent within-person relationships over time, potentially leading to erroneous conclusions about causal influences.",
    "solution": "The article proposes an alternative model that separates within-person processes from stable between-person differences by including random intercepts. This approach is analytically compared to the CLPM, and simulations demonstrate its effectiveness in avoiding spurious results. A modeling strategy is presented and illustrated with empirical data to help researchers avoid the pitfalls of the traditional CLPM.",
    "year": 2015,
    "journal": "Psychological Methods",
    "citations": {
      "total": 4166,
      "supporting": 40,
      "contradicting": 7,
      "mentioning": 4105,
      "unclassified": 14,
      "citingPublications": 3144
    }
  },
  {
    "query": "Granger Causality",
    "title": "The Reorienting System of the Human Brain: From Environment to Theory of Mind",
    "doi": "10.1016/j.neuron.2008.04.017",
    "problem": "The ability to adapt behavior in response to advantageous or threatening stimuli, known as 'reorienting,' requires the coordinated activity of specific brain networks. However, the mechanisms underlying how these networks interact and are regulated during different attentional states remain unclear. There is also uncertainty about the broader functional roles of the ventral attention network beyond redirecting attention.",
    "solution": "Recent evidence suggests that the ventral attention network, influenced by inputs from the locus coeruleus/norepinephrine system, plays a more general role in switching between brain networks rather than solely redirecting attention. This broader function may account for its involvement in diverse cognitive processes, including social cognition. The study proposes that understanding these patterns of network recruitment and suppression can clarify the mechanisms of adaptive behavioral responses.",
    "year": 2008,
    "journal": "Neuron",
    "citations": {
      "total": 3850,
      "supporting": 319,
      "contradicting": 25,
      "mentioning": 3486,
      "unclassified": 20,
      "citingPublications": 3722
    }
  },
  {
    "query": "Granger Causality",
    "title": "Investigating <strong class=\"highlight\">Causal</strong> Relations by Econometric Models and Cross-Spectral Methods",
    "doi": "10.1017/cbo9780511753978.002",
    "problem": "There is difficulty in determining the direction of causality between two related variables and in identifying whether feedback occurs. Additionally, the issue of apparent instantaneous causality arises, often due to delays in recording information or the omission of relevant causal variables.",
    "solution": "The authors propose testable definitions of causality and feedback, illustrated with simple two-variable models. They show that the cross spectrum between two variables can be decomposed to analyze each causal direction in a feedback situation, allowing for the construction of measures of causal lag and strength. A generalization using the partial cross spectrum is also suggested.",
    "year": null,
    "journal": "",
    "citations": {
      "total": 3801,
      "supporting": 9,
      "contradicting": 1,
      "mentioning": 3469,
      "unclassified": 322,
      "citingPublications": 4291
    }
  },
  {
    "query": "Granger Causality",
    "title": "Financial Development and Economic Growth: Views and Agenda",
    "doi": "10.1596/1813-9450-1678",
    "problem": "The relationship between financial development and economic growth is complex, with existing research often focusing narrowly on specific financial instruments or institutions. There is a need to understand the broader, functional role of financial systems in promoting economic growth and how differences in financial structure affect development outcomes. Additionally, questions remain about why financial structures change as countries grow and whether certain structures offer long-term growth advantages.",
    "solution": "Levine proposes a functional approach that examines the quality of functions provided by financial systems and their ties to economic growth, rather than focusing on specific instruments or institutions. This approach emphasizes understanding how financial systems mitigate information and transaction costs, thereby influencing savings, investment, innovation, and growth. He advocates for more comprehensive research into how financial development and structure affect economic outcomes and evolve over time.",
    "year": 1999,
    "journal": "",
    "citations": {
      "total": 3720,
      "supporting": 73,
      "contradicting": 5,
      "mentioning": 3263,
      "unclassified": 379,
      "citingPublications": 3240
    }
  },
  {
    "query": "Granger Causality",
    "title": "Testing for <strong class=\"highlight\">Granger</strong> non-<strong class=\"highlight\">causality</strong> in heterogeneous panels",
    "doi": "10.1016/j.econmod.2012.02.014",
    "problem": "Testing for Granger non-causality in heterogeneous panel data models is challenging, especially when accounting for cross-sectional dependence and small sample sizes. Existing methods may not perform well under these conditions. There is a need for a simple and robust statistical test for Granger non-causality in such settings.",
    "solution": "The paper proposes a simple test for Granger non-causality in heterogeneous panel data models, using an average of individual Wald statistics across cross-sectional units. The test statistic is shown to have desirable asymptotic properties, and a standardized version is developed based on moment approximations. Monte Carlo experiments demonstrate that the proposed standardized statistic performs well even with small samples and cross-sectional dependence.",
    "year": 2012,
    "journal": "Economic Modelling",
    "citations": {
      "total": 3487,
      "supporting": 30,
      "contradicting": 1,
      "mentioning": 3260,
      "unclassified": 196,
      "citingPublications": 4798
    }
  },
  {
    "query": "Kendall Tau",
    "title": "Trim and Fill: A Simple Funnel‐Plot–Based Method of Testing and Adjusting for Publication Bias in Meta‐Analysis",
    "doi": "10.1111/j.0006-341x.2000.00455.x",
    "problem": "The paper addresses the issue of estimating the number of missing studies in a meta-analysis and understanding the impact these missing studies might have on the overall outcome. This problem is closely related to detecting and correcting for publication bias, which can distort meta-analytic results.",
    "solution": "The authors propose using simple rank-based data augmentation techniques, specifically the trim and fill method, which formalize the use of funnel plots to detect and adjust for missing studies. They demonstrate that these methods provide effective and powerful tests for publication bias, and show that adjusting for missing studies improves the accuracy of effect size estimates and confidence interval coverage in meta-analyses.",
    "year": 2000,
    "journal": "Biometrics",
    "citations": {
      "total": 8752,
      "supporting": 17,
      "contradicting": 3,
      "mentioning": 8697,
      "unclassified": 35,
      "citingPublications": 12325
    }
  },
  {
    "query": "Kendall Tau",
    "title": "Global observed changes in daily climate extremes of temperature and precipitation",
    "doi": "10.1029/2005jd006290",
    "problem": "There is a need to accurately assess and understand global trends in extreme temperature and precipitation events, particularly in the context of climate change. Previous analyses have been limited by inconsistent methodologies, data sparsity in certain regions, and a lack of comprehensive, up-to-date global coverage. This has hindered the ability to detect and interpret significant changes in climate extremes over time.",
    "solution": "The researchers developed exact formulas for a suite of climate change indices focused on extreme events and utilized specially designed software to ensure consistent analysis across different countries. By combining high-quality station data from around the world and conducting workshops in data-sparse regions, they created gridded seasonal and annual indices for 1951-2003 and analyzed trends for statistical significance. This approach enabled the presentation of the most comprehensive and current global picture of trends in extreme temperature and precipitation indices.",
    "year": 2006,
    "journal": "Journal of Geophysical Research Atmospheres",
    "citations": {
      "total": 3382,
      "supporting": 236,
      "contradicting": 16,
      "mentioning": 3019,
      "unclassified": 111,
      "citingPublications": 4138
    }
  },
  {
    "query": "Kendall Tau",
    "title": "Interrater Reliability of a Modified Ashworth Scale of Muscle Spasticity",
    "doi": "10.1093/ptj/67.2.206",
    "problem": "The study addresses the need to determine the interrater reliability of manual tests for grading elbow flexor muscle spasticity using a modified Ashworth scale. There is uncertainty about how consistently different raters can assess spasticity in patients with intracranial lesions.",
    "solution": "The researchers independently graded the elbow flexor muscle spasticity of 30 patients and measured agreement using Kendall's tau correlation, finding a high level of reliability. Their results suggest that the modified Ashworth scale is reliable for this purpose and warrant further trials.",
    "year": 1987,
    "journal": "Physical Therapy",
    "citations": {
      "total": 3075,
      "supporting": 15,
      "contradicting": 5,
      "mentioning": 2952,
      "unclassified": 103,
      "citingPublications": 4915
    }
  },
  {
    "query": "Kendall Tau",
    "title": "Revised International Prognostic Scoring System for Myelodysplastic Syndromes",
    "doi": "10.1182/blood-2012-03-420489",
    "problem": "The International Prognostic Scoring System (IPSS) is widely used to assess prognosis in primary untreated adult patients with myelodysplastic syndromes (MDS), but it has limitations in precision and comprehensiveness. There is a need to refine the IPSS to improve prognostic accuracy using a larger and more diverse patient dataset. The existing system does not fully account for certain clinical and cytogenetic features relevant to patient outcomes.",
    "solution": "The researchers assembled a much larger, combined international database of MDS patients and developed a revised prognostic model (IPSS-R) using multiple statistically weighted clinical features. The new model incorporates refined cytogenetic subgroups, more detailed classifications of marrow blast percentage and cytopenias, and additional clinical variables, resulting in five prognostic categories instead of four. This comprehensive system provides a more precise method for analyzing MDS patient prognosis and is expected to improve clinical outcome prediction and clinical trial design.",
    "year": 2012,
    "journal": "Blood",
    "citations": {
      "total": 2854,
      "supporting": 49,
      "contradicting": 17,
      "mentioning": 2715,
      "unclassified": 73,
      "citingPublications": 3016
    }
  },
  {
    "query": "Kendall Tau",
    "title": "Functional Neuroimaging of Anxiety: A Meta-Analysis of Emotional Processing in PTSD, Social Anxiety Disorder, and Specific Phobia",
    "doi": "10.1176/appi.ajp.2007.07030504",
    "problem": "Findings from individual functional neuroimaging studies of anxiety disorders are highly variable, making it difficult to identify consistent neurobiological deficits across different disorders. There is a need to determine both common and disorder-specific functional brain abnormalities in anxiety disorders and to compare these with neural responses during anticipatory anxiety in healthy individuals.",
    "solution": "The authors conducted a quantitative meta-analysis of functional MRI and PET studies on PTSD, social anxiety disorder, specific phobia, and fear conditioning in healthy individuals. This approach synthesized disparate findings to identify consistent patterns of brain activation and revealed both shared and unique neural mechanisms underlying anxiety disorders, thereby refining our understanding of their neurobiological basis.",
    "year": 2007,
    "journal": "American Journal of Psychiatry",
    "citations": {
      "total": 2670,
      "supporting": 176,
      "contradicting": 18,
      "mentioning": 2452,
      "unclassified": 24,
      "citingPublications": 3148
    }
  },
  {
    "query": "Kendall Tau",
    "title": "What is Twitter, a social network or a news media?",
    "doi": "10.1145/1772690.1772751",
    "problem": "The paper addresses the lack of quantitative understanding of Twitter's topological characteristics and its effectiveness as a medium for information sharing. Specifically, it seeks to analyze the structure of the Twitter social network, identify influential users, and understand how information diffuses through the platform. There is also an interest in the nature and spread of trending topics.",
    "solution": "The authors conducted a large-scale crawl of the entire Twitter site, collecting extensive data on user profiles, social relations, trending topics, and tweets. They analyzed the follower-following topology, ranked users by various influence metrics, studied the temporal behavior and classification of trending topics, and examined the diffusion patterns of retweets. Their findings provide new insights into Twitter's network structure, influence dynamics, and information diffusion processes.",
    "year": 2010,
    "journal": "",
    "citations": {
      "total": 2584,
      "supporting": 91,
      "contradicting": 7,
      "mentioning": 2399,
      "unclassified": 87,
      "citingPublications": 5865
    }
  },
  {
    "query": "Kendall Tau",
    "title": "Prevention of relapse/recurrence in major depression by mindfulness-based cognitive therapy.",
    "doi": "10.1037/0022-006x.68.4.615",
    "problem": "Recovered recurrently depressed patients are at high risk of relapse or recurrence of major depression, particularly due to depressogenic thinking triggered by dysphoria. Existing treatments may not sufficiently prevent relapse in this population. There is a need for effective interventions to reduce relapse rates in these patients.",
    "solution": "The study evaluated mindfulness-based cognitive therapy (MBCT), a group intervention designed to help patients disengage from depressogenic thinking. Patients were randomized to receive either treatment as usual or treatment as usual plus MBCT, and relapse rates were monitored. MBCT was found to significantly reduce the risk of relapse/recurrence in patients with three or more previous episodes of depression, suggesting it is a promising and cost-efficient approach for preventing relapse in this group.",
    "year": 2000,
    "journal": "Journal of Consulting and Clinical Psychology",
    "citations": {
      "total": 2156,
      "supporting": 75,
      "contradicting": 19,
      "mentioning": 1965,
      "unclassified": 97,
      "citingPublications": 2611
    }
  },
  {
    "query": "Kendall Tau",
    "title": "The effect of mindfulness-based therapy on anxiety and depression: A meta-analytic review.",
    "doi": "10.1037/a0018555",
    "problem": "Although mindfulness-based therapy has become a popular treatment, there is limited knowledge about its efficacy. This lack of evidence raises questions about how effective mindfulness-based therapy truly is.",
    "solution": "The study aims to investigate and provide evidence regarding the efficacy of mindfulness-based therapy. By evaluating its effectiveness, the research seeks to address the current gap in knowledge.",
    "year": 2010,
    "journal": "Journal of Consulting and Clinical Psychology",
    "citations": {
      "total": 2156,
      "supporting": 113,
      "contradicting": 10,
      "mentioning": 1946,
      "unclassified": 87,
      "citingPublications": 3410
    }
  },
  {
    "query": "Divided Differences",
    "title": "Deep Residual Learning for Image Recognition",
    "doi": "10.1109/cvpr.2016.90",
    "problem": "Deeper neural networks are more difficult to train, which limits the ability to increase network depth for improved performance. Traditional architectures struggle with optimization and accuracy as depth increases. This poses a challenge for advancing visual recognition tasks that benefit from deeper representations.",
    "solution": "The authors propose a residual learning framework that reformulates network layers to learn residual functions with reference to the layer inputs, rather than unreferenced functions. This approach makes it easier to optimize very deep networks and allows them to gain accuracy from increased depth. Comprehensive empirical evidence demonstrates that these residual networks outperform previous architectures, achieving state-of-the-art results on major benchmarks.",
    "year": 2016,
    "journal": "",
    "citations": {
      "total": 120349,
      "supporting": 309,
      "contradicting": 22,
      "mentioning": 119363,
      "unclassified": 655,
      "citingPublications": 206992
    }
  },
  {
    "query": "Divided Differences",
    "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2",
    "doi": "10.1186/s13059-014-0550-8",
    "problem": "In comparative high-throughput sequencing assays, analyzing count data such as read counts per gene in RNA-seq to detect systematic changes across experimental conditions is challenging. Issues such as small replicate numbers, data discreteness, large dynamic range, and the presence of outliers complicate statistical analysis. A suitable approach is needed to address these challenges and provide reliable differential expression results.",
    "solution": "The authors present DESeq2, a method for differential analysis of count data that uses shrinkage estimation for dispersions and fold changes to enhance the stability and interpretability of estimates. This approach enables more quantitative analysis, focusing on the strength of differential expression rather than just its presence. DESeq2 is provided as a software package for the research community.",
    "year": 2014,
    "journal": "Genome Biology",
    "citations": {
      "total": 63870,
      "supporting": 97,
      "contradicting": 8,
      "mentioning": 63699,
      "unclassified": 66,
      "citingPublications": 80597
    }
  },
  {
    "query": "Divided Differences",
    "title": "MUSCLE: multiple sequence alignment with high accuracy and high throughput",
    "doi": "10.1093/nar/gkh340",
    "problem": "The problem addressed is the need for accurate and efficient methods to create multiple alignments of protein sequences. Existing tools may not optimally balance speed and accuracy, especially when handling large datasets. There is also a need for benchmarking these methods on diverse reference alignment sets.",
    "solution": "The solution proposed is MUSCLE, a new computer program that creates multiple alignments of protein sequences using fast distance estimation, a novel log-expectation profile function, and refinement via tree-dependent restricted partitioning. MUSCLE demonstrates superior or comparable accuracy to existing methods and is significantly faster, especially for large numbers of sequences. The program, along with its source code and benchmark data, is made freely available to the research community.",
    "year": 2004,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 32338,
      "supporting": 41,
      "contradicting": 1,
      "mentioning": 32123,
      "unclassified": 173,
      "citingPublications": 42368
    }
  },
  {
    "query": "Divided Differences",
    "title": "Bias in meta-analysis detected by a simple, graphical test",
    "doi": "10.1136/bmj.315.7109.629",
    "problem": "There is a need to detect bias in meta-analyses, particularly when their results are later contradicted by large trials. It is unclear whether asymmetry in funnel plots can predict discordance between meta-analyses and large trials. Additionally, the prevalence of bias in published meta-analyses is not well established.",
    "solution": "The study proposes using a simple test of funnel plot asymmetry, specifically the intercept from regression of standard normal deviates against precision, to predict bias and discordance in meta-analyses compared to large trials. The analysis demonstrates that funnel plot asymmetry is associated with discordant results and can indicate the presence of bias. However, the method's effectiveness is limited when meta-analyses are based on a small number of trials, so results should be interpreted with caution.",
    "year": 1997,
    "journal": "BMJ",
    "citations": {
      "total": 30448,
      "supporting": 69,
      "contradicting": 13,
      "mentioning": 30201,
      "unclassified": 165,
      "citingPublications": 49041
    }
  },
  {
    "query": "Divided Differences",
    "title": "When to use and how to report the results of PLS-SEM",
    "doi": "10.1108/ebr-11-2018-0203",
    "problem": "Researchers using partial least squares structural equation modeling (PLS-SEM) face challenges in understanding which considerations, metrics, and evaluation criteria are most appropriate for analysis and reporting of results. The field is evolving rapidly, with new methods and guidelines emerging that scholars need to be aware of and apply correctly. There is a need for a comprehensive and up-to-date overview of both established and recently proposed metrics and guidelines for PLS-SEM.",
    "solution": "This paper provides a concise yet comprehensive overview of the considerations, metrics, and rules of thumb for conducting and reporting PLS-SEM analyses. It summarizes both established and newly proposed evaluation criteria, including novel approaches such as PLSpredict, model comparison metrics, and robustness checks, offering updated guidelines for researchers. The paper serves as the most current summary of best practices in applying and assessing PLS-SEM.",
    "year": 2019,
    "journal": "European Business Review",
    "citations": {
      "total": 23047,
      "supporting": 563,
      "contradicting": 27,
      "mentioning": 21451,
      "unclassified": 1006,
      "citingPublications": 17285
    }
  },
  {
    "query": "Divided Differences",
    "title": "Densely Connected Convolutional Networks",
    "doi": "10.1109/cvpr.2017.243",
    "problem": "Traditional convolutional networks are limited by their depth due to issues such as the vanishing-gradient problem, inefficient feature propagation, and redundant parameters, which hinder their accuracy and training efficiency. There is a need for architectures that can be deeper, more accurate, and more efficient to train. Existing networks typically only connect each layer to its immediate successor, missing opportunities for richer information flow.",
    "solution": "The paper proposes the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward manner, allowing all preceding layers' feature maps to serve as inputs for each layer. This architecture alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and reduces the number of parameters. DenseNets demonstrate significant improvements over state-of-the-art methods on several object recognition benchmarks while requiring less computation.",
    "year": 2017,
    "journal": "",
    "citations": {
      "total": 22724,
      "supporting": 69,
      "contradicting": 6,
      "mentioning": 22499,
      "unclassified": 150,
      "citingPublications": 42146
    }
  },
  {
    "query": "Divided Differences",
    "title": "ImageNet Large Scale Visual Recognition Challenge",
    "doi": "10.1007/s11263-015-0816-y",
    "problem": "There is a need for a standardized benchmark to evaluate object category classification and detection across hundreds of categories and millions of images. Collecting large-scale ground truth annotations and accurately assessing the state of the art in computer vision are significant challenges. The field lacks comprehensive analysis and comparison with human-level accuracy.",
    "solution": "The paper introduces the ImageNet Large Scale Visual Recognition Challenge as a benchmark dataset and competition to advance object recognition research. It details the creation of the dataset, discusses key breakthroughs enabled by the challenge, and provides in-depth analysis of the field's progress. The authors also compare computer vision systems to human performance and suggest future improvements.",
    "year": 2015,
    "journal": "International Journal of Computer Vision",
    "citations": {
      "total": 22215,
      "supporting": 63,
      "contradicting": 2,
      "mentioning": 21960,
      "unclassified": 190,
      "citingPublications": 38692
    }
  },
  {
    "query": "Divided Differences",
    "title": "Common risk factors in the returns on stocks and bonds",
    "doi": "10.1016/0304-405x(93)90023-5",
    "problem": "The paper addresses the challenge of identifying the key factors that drive the returns on stocks and bonds. There is a need to understand the sources of shared variation in returns across these asset classes and to explain average returns. Existing models may not fully capture the common risk factors influencing both stocks and bonds.",
    "solution": "The authors propose a model with five common risk factors: three related to the stock market (overall market, firm size, and book-to-market equity) and two related to the bond market (maturity and default risks). These factors are shown to capture the shared variation in returns and explain average returns on both stocks and bonds. The model demonstrates that, except for low-grade corporate bonds, these five factors account for the common variation in bond returns as well.",
    "year": 1993,
    "journal": "Journal of Financial Economics",
    "citations": {
      "total": 21343,
      "supporting": 613,
      "contradicting": 88,
      "mentioning": 20232,
      "unclassified": 410,
      "citingPublications": 24694
    }
  },
  {
    "query": "Divided Differences",
    "title": "Coefficient Alpha and the Internal Structure of Tests",
    "doi": "10.1007/bf02310555",
    "problem": "The problem addressed is the need for accurate and practical estimation of test reliability, particularly when it is not feasible to administer a test twice to the same subjects. Traditional methods like the split-half Spearman-Brown procedure have limitations, and there is a need for improved indices that can estimate the reliability and equivalence of test items. Additionally, there is a challenge in ensuring that test scores are interpretable and that subtests are appropriately constructed.",
    "solution": "The proposed solution is a general formula, of which the Kuder-Richardson coefficient is a special case, that estimates reliability as the mean of all split-half coefficients from different test splittings. This formula provides an appropriate index of equivalence and inter-item homogeneity, and it is recommended that tests be divided into distinct subtests before applying the formula. The approach eliminates the necessity for parallel split coefficients in common test types and offers guidance for test design to maximize interpretability by increasing first-factor concentration and avoiding group-factor clusters.",
    "year": 1951,
    "journal": "Psychometrika",
    "citations": {
      "total": 21166,
      "supporting": 259,
      "contradicting": 17,
      "mentioning": 19149,
      "unclassified": 1741,
      "citingPublications": 37389
    }
  },
  {
    "query": "Divided Differences",
    "title": "Quantifying heterogeneity in a meta‐analysis",
    "doi": "10.1002/sim.1186",
    "problem": "In meta-analyses, the extent of heterogeneity affects the ability to draw overall conclusions, but current measures like between-study variance are specific to particular treatment effect metrics and tests for heterogeneity depend on the number of studies included. This makes interpretation and comparison across studies challenging.",
    "solution": "The authors develop and propose new measures of the impact of heterogeneity—specifically, the statistics H, R, and I2—that are independent of the number of studies and the treatment effect metric. They demonstrate the usefulness and interpretability of H and I2 as particularly effective summaries of heterogeneity's impact, recommending that one or both be reported in published meta-analyses instead of the traditional test for heterogeneity.",
    "year": 2002,
    "journal": "Statistics in Medicine",
    "citations": {
      "total": 20514,
      "supporting": 41,
      "contradicting": 6,
      "mentioning": 20359,
      "unclassified": 108,
      "citingPublications": 32366
    }
  },
  {
    "query": "ARIMA",
    "title": "A Pacific Interdecadal Climate Oscillation with Impacts on Salmon Production",
    "doi": "10.1175/1520-0477(1997)078<1069:apicow>2.0.co;2",
    "problem": "There is a recurring pattern of ocean-atmosphere climate variability over the midlatitude North Pacific basin, with irregular amplitude changes at interannual-to-interdecadal timescales. The timing and causes of reversals in the polarity of this oscillation are not well understood, despite their significant impact on salmon production and regional climate. Understanding these patterns is crucial for predicting environmental and ecological changes in the region.",
    "solution": "The study analyzes instrumental climate data to identify and characterize the recurring climate variability pattern in the North Pacific. By examining historical records, the research aims to pinpoint the timing of polarity reversals and assess their effects on regional climate and ecological systems, such as salmon production and river streamflow.",
    "year": 1997,
    "journal": "Bulletin of the American Meteorological Society",
    "citations": {
      "total": 6134,
      "supporting": 262,
      "contradicting": 19,
      "mentioning": 5725,
      "unclassified": 128,
      "citingPublications": 6702
    }
  },
  {
    "query": "ARIMA",
    "title": "A New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle",
    "doi": "10.2307/1912559",
    "problem": "The problem addressed is how to model and infer changes in regime, such as discrete shifts in the mean growth rate of a nonstationary time series, when these shifts are not directly observable. Econometricians must determine whether and when such regime changes have occurred based only on observed data. This is particularly relevant for understanding features like economic recessions in macroeconomic time series.",
    "solution": "The paper proposes modeling the parameters of an autoregression as outcomes of a discrete-state Markov process and presents a nonlinear iterative filtering algorithm to draw probabilistic inference about regime shifts. This filter enables estimation of population parameters via maximum likelihood and supports forecasting future series values. The approach is empirically applied to U.S. real GNP, demonstrating its utility in identifying and measuring economic recessions.",
    "year": 1989,
    "journal": "Econometrica",
    "citations": {
      "total": 6102,
      "supporting": 56,
      "contradicting": 9,
      "mentioning": 5692,
      "unclassified": 345,
      "citingPublications": 8168
    }
  },
  {
    "query": "ARIMA",
    "title": "The effects of feedback interventions on performance: A historical review, a meta-analysis, and a preliminary feedback intervention theory.",
    "doi": "10.1037/0033-2909.119.2.254",
    "problem": "Feedback interventions (FIs) have been found to sometimes negatively impact performance, with over a third of interventions actually decreasing performance. Existing theories and factors such as sampling error and feedback sign do not adequately explain these contradictory and confusing results. There is a lack of understanding regarding why FIs can have such varied effects on behavior.",
    "solution": "The authors propose a preliminary Feedback Intervention Theory (FIT), which posits that FIs alter the locus of attention among three hierarchical levels: task learning, task motivation, and meta-task (self-related) processes. They test this theory with moderator analyses, finding that FI effectiveness decreases as attention shifts away from the task and toward the self. The results suggest that understanding the locus of attention and task characteristics can help explain and potentially improve the effectiveness of FIs.",
    "year": 1996,
    "journal": "Psychological Bulletin",
    "citations": {
      "total": 4614,
      "supporting": 154,
      "contradicting": 13,
      "mentioning": 4281,
      "unclassified": 166,
      "citingPublications": 4931
    }
  },
  {
    "query": "ARIMA",
    "title": "Quantum Theory of Angular Momentum",
    "doi": "10.1142/0270",
    "problem": "Many essential formulas and relationships in the theory of angular momentum and irreducible tensors are scattered across various books and papers, often using inconsistent phase conventions, definitions, and symbols. This makes it difficult for researchers and practitioners in quantum mechanics and related fields to access and use these results efficiently.",
    "solution": "The authors compiled and organized a comprehensive collection of material on the quantum theory of angular momentum within a unified system of phases and definitions. Their book includes both fundamental theoretical results and numerous practical formulas, presented in a consistent manner to serve as a practical handbook for users.",
    "year": 1988,
    "journal": "",
    "citations": {
      "total": 4592,
      "supporting": 30,
      "contradicting": 1,
      "mentioning": 4533,
      "unclassified": 28,
      "citingPublications": 4455
    }
  },
  {
    "query": "ARIMA",
    "title": "Magnetic control of ferroelectric polarization",
    "doi": "10.1038/nature02018",
    "problem": "The mutual control of electric and magnetic properties through the magnetoelectric effect is technologically attractive, but the number of suitable materials is limited and the observed effects are typically too small for practical applications. There is a need to find new materials with significant magnetoelectric responses.",
    "solution": "The authors report the discovery of ferroelectricity in the perovskite manganite TbMnO3, where spin frustration leads to sinusoidal antiferromagnetic ordering and a spontaneous polarization. This material exhibits gigantic magnetoelectric and magnetocapacitance effects, attributed to the switching of electric polarization by magnetic fields, suggesting that frustrated spin systems are promising candidates for new magnetoelectric media.",
    "year": 2003,
    "journal": "Nature",
    "citations": {
      "total": 3756,
      "supporting": 147,
      "contradicting": 19,
      "mentioning": 3566,
      "unclassified": 24,
      "citingPublications": 4667
    }
  },
  {
    "query": "ARIMA",
    "title": "The Economic Costs of Conflict: A Case Study of the Basque Country",
    "doi": "10.1257/000282803321455188",
    "problem": "The paper addresses the economic impact of conflict, specifically terrorism, on regional economies. It focuses on the Basque Country, where terrorism began in the late 1960s, and examines how such conflict affects per capita GDP and firm performance.",
    "solution": "The authors use the Basque Country as a case study and apply a synthetic control method to compare its economic performance to a similar region without terrorism. They also analyze the effects of a 1998-1999 truce as a natural experiment, observing stock market reactions of firms with significant business in the region during and after the cease-fire.",
    "year": 2003,
    "journal": "American Economic Review",
    "citations": {
      "total": 3216,
      "supporting": 33,
      "contradicting": 2,
      "mentioning": 3127,
      "unclassified": 54,
      "citingPublications": 4113
    }
  },
  {
    "query": "ARIMA",
    "title": "The Interacting Boson Model",
    "doi": "10.1017/cbo9780511895517",
    "problem": "There has been a need to describe the collective properties of nuclei in a unified way. Over the years, the interacting boson model has been extended and studied to address various aspects of nuclear structure. However, the mathematical techniques and formulas developed for this model have been scattered and not easily accessible.",
    "solution": "This book provides a comprehensive account of the interacting boson model, detailing the mathematical techniques used to analyze its structure. It compiles all relevant formulas into a single, easily accessible reference, making it useful for both theorists and experimentalists.",
    "year": 1987,
    "journal": "",
    "citations": {
      "total": 2598,
      "supporting": 59,
      "contradicting": 3,
      "mentioning": 2527,
      "unclassified": 9,
      "citingPublications": 2058
    }
  },
  {
    "query": "ARIMA",
    "title": "Automatic Time Series Forecasting: TheforecastPackage forR",
    "doi": "10.18637/jss.v027.i03",
    "problem": "There is a need for automatic forecasting of large numbers of univariate time series in business and other contexts. Efficient and reliable forecasting methods are required to handle both seasonal and non-seasonal data. Existing methods may require manual intervention or may not be suitable for large-scale applications.",
    "solution": "The paper proposes two automatic forecasting algorithms implemented in the forecast package for R: one based on innovations state space models for exponential smoothing, and another using a step-wise algorithm for ARIMA models. These algorithms are designed to handle both seasonal and non-seasonal time series automatically. Their effectiveness is demonstrated through comparisons and illustrations using four real time series.",
    "year": 2008,
    "journal": "Journal of Statistical Software",
    "citations": {
      "total": 2476,
      "supporting": 1,
      "contradicting": 0,
      "mentioning": 2382,
      "unclassified": 93,
      "citingPublications": 3392
    }
  },
  {
    "query": "ADFuller",
    "title": "Host lifestyle affects human microbiota on daily timescales",
    "doi": "10.1186/gb-2014-15-7-r89",
    "problem": "Disturbances to human microbiota are thought to contribute to various diseases, but there is a lack of comprehensive understanding of how lifestyle factors influence the dynamics of human-associated microbial communities. Specifically, the impact of daily behaviors and rare life events on the stability and composition of the gut and salivary microbiota remains unclear.",
    "solution": "The researchers conducted a year-long study linking over 10,000 longitudinal measurements of human wellness and actions to daily gut and salivary microbiota profiles in two individuals. By analyzing these time series, they demonstrated that while microbial communities are generally stable, they can be rapidly and significantly altered by specific life events and behaviors, such as travel, infection, and dietary changes.",
    "year": 2014,
    "journal": "Genome Biology",
    "citations": {
      "total": 976,
      "supporting": 52,
      "contradicting": 2,
      "mentioning": 919,
      "unclassified": 3,
      "citingPublications": 944
    }
  },
  {
    "query": "ADFuller",
    "title": "Ecological Stability Emerges at the Level of Strains in the Human Gut Microbiome",
    "doi": "10.1128/mbio.02502-22",
    "problem": "Research on the human gut microbiome has primarily focused on ecological dynamics at the species level, overlooking significant genetic diversity within species. Strain-level differences within species can lead to important phenotypic effects on the host, influencing digestion and drug metabolism.",
    "solution": "The study proposes to address this gap by investigating the genetic diversity and ecological dynamics at the strain level within the human gut microbiome, aiming to better understand how intraspecific differences affect host phenotypes.",
    "year": 2023,
    "journal": "Mbio",
    "citations": {
      "total": 60,
      "supporting": 3,
      "contradicting": 0,
      "mentioning": 57,
      "unclassified": 0,
      "citingPublications": 46
    }
  },
  {
    "query": "ADFuller",
    "title": "Forecasting E-Commerce Products Prices by Combining an Autoregressive Integrated Moving Average (ARIMA) Model and Google Trends Data",
    "doi": "10.3390/fi11010005",
    "problem": "With the rise of e-commerce as a primary channel for selling goods, there is increasing demand for algorithms that can accurately predict future product prices. Accurate price forecasting enables the development of smart systems that help consumers find more affordable goods and services. The challenge lies in effectively leveraging time series data, reputation, and sentiment analysis to improve prediction accuracy.",
    "solution": "The authors propose Price Probe, a suite of software tools designed to forecast product price trends using the ARIMA model. The system collects product price data from Amazon, as well as information from social media and Google Trends, which are used as exogenous features in the ARIMA model. Experimental results show that incorporating Google Trends data significantly enhances the accuracy of price predictions.",
    "year": 2018,
    "journal": "Future Internet",
    "citations": {
      "total": 33,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 29,
      "unclassified": 4,
      "citingPublications": 60
    }
  },
  {
    "query": "ADFuller",
    "title": "Sentinel node approach to monitoring online COVID-19 misinformation",
    "doi": "10.1038/s41598-022-12450-8",
    "problem": "Understanding how different online communities engage with COVID-19 misinformation is essential for effective public health responses. The risk posed by misinformation varies depending on whether it is confined to isolated communities or spreads across diverse populations. There is a need for efficient methods to assess the breadth and depth of misinformation engagement on social media platforms like Twitter.",
    "solution": "The researchers propose a longitudinal approach using network science tools to study COVID-19 misinformation on Twitter by tracking 'sentinel nodes'—accounts from various communities—over time. They analyze these nodes using domain preference and similarity scores to characterize misinformation engagement within and between communities. This sentinel node approach enables efficient assessment of misinformation spread with modest data and computational resources.",
    "year": 2022,
    "journal": "Scientific Reports",
    "citations": {
      "total": 21,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 21,
      "unclassified": 0,
      "citingPublications": 4
    }
  },
  {
    "query": "ADFuller",
    "title": "Kernel-based joint independence tests for multivariate stationary and non-stationary time series",
    "doi": "10.1098/rsos.230857",
    "problem": "Understanding the complex relationships and potential dependencies among co-observed variables in multivariate time-series data is crucial for accurate statistical modelling and analysis. Existing methods may not adequately capture higher-order dependencies, especially in both stationary and non-stationary processes. There is a need for robust statistical tools to analyze such complex interactions in real-world multivariate time series.",
    "solution": "The authors introduce kernel-based statistical tests of joint independence in multivariate time series by extending the d-variable Hilbert–Schmidt independence criterion to both stationary and non-stationary processes. They leverage resampling techniques tailored for single- and multiple-realization time series, demonstrating the method's ability to robustly uncover significant higher-order dependencies in both synthetic and real-world datasets. This approach enhances the mathematical toolbox for analyzing multivariate time series and aids in uncovering high-order interactions in data.",
    "year": 2023,
    "journal": "Royal Society Open Science",
    "citations": {
      "total": 2,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 2,
      "unclassified": 0,
      "citingPublications": 2
    }
  },
  {
    "query": "ADFuller",
    "title": "The Complex Nature of Magnetic Element Transport in the Quiet Sun: The Lévy-walk Character",
    "doi": "10.3847/1538-4357/ab1be2",
    "problem": "The dynamic properties of small-scale magnetic fields (magnetic elements, MEs) in the solar photosphere are not fully understood, particularly regarding their evolution and the nature of their transport in the quiet Sun. There is a need to better characterize the spatial and temporal scales of these magnetic fields and to identify their dynamic regime.",
    "solution": "The researchers track MEs in long-term magnetogram series from the Hinode mission and apply diffusion entropy analysis (DEA), a method based on entropy, to detect their dynamic regime under the assumption of passive transport by plasma flow. DEA, which outperforms standard techniques, is used for the first time to determine the scaling properties of ME displacement, revealing a common turbulent regime and complex transport dynamics consistent with a Lévy walk.",
    "year": 2019,
    "journal": "The Astrophysical Journal",
    "citations": {
      "total": 2,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 2,
      "unclassified": 0,
      "citingPublications": 6
    }
  },
  {
    "query": "ADFuller",
    "title": "Benchmark of Holt-Winters and SARIMA Methods in Predicting Jakarta Climate",
    "doi": "10.20944/preprints202204.0295.v1",
    "problem": "Jakarta, as Indonesia's capital, is significantly affected by climate change and disastrous weather events, making it crucial to understand and predict future climatic conditions. There is a need to analyze meteorological data to forecast how climate parameters will influence Jakarta's weather. Accurate forecasting is essential for preparing for and mitigating the impacts of climate change in the region.",
    "solution": "The study investigates meteorological data from 1996 to 2021 and compares the SARIMA and Holt-Winters forecasting methods to predict future climatic influences on Jakarta's weather. It finds that the SARIMA method outperforms the Holt-Winters model, especially in forecasting humidity data. The forecasts effectively characterize Jakarta's climate, identifying the dry and wet seasons.",
    "year": 2022,
    "journal": "",
    "citations": {
      "total": 1,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 1,
      "unclassified": 0,
      "citingPublications": 3
    }
  },
  {
    "query": "ADFuller",
    "title": "MONITORING SYSTEM FOR LTE-A CELLULAR COMMUNICATION NETWORK ACCESSIBILITY INDICATORS",
    "doi": "10.36724/2072-8735-2021-15-3-4-16",
    "problem": "The paper addresses the challenge of predicting accessibility indicators, specifically E-RAB and E-RRC failure rates, in an LTE-A communication network operated by a regional provider in the Russian Federation. Accurate prediction of these indicators is crucial for identifying abnormal situations and improving network reliability. The study also seeks to evaluate the performance of various prediction algorithms for these indicators.",
    "solution": "The authors analyze the temporal dynamics and stationarity of the failure rate time series using ADFuller and KPSS tests, followed by ETS decomposition. They employ several predictive models, including SARIMA, Holt-Winters, Facebook Prophet, Prony decomposition, and XGBoost, and assess their performance using Median Absolute Error through test-sequence and cross-validation approaches. Additionally, they propose a monitoring system architecture that collects, analyzes, and visualizes network metrics, exploring the use of open-source solutions at each stage of the monitoring and prediction process.",
    "year": 2021,
    "journal": "T-Comm",
    "citations": {
      "total": 1,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 0,
      "unclassified": 1,
      "citingPublications": 3
    }
  },
  {
    "query": "ADFuller",
    "title": "Modeling the Direction and Volume of Trade Flows in Global Crisis, COVID-19",
    "doi": "10.1007/s40031-021-00560-2",
    "problem": "The COVID-19 pandemic has severely disrupted global trading activities, especially impacting trade-dependent nations like New Zealand due to restrictions on exports and imports. This disruption has led to significant economic challenges for New Zealand's trade economy. There is a need to understand and predict the implications of the pandemic on the country's trade performance.",
    "solution": "The paper proposes using exploratory data analysis and ARIMA modeling to evaluate and predict the impact of COVID-19 on New Zealand's trade economy. By processing trade data and implementing the ARIMA model, the study assesses the total value of imports and exports, with results validated through standard error analytical techniques. The findings support recommendations such as allowing trade for essential goods to help balance the economy during the pandemic.",
    "year": 2021,
    "journal": "Journal of the Institution of Engineers (India) Series B",
    "citations": {
      "total": 1,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 1,
      "unclassified": 0,
      "citingPublications": 4
    }
  },
  {
    "query": "F Test",
    "title": "Deep Residual Learning for Image Recognition",
    "doi": "10.1109/cvpr.2016.90",
    "problem": "Deeper neural networks are more difficult to train, often leading to optimization challenges and diminishing accuracy gains as depth increases. Existing architectures struggle to efficiently learn as network depth grows. This limits the performance improvements that could be achieved by using substantially deeper models for visual recognition tasks.",
    "solution": "The authors propose a residual learning framework that reformulates network layers to learn residual functions with reference to their inputs, rather than unreferenced functions. This approach enables the training of much deeper networks by making them easier to optimize and allowing them to benefit from increased depth. The residual networks demonstrate significant improvements in accuracy and achieve state-of-the-art results on several benchmark datasets and competitions.",
    "year": 2016,
    "journal": "",
    "citations": {
      "total": 120349,
      "supporting": 309,
      "contradicting": 22,
      "mentioning": 119363,
      "unclassified": 655,
      "citingPublications": 206992
    }
  },
  {
    "query": "F Test",
    "title": "ImageNet classification with deep convolutional neural networks",
    "doi": "10.1145/3065386",
    "problem": "The task of accurately classifying 1.2 million high-resolution images into 1000 different classes in the ImageNet LSVRC-2010 contest presents significant challenges, with previous methods yielding relatively high error rates. Improving classification accuracy on such large-scale image datasets is a key problem in computer vision.",
    "solution": "The researchers trained a large, deep convolutional neural network with 60 million parameters and 650,000 neurons, using techniques such as nonsaturating neurons, efficient GPU implementation, and dropout regularization to improve performance and reduce overfitting. This approach achieved significantly lower error rates than previous methods, winning the ILSVRC-2012 competition.",
    "year": 2017,
    "journal": "Communications of the Acm",
    "citations": {
      "total": 72304,
      "supporting": 171,
      "contradicting": 14,
      "mentioning": 71464,
      "unclassified": 655,
      "citingPublications": 71009
    }
  },
  {
    "query": "F Test",
    "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2",
    "doi": "10.1186/s13059-014-0550-8",
    "problem": "In comparative high-throughput sequencing assays, analyzing count data such as read counts per gene in RNA-seq to detect systematic changes across experimental conditions is challenging. Issues such as small replicate numbers, discreteness, large dynamic range, and outliers complicate the statistical analysis. A suitable approach is needed to address these challenges and provide reliable results.",
    "solution": "The authors present DESeq2, a method for differential analysis of count data that uses shrinkage estimation for dispersions and fold changes to improve the stability and interpretability of estimates. This approach enables more quantitative analysis focused on the strength of differential expression rather than just its presence. DESeq2 is available as a package for use by the research community.",
    "year": 2014,
    "journal": "Genome Biology",
    "citations": {
      "total": 63870,
      "supporting": 97,
      "contradicting": 8,
      "mentioning": 63699,
      "unclassified": 66,
      "citingPublications": 80597
    }
  },
  {
    "query": "F Test",
    "title": "The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations.",
    "doi": "10.1037/0022-3514.51.6.1173",
    "problem": "Researchers and theorists in the social sciences often confuse or use interchangeably the terms 'moderator' and 'mediator' when referring to third variables in research designs. This confusion can lead to conceptual and strategic misunderstandings about how variables influence behavior and outcomes. There is a need to clearly distinguish between the properties and functions of moderator and mediator variables.",
    "solution": "The authors clarify the differences between moderator and mediator variables by elaborating on their conceptual and strategic distinctions and providing examples from social science research. They also offer a compendium of analytic procedures for effectively using the moderator and mediator distinction, both separately and within broader causal systems. This approach aims to enhance researchers' understanding and application of these concepts in explaining behavioral differences.",
    "year": 1986,
    "journal": "Journal of Personality and Social Psychology",
    "citations": {
      "total": 61948,
      "supporting": 1324,
      "contradicting": 120,
      "mentioning": 59057,
      "unclassified": 1447,
      "citingPublications": 73718
    }
  },
  {
    "query": "F Test",
    "title": "Fitting Linear Mixed-Effects Models Usinglme4",
    "doi": "10.18637/jss.v067.i01",
    "problem": "Estimating parameters in linear mixed-effects models can be complex, especially when incorporating both fixed and random effects. Existing model-fitting functions in R require a clear structure and process for evaluating and optimizing the relevant statistical criteria. There is also a need for sufficient detail to allow users to extend these models for specialized applications.",
    "solution": "The lmer function in the lme4 package for R is used to determine maximum likelihood or restricted maximum likelihood estimates for linear mixed-effects models, using a formula-based approach to specify both fixed and random effects. The process involves evaluating the profiled deviance or REML criterion and optimizing it with constrained optimization functions in R. The paper describes the model structure, evaluation steps, and class structures in enough detail to enable users to write their own functions for specialized linear mixed models.",
    "year": 2015,
    "journal": "Journal of Statistical Software",
    "citations": {
      "total": 61400,
      "supporting": 67,
      "contradicting": 8,
      "mentioning": 61004,
      "unclassified": 321,
      "citingPublications": 84077
    }
  },
  {
    "query": "F Test",
    "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
    "doi": "10.1037/0021-9010.88.5.879",
    "problem": "There is a longstanding issue in behavioral sciences regarding method biases, but a comprehensive summary of their sources and control methods is lacking. This gap makes it difficult for researchers to effectively identify and manage method biases in their studies.",
    "solution": "The article aims to examine the impact of method biases on behavioral research, identify their sources, and explain the cognitive processes involved. It evaluates various procedural and statistical techniques to control for these biases and offers recommendations for selecting appropriate remedies in different research contexts.",
    "year": 2003,
    "journal": "Journal of Applied Psychology",
    "citations": {
      "total": 51016,
      "supporting": 569,
      "contradicting": 162,
      "mentioning": 49580,
      "unclassified": 705,
      "citingPublications": 63567
    }
  },
  {
    "query": "F Test",
    "title": "Gapped BLAST and PSI-BLAST: a new generation of protein database search programs",
    "doi": "10.1093/nar/25.17.3389",
    "problem": "Searching protein and DNA databases for sequence similarities using BLAST programs can be time-consuming and may lack sensitivity to weak but biologically relevant similarities. There is a need to improve both the speed and sensitivity of these tools to detect subtle sequence relationships.",
    "solution": "The authors introduce several refinements, including a new criterion for extending word hits and a heuristic for generating gapped alignments, resulting in a faster and more sensitive gapped BLAST program. They also develop PSI-BLAST, which automatically combines significant alignments into a position-specific score matrix for iterative searching, achieving greater sensitivity to weak similarities while maintaining efficient execution times.",
    "year": 1997,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 50326,
      "supporting": 111,
      "contradicting": 12,
      "mentioning": 49877,
      "unclassified": 326,
      "citingPublications": 69682
    }
  },
  {
    "query": "F Test",
    "title": "The Measurement of Observer Agreement for Categorical Data",
    "doi": "10.2307/2529310",
    "problem": "The paper addresses the challenge of analyzing multivariate categorical data from observer reliability studies, specifically focusing on measuring the extent of agreement among observers. There is a need for statistical methods to test hypotheses about observer agreement and detect interobserver bias.",
    "solution": "The authors propose a general statistical methodology that constructs functions of observed proportions to assess observer agreement and develops test statistics for relevant hypotheses. They introduce tests for interobserver bias based on first-order marginal homogeneity and create generalized kappa-type statistics to measure agreement. The methodology is demonstrated using a clinical diagnosis example from epidemiological research.",
    "year": 1977,
    "journal": "Biometrics",
    "citations": {
      "total": 42579,
      "supporting": 791,
      "contradicting": 72,
      "mentioning": 39999,
      "unclassified": 1717,
      "citingPublications": 69054
    }
  },
  {
    "query": "F Test",
    "title": "A consistent and accurateab initioparametrization of density functional dispersion correction (DFT-D) for the 94 elements H-Pu",
    "doi": "10.1063/1.3382344",
    "problem": "Modern quantum chemistry methods produce extremely complex data, such as multidimensional wavefunctions or electron densities, making it difficult for chemists to gain intuitive insight and rationalize results. This complexity is particularly challenging when trying to understand phenomena governed by non-covalent interactions, as traditional representations do not easily convey the necessary information.",
    "solution": "To address this, computational chemists have developed a range of tools and methodologies designed to analyze, identify, quantify, and visualize non-covalent interactions in a more intuitive way. These include quantitative energy decomposition analysis schemes and qualitative approaches like the Non-covalent Interaction index, Density Overlap Region Indicator, and quantum theory of atoms in molecules, which together enhance understanding and provide a roadmap for further methodological development.",
    "year": 2010,
    "journal": "The Journal of Chemical Physics",
    "citations": {
      "total": 38834,
      "supporting": 439,
      "contradicting": 14,
      "mentioning": 38203,
      "unclassified": 178,
      "citingPublications": 49100
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "Deep Residual Learning for Image Recognition",
    "doi": "10.1109/cvpr.2016.90",
    "problem": "Deeper neural networks are more difficult to train, which limits the ability to improve performance by increasing network depth. Traditional approaches struggle to optimize very deep models, leading to diminishing returns or even worse accuracy.",
    "solution": "The authors propose a residual learning framework that reformulates network layers to learn residual functions with reference to the layer inputs, rather than unreferenced functions. This approach makes it easier to train substantially deeper networks, resulting in improved optimization and higher accuracy, as demonstrated on large-scale datasets such as ImageNet and COCO.",
    "year": 2016,
    "journal": "",
    "citations": {
      "total": 120349,
      "supporting": 309,
      "contradicting": 22,
      "mentioning": 119363,
      "unclassified": 655,
      "citingPublications": 206992
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2",
    "doi": "10.1186/s13059-014-0550-8",
    "problem": "In comparative high-throughput sequencing assays, analyzing count data such as read counts per gene in RNA-seq to detect systematic changes across experimental conditions is challenging. The analysis is complicated by small replicate numbers, data discreteness, large dynamic range, and the presence of outliers, necessitating a suitable statistical approach.",
    "solution": "The authors present DESeq2, a method for differential analysis of count data that uses shrinkage estimation for dispersions and fold changes to improve the stability and interpretability of estimates. This approach enables more quantitative analysis focused on the strength of differential expression rather than just its presence.",
    "year": 2014,
    "journal": "Genome Biology",
    "citations": {
      "total": 63870,
      "supporting": 97,
      "contradicting": 8,
      "mentioning": 63699,
      "unclassified": 66,
      "citingPublications": 80597
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations.",
    "doi": "10.1037/0022-3514.51.6.1173",
    "problem": "Researchers and theorists in the social sciences often confuse and use the terms 'moderator' and 'mediator' variables interchangeably, leading to conceptual and strategic misunderstandings. This confusion can obscure the different ways in which third variables account for differences in behavior. The lack of clear distinction between these two types of variables affects the interpretation and analysis of research findings.",
    "solution": "The article clarifies the differences between moderator and mediator variables by elaborating on their distinct conceptual and strategic properties. It provides a compendium of analytic procedures for effectively distinguishing and utilizing moderators and mediators, both separately and within broader causal systems. This approach aims to improve theoretical understanding and research practices by ensuring that the correct functions of third variables are recognized and applied.",
    "year": 1986,
    "journal": "Journal of Personality and Social Psychology",
    "citations": {
      "total": 61948,
      "supporting": 1324,
      "contradicting": 120,
      "mentioning": 59057,
      "unclassified": 1447,
      "citingPublications": 73718
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "Fitting Linear Mixed-Effects Models Usinglme4",
    "doi": "10.18637/jss.v067.i01",
    "problem": "Estimating parameters in linear mixed-effects models can be complex, especially when incorporating both fixed and random effects. Existing model-fitting functions in R, such as lmer in the lme4 package, require a clear structure and process for describing and optimizing these models. There is also a need for sufficient detail to allow users to extend these models for specialized applications not easily handled by the standard formula language.",
    "solution": "The paper describes how maximum likelihood or restricted maximum likelihood (REML) estimates can be determined using the lmer function in the lme4 package for R. It details the structure of the model, the steps involved in evaluating the profiled deviance or REML criterion, and the class structures representing such models. This information enables users to specialize and extend these structures to fit more complex linear mixed models, such as those involving pedigrees or smoothing splines.",
    "year": 2015,
    "journal": "Journal of Statistical Software",
    "citations": {
      "total": 61400,
      "supporting": 67,
      "contradicting": 8,
      "mentioning": 61004,
      "unclassified": 321,
      "citingPublications": 84077
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
    "doi": "10.1037/0021-9010.88.5.879",
    "problem": "There is a longstanding issue of method biases affecting results in behavioral sciences research. However, a comprehensive summary of the sources of these biases and effective ways to control them is lacking.",
    "solution": "The article examines the impact of method biases on behavioral research, identifies their sources, and discusses the cognitive mechanisms through which they operate. It evaluates various procedural and statistical techniques to control for these biases and provides recommendations for selecting appropriate remedies in different research contexts.",
    "year": 2003,
    "journal": "Journal of Applied Psychology",
    "citations": {
      "total": 51016,
      "supporting": 569,
      "contradicting": 162,
      "mentioning": 49580,
      "unclassified": 705,
      "citingPublications": 63567
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "The Measurement of Observer Agreement for Categorical Data",
    "doi": "10.2307/2529310",
    "problem": "The paper addresses the challenge of analyzing multivariate categorical data from observer reliability studies, specifically focusing on assessing the extent of agreement among observers. There is a need for statistical methods to test hypotheses about observer agreement and to detect interobserver bias.",
    "solution": "The authors propose a methodology that constructs functions of observed proportions to evaluate observer agreement and develops test statistics for related hypotheses. They introduce tests for interobserver bias based on first-order marginal homogeneity and develop generalized kappa-type statistics to measure agreement. The approach is demonstrated using a clinical diagnosis example from epidemiological research.",
    "year": 1977,
    "journal": "Biometrics",
    "citations": {
      "total": 42579,
      "supporting": 791,
      "contradicting": 72,
      "mentioning": 39999,
      "unclassified": 1717,
      "citingPublications": 69054
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "A short history ofSHELX",
    "doi": "10.1107/s0108767307043930",
    "problem": "The paper addresses the challenges and limitations in the development and continued use of the SHELX system of computer programs for crystal-structure determination, including outdated features, missed opportunities, and the need for improvements. It also explores how SHELX has remained relevant despite being originally designed for much older technology.",
    "solution": "The solution involves a critical analysis of the innovations and shortcomings of the SHELX programs, highlighting their evolution and current applications in both small-molecule and macromolecular structure determination. The paper also discusses the utility of various SHELX components in modern crystallography workflows and suggests directions for future enhancements.",
    "year": 2007,
    "journal": "Acta Crystallographica Section a Foundations of Crystallography",
    "citations": {
      "total": 41114,
      "supporting": 144,
      "contradicting": 2,
      "mentioning": 40783,
      "unclassified": 185,
      "citingPublications": 86469
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "Measuring inconsistency in meta-analyses",
    "doi": "10.1136/bmj.327.7414.557",
    "problem": "Postoperative delirium is a common complication after surgery, characterized by confusion and inattentiveness, and it remains unclear whether intraoperative EEG monitoring can reduce its incidence. There is a need to evaluate the effectiveness of EEG-guided anesthesia in preventing postoperative delirium.",
    "solution": "The study systematically reviewed and analyzed randomized controlled trials to assess the impact of EEG-guided anesthesia on postoperative delirium. The findings suggest that intraoperative EEG monitoring can decrease the risk of postoperative delirium, although further large-scale trials are recommended to confirm this effect.",
    "year": 2003,
    "journal": "BMJ",
    "citations": {
      "total": 36427,
      "supporting": 85,
      "contradicting": 15,
      "mentioning": 36083,
      "unclassified": 244,
      "citingPublications": 55602
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "Bias in meta-analysis detected by a simple, graphical <strong class=\"highlight\">test</strong>",
    "doi": "10.1136/bmj.315.7109.629",
    "problem": "Bias in meta-analyses can lead to results that are later contradicted by large trials, making it difficult to assess the reliability of meta-analytic findings. There is a need for a method to predict when meta-analyses might be discordant with large trials and to assess the prevalence of such bias in published meta-analyses.",
    "solution": "The study proposes using a simple test of funnel plot asymmetry, specifically the intercept from regression of standard normal deviates against precision, to detect bias in meta-analyses. This analysis of funnel plots can help predict the likelihood of bias, although its effectiveness is limited when meta-analyses are based on a small number of trials.",
    "year": 1997,
    "journal": "BMJ",
    "citations": {
      "total": 30448,
      "supporting": 69,
      "contradicting": 13,
      "mentioning": 30201,
      "unclassified": 165,
      "citingPublications": 49041
    }
  },
  {
    "query": "Chi-Squared Test",
    "title": "Regression Shrinkage and Selection Via the Lasso",
    "doi": "10.1111/j.2517-6161.1996.tb02080.x",
    "problem": "The challenge addressed is the estimation of coefficients in linear models, where traditional methods often result in models that are either difficult to interpret or lack stability. Existing approaches like subset selection and ridge regression each have their own limitations in terms of interpretability and stability.",
    "solution": "The proposed solution is the 'lasso' method, which minimizes the residual sum of squares subject to a constraint on the sum of the absolute values of the coefficients. This approach tends to set some coefficients exactly to zero, resulting in more interpretable models, while also retaining the stability properties of ridge regression. The method is general and can be extended to other statistical models.",
    "year": 1996,
    "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
    "citations": {
      "total": 30306,
      "supporting": 84,
      "contradicting": 9,
      "mentioning": 29998,
      "unclassified": 215,
      "citingPublications": 46002
    }
  },
  {
    "query": "Exponential Smoothing",
    "title": "Deep learning",
    "doi": "10.1038/nature14539",
    "problem": "There is a need to learn complex representations of data with multiple levels of abstraction to improve performance in domains such as speech recognition, visual object recognition, object detection, drug discovery, and genomics. Traditional methods have limitations in discovering intricate structures in large datasets.",
    "solution": "Deep learning, utilizing computational models with multiple processing layers and the backpropagation algorithm, enables machines to learn hierarchical representations from data. Deep convolutional networks excel at processing images, video, speech, and audio, while recurrent networks are effective for sequential data like text and speech.",
    "year": 2015,
    "journal": "Nature",
    "citations": {
      "total": 0,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 0,
      "unclassified": 0,
      "citingPublications": 0
    }
  },
  {
    "query": "Exponential Smoothing",
    "title": "limma powers differential expression analyses for RNA-sequencing and microarray studies",
    "doi": "10.1093/nar/gkv007",
    "problem": "Gene expression experiments often involve complex experimental designs and suffer from small sample sizes, making data analysis challenging. Traditional tools have been limited in their ability to handle both microarray and RNA-seq data, as well as in providing comprehensive downstream analysis and biological interpretation. There is a need for integrated software that can address these limitations and support advanced analyses.",
    "solution": "The limma R/Bioconductor package offers an integrated solution for analyzing gene expression data, supporting both microarray and RNA-seq experiments with similar analysis pipelines. It provides robust features for data normalization, exploration, and differential expression/splicing analyses, as well as new capabilities for analyzing co-regulated gene sets and higher-order expression signatures. These enhancements enable more comprehensive and biologically meaningful interpretations of gene expression differences.",
    "year": 2015,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 28343,
      "supporting": 33,
      "contradicting": 1,
      "mentioning": 28269,
      "unclassified": 40,
      "citingPublications": 33810
    }
  },
  {
    "query": "Exponential Smoothing",
    "title": "The electronic properties of graphene",
    "doi": "10.1103/revmodphys.81.109",
    "problem": "The electronic properties of graphene, a one-atom-thick carbon material with unique Dirac-like excitations, are not fully understood, especially regarding how they are influenced by external fields, geometry, stacking order, edge termination, disorder, and interactions. There is a need to clarify how these factors affect phenomena such as tunneling, confinement, quantum Hall effects, and transport properties.",
    "solution": "The article reviews and analyzes the theoretical aspects of graphene's electronic properties, demonstrating how Dirac electrons behave under various conditions and how these behaviors are influenced by external controls, stacking, edge termination, and disorder. It also discusses the effects of electron-electron and electron-phonon interactions in both single-layer and multilayer graphene, providing a comprehensive overview of the factors that modify graphene's physical and spectroscopic properties.",
    "year": 2009,
    "journal": "Reviews of Modern Physics",
    "citations": {
      "total": 18350,
      "supporting": 319,
      "contradicting": 12,
      "mentioning": 17837,
      "unclassified": 182,
      "citingPublications": 23990
    }
  },
  {
    "query": "Exponential Smoothing",
    "title": "A <strong class=\"highlight\">smooth</strong> particle mesh Ewald method",
    "doi": "10.1063/1.470117",
    "problem": "The existing particle mesh Ewald method is limited in its interpolation approach and accuracy, particularly for potentials of the form 1/r^p with p ≠ 1. Calculating analytic gradients and achieving high accuracy for large biomolecular systems is computationally challenging. There is a need for a method that provides arbitrary accuracy and efficient computation of the virial tensor, while maintaining feasible computational costs for large systems.",
    "solution": "The method is reformulated using efficient B-spline interpolation of the structure factors, replacing Lagrange interpolation. This allows for analytic gradients, improved accuracy, and extension to a wider class of potentials. The approach achieves arbitrary accuracy independent of system size, with computational cost scaling as N log(N), making Ewald summation practical for large biomolecular systems at costs comparable to simple truncation methods.",
    "year": 1995,
    "journal": "The Journal of Chemical Physics",
    "citations": {
      "total": 16931,
      "supporting": 43,
      "contradicting": 1,
      "mentioning": 16840,
      "unclassified": 47,
      "citingPublications": 21528
    }
  },
  {
    "query": "Exponential Smoothing",
    "title": "Maximum entropy modeling of species geographic distributions",
    "doi": "10.1016/j.ecolmodel.2005.03.026",
    "problem": "Most species lack absence data, which limits the use of standard statistical techniques for modeling species' environmental requirements and geographic distributions. This creates a challenge for accurately predicting species distributions using only presence data.",
    "solution": "The paper introduces the maximum entropy method (Maxent) as a machine learning approach for modeling species distributions using presence-only data. Maxent is shown to outperform a commonly used method (GARP) in predicting species ranges, providing better discrimination of suitable versus unsuitable areas, and can be applied to many presence-only datasets.",
    "year": 2006,
    "journal": "Ecological Modelling",
    "citations": {
      "total": 16098,
      "supporting": 39,
      "contradicting": 2,
      "mentioning": 15346,
      "unclassified": 711,
      "citingPublications": 15801
    }
  },
  {
    "query": "Exponential Smoothing",
    "title": "Scalable molecular dynamics with NAMD",
    "doi": "10.1002/jcc.20289",
    "problem": "Simulating large biomolecular systems requires high-performance molecular dynamics software that can efficiently utilize both high-end parallel platforms and low-cost clusters, while also being accessible to individual users. Existing tools may lack scalability, flexibility, or compatibility with common force fields and file formats. There is a need for a robust, scalable, and user-friendly molecular dynamics code that addresses these challenges.",
    "solution": "NAMD is introduced as a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems, scaling efficiently across a wide range of hardware. It supports AMBER and CHARMM force fields, offers advanced features such as efficient electrostatics, free energy calculations, and scripting, and is implemented using C++ and Charm++ for parallelism. NAMD is freely distributed with source code and integrates with visualization and collaborative tools like VMD and BioCoRE.",
    "year": 2005,
    "journal": "Journal of Computational Chemistry",
    "citations": {
      "total": 15009,
      "supporting": 35,
      "contradicting": 2,
      "mentioning": 14899,
      "unclassified": 73,
      "citingPublications": 16703
    }
  },
  {
    "query": "Exponential Smoothing",
    "title": "Regression Models and Life-Tables",
    "doi": "10.1111/j.2517-6161.1972.tb00899.x",
    "problem": "The paper addresses the analysis of censored failure times, where data on one or more explanatory variables are available for each individual. The main challenge is to model the hazard function as a function of these explanatory variables and unknown regression coefficients, while also accounting for an arbitrary and unknown function of time.",
    "solution": "The authors propose obtaining a conditional likelihood that allows for inference about the unknown regression coefficients, despite the arbitrary and unknown time function. This approach enables the estimation of the effects of explanatory variables on the hazard function. Some generalizations of the method are also discussed.",
    "year": 1972,
    "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
    "citations": {
      "total": 12648,
      "supporting": 30,
      "contradicting": 1,
      "mentioning": 12307,
      "unclassified": 310,
      "citingPublications": 39510
    }
  },
  {
    "query": "Exponential Smoothing",
    "title": "Integrated analysis of multimodal single-cell data",
    "doi": "10.1016/j.cell.2021.04.048",
    "problem": "The integration and analysis of multiple modalities in single-cell genomics is challenging, as it requires computational methods capable of defining cellular states using multimodal data. Existing approaches may not effectively leverage the relative utility of each data type in each cell. This limits the ability to resolve complex cellular identities and interpret immune responses.",
    "solution": "The authors propose 'weighted-nearest neighbor' analysis, an unsupervised framework that learns the relative utility of each data type in each cell for integrative multimodal analysis. They demonstrate its effectiveness by constructing a multimodal reference atlas of human immune cells and show that it improves cell state resolution and enables rapid mapping and interpretation of new datasets. This approach provides a generalizable strategy for analyzing single-cell multimodal datasets and defining cellular identity beyond transcriptomic data alone.",
    "year": 2021,
    "journal": "Cell",
    "citations": {
      "total": 11217,
      "supporting": 66,
      "contradicting": 0,
      "mentioning": 11150,
      "unclassified": 1,
      "citingPublications": 11619
    }
  },
  {
    "query": "Exponential",
    "title": "Deep Residual Learning for Image Recognition",
    "doi": "10.1109/cvpr.2016.90",
    "problem": "Deeper neural networks are more difficult to train, often leading to optimization challenges and degraded performance as network depth increases. Existing architectures struggle to effectively utilize substantially increased depth for improved accuracy in visual recognition tasks.",
    "solution": "The authors propose a residual learning framework that reformulates network layers to learn residual functions with reference to their inputs, rather than unreferenced functions. This approach makes it easier to optimize very deep networks and allows for significant accuracy improvements, as demonstrated by their successful application to large-scale image recognition and detection benchmarks.",
    "year": 2016,
    "journal": "",
    "citations": {
      "total": 120349,
      "supporting": 309,
      "contradicting": 22,
      "mentioning": 119363,
      "unclassified": 655,
      "citingPublications": 206992
    }
  },
  {
    "query": "Exponential",
    "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
    "doi": "10.1037/0021-9010.88.5.879",
    "problem": "There is a longstanding issue of method biases affecting results in behavioral sciences research. Despite its significance, there is no comprehensive summary detailing the sources of these biases or strategies to control them.",
    "solution": "The article aims to examine the impact of method biases on behavioral research, identify their sources, and discuss the cognitive processes involved. It also evaluates various procedural and statistical techniques to control method biases and provides recommendations for selecting appropriate remedies in different research contexts.",
    "year": 2003,
    "journal": "Journal of Applied Psychology",
    "citations": {
      "total": 51016,
      "supporting": 569,
      "contradicting": 162,
      "mentioning": 49580,
      "unclassified": 705,
      "citingPublications": 63567
    }
  },
  {
    "query": "Exponential",
    "title": "The Measurement of Observer Agreement for Categorical Data",
    "doi": "10.2307/2529310",
    "problem": "The paper addresses the challenge of analyzing multivariate categorical data from observer reliability studies, specifically focusing on measuring the extent of agreement among observers and detecting interobserver bias. Traditional methods may not adequately capture the complexity of such data or provide robust statistical tests for agreement and bias.",
    "solution": "The authors propose a general statistical methodology that constructs functions of observed proportions to assess observer agreement and develops test statistics for relevant hypotheses. They introduce tests for interobserver bias based on first-order marginal homogeneity and create generalized kappa-type statistics to measure interobserver agreement. The methodology is demonstrated using a clinical diagnosis example from epidemiological research.",
    "year": 1977,
    "journal": "Biometrics",
    "citations": {
      "total": 42579,
      "supporting": 791,
      "contradicting": 72,
      "mentioning": 39999,
      "unclassified": 1717,
      "citingPublications": 69054
    }
  },
  {
    "query": "Exponential",
    "title": "Deep learning",
    "doi": "10.1038/nature14539",
    "problem": "There is a need for computational models that can learn complex representations of data with multiple levels of abstraction to improve performance in tasks such as speech recognition, visual object recognition, object detection, and other domains. Traditional approaches have struggled to effectively capture intricate structures in large datasets.",
    "solution": "Deep learning methods, particularly those using multiple processing layers and the backpropagation algorithm, enable the learning of hierarchical data representations. Techniques such as deep convolutional networks and recurrent networks have led to significant breakthroughs in processing images, video, speech, audio, and sequential data like text.",
    "year": 2015,
    "journal": "Nature",
    "citations": {
      "total": 41814,
      "supporting": 87,
      "contradicting": 4,
      "mentioning": 41107,
      "unclassified": 616,
      "citingPublications": 75083
    }
  },
  {
    "query": "Exponential",
    "title": "A consistent and accurateab initioparametrization of density functional dispersion correction (DFT-D) for the 94 elements H-Pu",
    "doi": "10.1063/1.3382344",
    "problem": "Quantum chemistry methods, while powerful, produce highly complex data such as multidimensional wavefunctions or electron densities, making it difficult for chemists to gain intuitive understanding and rationalize chemical behavior. This complexity is especially problematic when studying phenomena governed by non-covalent interactions, as traditional representations do not easily yield fundamental insights.",
    "solution": "To address this, computational chemists have developed specialized tools and methodologies for analyzing, identifying, quantifying, and visualizing non-covalent interactions. These include quantitative energy decomposition analysis schemes and qualitative approaches like the Non-covalent Interaction index, Density Overlap Region Indicator, and quantum theory of atoms in molecules, which together provide more informative and intuitive representations of chemical phenomena.",
    "year": 2010,
    "journal": "The Journal of Chemical Physics",
    "citations": {
      "total": 38834,
      "supporting": 439,
      "contradicting": 14,
      "mentioning": 38203,
      "unclassified": 178,
      "citingPublications": 49100
    }
  },
  {
    "query": "Exponential",
    "title": "limma powers differential expression analyses for RNA-sequencing and microarray studies",
    "doi": "10.1093/nar/gkv007",
    "problem": "Gene expression experiments often involve complex experimental designs and small sample sizes, making data analysis challenging. Traditional tools were limited in their ability to handle both microarray and RNA-seq data, and often focused only on gene-wise analyses, restricting biological interpretation.",
    "solution": "The limma R/Bioconductor package provides an integrated solution for analyzing gene expression data, supporting both microarray and RNA-seq data with similar analysis pipelines. It offers advanced features for differential expression and splicing analyses, as well as tools for exploring co-regulated gene sets and higher-order expression signatures, thereby enhancing biological interpretation.",
    "year": 2015,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 28343,
      "supporting": 33,
      "contradicting": 1,
      "mentioning": 28269,
      "unclassified": 40,
      "citingPublications": 33810
    }
  },
  {
    "query": "Exponential",
    "title": "Adam: A Method for Stochastic Optimization",
    "doi": "10.48550/arxiv.1412.6980",
    "problem": "Optimizing stochastic objective functions, especially in large-scale problems with non-stationary objectives and very noisy or sparse gradients, is challenging. Existing first-order gradient-based optimization methods may not be efficient or robust enough for such scenarios. There is a need for an algorithm that is computationally efficient, memory-friendly, and effective under these conditions.",
    "solution": "The authors propose Adam, an algorithm that uses adaptive estimates of lower-order moments for first-order gradient-based optimization. Adam is easy to implement, computationally efficient, requires little memory, and is invariant to diagonal rescaling of gradients, making it suitable for large-scale and noisy problems. They also introduce AdaMax, a variant based on the infinity norm, and provide theoretical and empirical evidence of Adam's effectiveness compared to other methods.",
    "year": 2014,
    "journal": "",
    "citations": {
      "total": 23561,
      "supporting": 22,
      "contradicting": 0,
      "mentioning": 23465,
      "unclassified": 74,
      "citingPublications": 60969
    }
  },
  {
    "query": "Exponential",
    "title": "Maximum Likelihood from Incomplete Data Via the EM Algorithm",
    "doi": "10.1111/j.2517-6161.1977.tb01600.x",
    "problem": "Computing maximum likelihood estimates from incomplete data is a challenging problem encountered in various statistical applications, such as missing value situations, grouped, censored or truncated data, and mixture models. Traditional methods often struggle with convergence or applicability across different types of incomplete data scenarios.",
    "solution": "The paper presents a broadly applicable algorithm designed to compute maximum likelihood estimates from incomplete data. Theoretical results are provided to demonstrate the monotone behavior of the likelihood and the convergence of the algorithm. The approach is illustrated with numerous examples across diverse statistical problems.",
    "year": 1977,
    "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
    "citations": {
      "total": 23283,
      "supporting": 27,
      "contradicting": 2,
      "mentioning": 22884,
      "unclassified": 370,
      "citingPublications": 44904
    }
  },
  {
    "query": "Exponential",
    "title": "A pneumonia outbreak associated with a new coronavirus of probable bat origin",
    "doi": "10.1038/s41586-020-2012-7",
    "problem": "The study addresses the problem of understanding which human cell surface receptors are used by the 2019-nCoV (now known as SARS-CoV-2) virus to infect cells. Specifically, it investigates whether receptors such as ACE2, APN, and DPP4 facilitate viral entry into human cells.",
    "solution": "The researchers tested virus infectivity in HeLa cells engineered to express human ACE2, APN, or DPP4 receptors. They used tagged plasmids and immunofluorescence to detect the presence of these receptors and viral proteins, allowing them to analyze which receptors support 2019-nCoV entry.",
    "year": 2020,
    "journal": "Nature",
    "citations": {
      "total": 23058,
      "supporting": 260,
      "contradicting": 15,
      "mentioning": 21997,
      "unclassified": 786,
      "citingPublications": 22452
    }
  },
  {
    "query": "Exponential",
    "title": "When to use and how to report the results of PLS-SEM",
    "doi": "10.1108/ebr-11-2018-0203",
    "problem": "Researchers using partial least squares structural equation modeling (PLS-SEM) face challenges in understanding which considerations and metrics are necessary for proper analysis and reporting of results. Existing guidelines may not fully encompass recent methodological developments, leading to potential gaps in evaluation and application. There is a need for updated and comprehensive guidance on both established and newly proposed metrics for PLS-SEM.",
    "solution": "This paper provides a concise yet comprehensive overview of the considerations, metrics, and rules of thumb required for PLS-SEM analysis and result reporting. It introduces new guidelines, such as PLSpredict for out-of-sample prediction, model comparison metrics, and complementary methods for robustness checks, alongside established evaluation criteria. The paper serves as the most current summary of PLS-SEM methods and metrics, ensuring researchers are informed about both traditional and emerging best practices.",
    "year": 2019,
    "journal": "European Business Review",
    "citations": {
      "total": 23047,
      "supporting": 563,
      "contradicting": 27,
      "mentioning": 21451,
      "unclassified": 1006,
      "citingPublications": 17285
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "Moderated estimation <strong class=\"highlight\">of</strong> fold change and dispersion for RNA-seq data with DESeq2",
    "doi": "10.1186/s13059-014-0550-8",
    "problem": "In comparative high-throughput sequencing assays, analyzing count data such as read counts per gene in RNA-seq for systematic changes across experimental conditions is a fundamental task. This analysis is complicated by small replicate numbers, discreteness, large dynamic range, and the presence of outliers, necessitating a suitable statistical approach.",
    "solution": "The authors present DESeq2, a method for differential analysis of count data that uses shrinkage estimation for dispersions and fold changes to improve the stability and interpretability of estimates. This approach enables more quantitative analysis focused on the strength of differential expression rather than just its presence.",
    "year": 2014,
    "journal": "Genome Biology",
    "citations": {
      "total": 63870,
      "supporting": 97,
      "contradicting": 8,
      "mentioning": 63699,
      "unclassified": 66,
      "citingPublications": 80597
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "Fitting Linear Mixed-Effects Models Usinglme4",
    "doi": "10.18637/jss.v067.i01",
    "problem": "Estimating parameters in linear mixed-effects models can be challenging, especially for specialized models such as those incorporating pedigrees or smoothing splines, which are not easily expressible in the standard formula language used by common R functions. There is a need for a flexible framework that allows users to fit such specialized models.",
    "solution": "The authors describe the structure of linear mixed-effects models as implemented in the lmer function of the lme4 package for R, detailing how the profiled deviance or REML criterion can be evaluated and optimized for parameter estimation. They provide sufficient detail on the model structures and classes to enable users to write their own functions for fitting specialized linear mixed models beyond the standard formula interface.",
    "year": 2015,
    "journal": "Journal <strong Class=\"highlight\">of</Strong> Statistical Software",
    "citations": {
      "total": 61400,
      "supporting": 67,
      "contradicting": 8,
      "mentioning": 61004,
      "unclassified": 321,
      "citingPublications": 84077
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "Common method biases in behavioral research: A critical review <strong class=\"highlight\">of</strong> the literature and recommended remedies.",
    "doi": "10.1037/0021-9010.88.5.879",
    "problem": "There is a longstanding issue of method biases in the behavioral sciences, which can influence research results. However, a comprehensive summary of the potential sources of these biases and effective ways to control them is lacking.",
    "solution": "The article examines the extent to which method biases affect behavioral research, identifies their potential sources, discusses the cognitive processes involved, evaluates various procedural and statistical control techniques, and provides recommendations for selecting appropriate remedies in different research settings.",
    "year": 2003,
    "journal": "Journal <strong Class=\"highlight\">of</Strong> Applied Psychology",
    "citations": {
      "total": 51016,
      "supporting": 569,
      "contradicting": 162,
      "mentioning": 49580,
      "unclassified": 705,
      "citingPublications": 63567
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "Gapped BLAST and PSI-BLAST: a new generation <strong class=\"highlight\">of</strong> protein database search programs",
    "doi": "10.1093/nar/25.17.3389",
    "problem": "The BLAST programs, while widely used for searching protein and DNA databases for sequence similarities, face challenges in execution speed and sensitivity to weak similarities. There is a need to improve both the efficiency and the ability to detect biologically relevant but weak sequence similarities.",
    "solution": "The authors introduce several definitional, algorithmic, and statistical refinements to the BLAST programs, including a new criterion for word hit extension and a heuristic for generating gapped alignments, resulting in a faster and more sensitive gapped BLAST program. Additionally, they propose PSI-BLAST, which automatically combines significant alignments into a position-specific score matrix for iterative searching, greatly enhancing sensitivity to weak similarities while maintaining speed.",
    "year": 1997,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 50326,
      "supporting": 111,
      "contradicting": 12,
      "mentioning": 49877,
      "unclassified": 326,
      "citingPublications": 69682
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "A consistent and accurate<i>ab initio</i>parametrization <strong class=\"highlight\">of</strong> density functional dispersion correction (DFT-D) for the 94 elements H-Pu",
    "doi": "10.1063/1.3382344",
    "problem": "Quantum chemistry methods generate complex data, such as multidimensional wavefunctions or electron densities, which can obscure chemical insight and intuition. This complexity makes it difficult for chemists to rationalize and understand the results, especially for phenomena governed by non-covalent interactions. Traditional representations like total wavefunction or electron density are not always informative for these cases.",
    "solution": "The paper reviews and discusses specialized tools and methodologies developed to analyze, identify, quantify, and visualize non-covalent interactions in chemical systems. These include quantitative energy decomposition analysis schemes and qualitative approaches such as the Non-covalent Interaction index, Density Overlap Region Indicator, and quantum theory of atoms in molecules. The strengths, limitations, and future directions for enhancing these tools are also emphasized.",
    "year": 2010,
    "journal": "The Journal <strong Class=\"highlight\">of</Strong> Chemical Physics",
    "citations": {
      "total": 38834,
      "supporting": 439,
      "contradicting": 14,
      "mentioning": 38203,
      "unclassified": 178,
      "citingPublications": 49100
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "Measuring inconsistency in meta-analyses",
    "doi": "10.1136/bmj.327.7414.557",
    "problem": "Postoperative delirium is a common complication after surgery, characterized by confusion and inattentiveness. It remains unclear whether intraoperative electroencephalogram (EEG) monitoring can reduce the incidence of this condition. The study aims to evaluate the effectiveness of EEG-guided anesthesia in decreasing postoperative delirium based on evidence from randomized controlled trials.",
    "solution": "The study conducted a systematic review and meta-analysis of seven randomized controlled trials involving 3,859 patients to assess the impact of EEG monitoring during surgery. The findings indicate that intraoperative EEG monitoring is associated with a lower incidence of postoperative delirium, although it does not significantly affect the length of hospitalization. The authors recommend further large-scale trials to confirm these results and suggest a multi-component strategy for high-risk patients.",
    "year": 2003,
    "journal": "BMJ",
    "citations": {
      "total": 36427,
      "supporting": 85,
      "contradicting": 15,
      "mentioning": 36083,
      "unclassified": 244,
      "citingPublications": 55602
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "MUSCLE: multiple sequence alignment with high accuracy and high throughput",
    "doi": "10.1093/nar/gkh340",
    "problem": "Accurately and efficiently creating multiple alignments of protein sequences is a significant challenge in bioinformatics, especially as the number and length of sequences increase. Existing methods vary in speed and accuracy, and there is a need for improved algorithms that can handle large datasets effectively.",
    "solution": "The authors propose MUSCLE, a new computer program for multiple protein sequence alignment that incorporates fast distance estimation, a novel log-expectation scoring function, and a tree-dependent refinement process. MUSCLE demonstrates superior or comparable accuracy to existing methods and offers significantly faster performance on large sequence sets. The software, along with its source code and benchmark data, is made freely available to the research community.",
    "year": 2004,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 32338,
      "supporting": 41,
      "contradicting": 1,
      "mentioning": 32123,
      "unclassified": 173,
      "citingPublications": 42368
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "Regression Shrinkage and Selection Via the Lasso",
    "doi": "10.1111/j.2517-6161.1996.tb02080.x",
    "problem": "Estimating parameters in linear models often leads to challenges in interpretability and stability, especially when dealing with many predictors. Traditional methods like subset selection and ridge regression each have their own limitations in terms of model interpretability and coefficient shrinkage. There is a need for a method that combines the advantages of both approaches.",
    "solution": "The proposed solution is the 'lasso', which minimizes the residual sum of squares subject to a constraint on the sum of the absolute values of the coefficients. This constraint tends to produce models where some coefficients are exactly zero, resulting in more interpretable models. The lasso combines the interpretability of subset selection with the stability of ridge regression and can be extended to various statistical models.",
    "year": 1996,
    "journal": "Journal <strong Class=\"highlight\">of</Strong> the Royal Statistical Society Series B (Statistical Methodology)",
    "citations": {
      "total": 30306,
      "supporting": 84,
      "contradicting": 9,
      "mentioning": 29998,
      "unclassified": 215,
      "citingPublications": 46002
    }
  },
  {
    "query": "Sum of Sines Splines",
    "title": "PLINK: A Tool Set for Whole-Genome Association and Population-Based Linkage Analyses",
    "doi": "10.1086/519795",
    "problem": "Whole-genome association studies (WGAS) present significant computational and analytic challenges due to the large size of the data sets involved. Many existing genetic-analysis tools are not equipped to efficiently handle or fully exploit these large-scale data sets. This limits researchers' ability to perform comprehensive analyses and leverage new opportunities provided by whole-genome data.",
    "solution": "The authors developed PLINK, an open-source C/C++ tool set specifically designed for WGAS data. PLINK enables rapid manipulation and analysis of large data sets, supports efficient basic analytic steps, and introduces novel approaches that utilize whole-genome coverage. It provides functionality in five main domains, including data management, summary statistics, population stratification, association analysis, and identity-by-descent estimation, thereby addressing the computational and analytic needs of WGAS.",
    "year": 2007,
    "journal": "The American Journal <strong Class=\"highlight\">of</Strong> Human Genetics",
    "citations": {
      "total": 30262,
      "supporting": 53,
      "contradicting": 2,
      "mentioning": 30094,
      "unclassified": 113,
      "citingPublications": 30846
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2",
    "doi": "10.1186/s13059-014-0550-8",
    "problem": "In comparative high-throughput sequencing assays, analyzing count data such as read counts per gene in RNA-seq to detect systematic changes across experimental conditions is challenging. The analysis is complicated by small replicate numbers, data discreteness, large dynamic range, and the presence of outliers. These factors necessitate a suitable statistical approach for reliable results.",
    "solution": "The authors present DESeq2, a method for differential analysis of count data that uses shrinkage estimation for dispersions and fold changes to improve the stability and interpretability of estimates. This approach enables more quantitative analysis focused on the strength of differential expression rather than just its presence. The DESeq2 package is made available for use through Bioconductor.",
    "year": 2014,
    "journal": "Genome Biology",
    "citations": {
      "total": 63870,
      "supporting": 97,
      "contradicting": 8,
      "mentioning": 63699,
      "unclassified": 66,
      "citingPublications": 80597
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
    "doi": "10.1037/0021-9010.88.5.879",
    "problem": "There is a longstanding issue of method biases affecting results in behavioral sciences. However, there is no comprehensive summary detailing the sources of these biases or how to control them. This gap makes it difficult for researchers to effectively address method biases in their studies.",
    "solution": "The article examines the impact of method biases on behavioral research, identifies their potential sources, and discusses the cognitive processes involved. It also evaluates various procedural and statistical techniques for controlling method biases. Additionally, the article provides recommendations for selecting appropriate remedies based on different research settings.",
    "year": 2003,
    "journal": "Journal of Applied Psychology",
    "citations": {
      "total": 51016,
      "supporting": 569,
      "contradicting": 162,
      "mentioning": 49580,
      "unclassified": 705,
      "citingPublications": 63567
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "Gradient-based learning applied to document recognition",
    "doi": "10.1109/5.726791",
    "problem": "The paper addresses the challenge of accurately recognizing handwritten characters, particularly handwritten digits, which is a complex task due to the high dimensionality and variability of handwriting. Existing methods often require significant preprocessing and may not effectively handle the variability in two-dimensional shapes. Additionally, real-life document recognition systems involve multiple interconnected modules that are traditionally trained separately, potentially limiting overall system performance.",
    "solution": "The paper proposes the use of convolutional neural networks (CNNs), which are specifically designed to manage the variability of 2-D shapes, for handwritten character recognition, demonstrating superior performance over other techniques. Furthermore, it introduces graph transformer networks (GTNs), a new learning paradigm that enables global training of multimodule document recognition systems using gradient-based methods to optimize overall performance. These approaches are validated through experiments and commercial deployment, achieving record accuracy in tasks such as bank check reading.",
    "year": 1998,
    "journal": "Proceedings of the Ieee",
    "citations": {
      "total": 25812,
      "supporting": 80,
      "contradicting": 8,
      "mentioning": 25415,
      "unclassified": 309,
      "citingPublications": 52110
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "Adam: A Method for Stochastic Optimization",
    "doi": "10.48550/arxiv.1412.6980",
    "problem": "Optimizing stochastic objective functions, especially those that are large-scale, non-stationary, or have noisy and sparse gradients, is challenging for existing first-order gradient-based methods. Such problems require algorithms that are computationally efficient, memory-efficient, and robust to varying gradient scales. Additionally, there is a need for methods with hyper-parameters that are easy to interpret and tune.",
    "solution": "The paper introduces Adam, an optimization algorithm that uses adaptive estimates of lower-order moments to efficiently and effectively optimize stochastic objective functions. Adam is simple to implement, computationally efficient, memory-friendly, and invariant to diagonal rescaling of gradients, making it suitable for large and complex problems. The algorithm's theoretical convergence properties are analyzed, and empirical results show that Adam outperforms other stochastic optimization methods; a variant called AdaMax is also presented.",
    "year": 2014,
    "journal": "",
    "citations": {
      "total": 23561,
      "supporting": 22,
      "contradicting": 0,
      "mentioning": 23465,
      "unclassified": 74,
      "citingPublications": 60969
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "When to use and how to report the results of PLS-SEM",
    "doi": "10.1108/ebr-11-2018-0203",
    "problem": "Researchers using partial least squares structural equation modeling (PLS-SEM) face challenges in understanding the appropriate considerations, metrics, and evaluation criteria for conducting and reporting their analyses. There is a need for up-to-date guidance due to rapidly emerging methodological developments and new metrics in the PLS-SEM domain.",
    "solution": "This paper provides a comprehensive and concise overview of both established and newly proposed metrics and guidelines for evaluating PLS-SEM results. It introduces new approaches such as PLSpredict for out-of-sample prediction, model comparison metrics, and complementary robustness checks, ensuring that researchers are informed about the latest best practices in PLS-SEM analysis and reporting.",
    "year": 2019,
    "journal": "European Business Review",
    "citations": {
      "total": 23047,
      "supporting": 563,
      "contradicting": 27,
      "mentioning": 21451,
      "unclassified": 1006,
      "citingPublications": 17285
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "ImageNet Large Scale Visual Recognition Challenge",
    "doi": "10.1007/s11263-015-0816-y",
    "problem": "There is a need for a standardized benchmark to evaluate and advance object category classification and detection across hundreds of categories and millions of images. Collecting large-scale ground truth annotation for such datasets poses significant challenges. The field also requires a detailed analysis of progress and comparison with human-level accuracy.",
    "solution": "The paper describes the creation of the ImageNet Large Scale Visual Recognition Challenge as a benchmark dataset, which has enabled significant advances in object recognition. It discusses the process of collecting annotations, analyzes breakthroughs and the current state of large-scale image classification and detection, and compares computer vision accuracy with human performance. The authors also share lessons learned and propose future directions for improvement.",
    "year": 2015,
    "journal": "International Journal of Computer Vision",
    "citations": {
      "total": 22215,
      "supporting": 63,
      "contradicting": 2,
      "mentioning": 21960,
      "unclassified": 190,
      "citingPublications": 38692
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "The electronic properties of graphene",
    "doi": "10.1103/revmodphys.81.109",
    "problem": "The electronic properties of graphene, a one-atom-thick carbon material with unique Dirac-like excitations, are not fully understood, particularly in relation to how they are influenced by external fields, geometry, stacking, edge termination, disorder, and interactions. There is a need to clarify how these factors affect phenomena such as tunneling, confinement, quantum Hall effect, and the behavior of edge states. Understanding these influences is crucial for both fundamental physics and potential applications.",
    "solution": "The article provides a comprehensive review of the theoretical aspects of graphene, analyzing how Dirac electrons respond to external electric and magnetic fields, changes in geometry, stacking order, edge termination, and disorder. It discusses the resulting effects on electronic properties, including tunneling, confinement, quantum Hall effect, and edge states in nanoribbons. Additionally, it examines the impact of electron-electron and electron-phonon interactions in both single-layer and multilayer graphene.",
    "year": 2009,
    "journal": "Reviews of Modern Physics",
    "citations": {
      "total": 18350,
      "supporting": 319,
      "contradicting": 12,
      "mentioning": 17837,
      "unclassified": 182,
      "citingPublications": 23990
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "Statistical mechanics of complex networks",
    "doi": "10.1103/revmodphys.74.47",
    "problem": "Traditional models of complex systems, such as biological or technological networks, have relied on random graphs, which fail to capture the robust organizing principles that govern the topology and evolution of real-world networks. There is a need to better understand the statistical mechanics underlying the structure and dynamics of these complex networks.",
    "solution": "The paper reviews recent advances in the study of complex networks, focusing on empirical data and analytical tools that have led to new models such as random graphs, small-world, and scale-free networks. It also discusses the interplay between network topology and robustness, providing a comprehensive overview of the statistical mechanics of network topology and dynamics.",
    "year": 2002,
    "journal": "Reviews of Modern Physics",
    "citations": {
      "total": 16007,
      "supporting": 231,
      "contradicting": 20,
      "mentioning": 15477,
      "unclassified": 279,
      "citingPublications": 19423
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "Scalable molecular dynamics with NAMD",
    "doi": "10.1002/jcc.20289",
    "problem": "Simulating large biomolecular systems with molecular dynamics requires high-performance software that can efficiently utilize parallel computing resources and handle various force fields and file formats. Existing solutions may not scale well across different hardware platforms or may lack features for advanced simulation control and analysis. There is a need for a flexible, high-performance tool that is accessible to both novices and experts.",
    "solution": "The paper presents NAMD, a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NAMD is implemented in C++ using Charm++ parallel objects, supports AMBER and CHARMM force fields, and scales efficiently from desktops to high-end parallel platforms. It includes features for advanced simulation control, free energy calculations, and integrates with visualization and analysis tools like VMD and BioCoRE.",
    "year": 2005,
    "journal": "Journal of Computational Chemistry",
    "citations": {
      "total": 15009,
      "supporting": 35,
      "contradicting": 2,
      "mentioning": 14899,
      "unclassified": 73,
      "citingPublications": 16703
    }
  },
  {
    "query": "Weighted Moving Average",
    "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python",
    "doi": "10.1038/s41592-019-0686-2",
    "problem": "There is a need for accessible and reliable scientific computing tools in the Python programming language to support a wide range of scientific and engineering applications.",
    "solution": "SciPy is an open-source library that provides a comprehensive suite of scientific algorithms for Python. The paper offers an overview of SciPy 1.0's capabilities, development practices, and recent technical advancements.",
    "year": 2020,
    "journal": "Nature Chemical Biology",
    "citations": {
      "total": 14943,
      "supporting": 24,
      "contradicting": 1,
      "mentioning": 14752,
      "unclassified": 166,
      "citingPublications": 30852
    }
  },
  {
    "query": "Bollinger Bands",
    "title": "Optical atomic clocks",
    "doi": "10.1103/revmodphys.87.637",
    "problem": "There is a need to advance the precision and reliability of time measurement, which is fundamental for various scientific and technological applications. Current measurement science seeks to overcome limitations in accuracy and systematic uncertainties in atomic clocks.",
    "solution": "The article reviews the development of optical atomic clocks based on trapped single ions and many neutral atoms, discussing key technical components that enhance their performance. It presents recent achievements in measurement precision and systematic uncertainty, and offers insights into future applications of these advanced clocks.",
    "year": 2015,
    "journal": "Reviews of Modern Physics",
    "citations": {
      "total": 1662,
      "supporting": 15,
      "contradicting": 1,
      "mentioning": 1639,
      "unclassified": 7,
      "citingPublications": 2159
    }
  },
  {
    "query": "Bollinger Bands",
    "title": "Quantum properties of atomic-sized conductors",
    "doi": "10.1016/s0370-1573(02)00633-6",
    "problem": "Understanding the properties and behavior of metallic nanowires, especially at the atomic scale, is challenging due to the complexity of atomic structures in metallic contacts. There is a need to quantitatively compare theoretical predictions with experimental results for various quantum and mechanical properties of these atomic-scale contacts.",
    "solution": "The researchers use simple experimental techniques to gently break metallic contacts and form nanowires, reducing them to single atoms or atomic chains. This approach simplifies the system, allowing the properties of the contact to be dominantly determined by a single atom, thus enabling quantitative comparisons between theory and experiment and providing a test-bed for mesoscopic physics concepts.",
    "year": 2003,
    "journal": "Physics Reports",
    "citations": {
      "total": 1280,
      "supporting": 66,
      "contradicting": 1,
      "mentioning": 1202,
      "unclassified": 11,
      "citingPublications": 1475
    }
  },
  {
    "query": "Bollinger Bands",
    "title": "Optimizing performance through intrinsic motivation and attention for learning: The OPTIMAL theory of motor learning",
    "doi": "10.3758/s13423-015-0999-9",
    "problem": "Existing theories of motor learning do not adequately account for recent evidence showing that motivational and attentional factors significantly influence motor performance and learning. These theories often overlook the roles of enhanced expectancies, learner autonomy, and external focus of attention. There is a need for a more comprehensive framework that integrates these factors to optimize motor skill acquisition and performance.",
    "solution": "The authors propose the OPTIMAL (Optimizing Performance through Intrinsic Motivation and Attention for Learning) theory of motor learning, which emphasizes the importance of motivational and attentional factors in strengthening the link between goals and actions. This theory explains the advantages of enhanced expectancies, autonomy, and external attentional focus using psychological and neuroscientific evidence. The framework provides mechanisms for these effects, such as dopamine responses to positive anticipation and efficient brain network connections, and offers practical implications for optimizing motor learning.",
    "year": 2016,
    "journal": "Psychonomic Bulletin & Review",
    "citations": {
      "total": 1034,
      "supporting": 41,
      "contradicting": 18,
      "mentioning": 935,
      "unclassified": 40,
      "citingPublications": 943
    }
  },
  {
    "query": "Bollinger Bands",
    "title": "The First Direct Characterization of a High-Valent Iron Intermediate in the Reaction of an α-Ketoglutarate-Dependent Dioxygenase:  A High-Spin Fe(IV) Complex in Taurine/α-Ketoglutarate Dioxygenase (TauD) from Escherichia coli",
    "doi": "10.1021/bi030011f",
    "problem": "The consensus mechanism for Fe(II)- and alpha-ketoglutarate-dependent dioxygenases involves several postulated intermediates after the addition of O(2), but none of these intermediates had been directly detected prior to this study. This lack of direct evidence limited understanding of the detailed reaction mechanism of these important enzymes.",
    "solution": "This work directly demonstrates an oxidized Fe intermediate in the reaction of taurine/alpha-ketoglutarate dioxygenase (TauD) from Escherichia coli using rapid kinetic and spectroscopic methods. Characterization of the intermediate establishes it as a high-spin, formally Fe(IV) complex, providing the first direct evidence of such an intermediate and enabling more detailed mechanistic studies of this enzyme family.",
    "year": 2003,
    "journal": "Biochemistry",
    "citations": {
      "total": 1004,
      "supporting": 61,
      "contradicting": 0,
      "mentioning": 941,
      "unclassified": 2,
      "citingPublications": 711
    }
  },
  {
    "query": "Bollinger Bands",
    "title": "Non-Heme Fe(IV)–Oxo Intermediates",
    "doi": "10.1021/ar700066p",
    "problem": "The key intermediates involved in biological oxidation reactions catalyzed by non-heme iron enzymes, particularly high-valent iron-oxo species, have been proposed but not directly characterized for decades. There is uncertainty about the mechanisms by which these enzymes, especially αKG-dependent oxygenases, achieve substrate hydroxylation or halogenation. Understanding the mechanistic divergence between hydroxylation and halogenation pathways remains a challenge.",
    "solution": "Recent studies have directly characterized Fe(IV)-oxo intermediates in several αKG-dependent oxygenases, revealing their role in C-H bond cleavage to initiate substrate hydroxylation or halogenation. Comparative spectroscopic analysis suggests a conserved mechanism among hydroxylases, while the identification of two distinct Fe(IV) complexes in the halogenase CytC3 indicates that conformational differences at the Fe site may drive mechanistic divergence. These findings provide insight into the conserved and divergent pathways of non-heme iron enzymes following C-H cleavage.",
    "year": 2007,
    "journal": "Accounts of Chemical Research",
    "citations": {
      "total": 917,
      "supporting": 36,
      "contradicting": 0,
      "mentioning": 880,
      "unclassified": 1,
      "citingPublications": 943
    }
  },
  {
    "query": "Bollinger Bands",
    "title": "Oxygen Activation and Radical Transformations in Heme Proteins and Metalloporphyrins",
    "doi": "10.1021/acs.chemrev.7b00373",
    "problem": "Despite the diversity of metalloproteins involved in oxidative metabolism and protection against reactive oxygen species, the mechanistic understanding of how these proteins activate oxygen and mediate various biological functions remains incomplete. There is a need to better characterize the reactive iron–oxygen intermediates and understand the structural and electronic factors that govern their reactivity. This knowledge gap limits the development of new bioinspired catalysts and enzymatic transformations.",
    "solution": "The review surveys recent progress in the characterization and reactivity of important iron–oxygen intermediates in heme proteins and model compounds. It discusses fundamental mechanistic features, structural and electronic factors influencing reactivity, and highlights representative reactions mediated by these intermediates. By synthesizing recent advances and contextualizing them with prior work, the review aims to inform the development of new enzymatic and synthetic transformations based on metalloprotein-mediated radical processes.",
    "year": 2017,
    "journal": "Chemical Reviews",
    "citations": {
      "total": 916,
      "supporting": 16,
      "contradicting": 2,
      "mentioning": 871,
      "unclassified": 27,
      "citingPublications": 893
    }
  },
  {
    "query": "Bollinger Bands",
    "title": "Electron tunneling through proteins",
    "doi": "10.1017/s0033583503003913",
    "problem": "The mechanisms and factors regulating electron transfer processes in biological systems, particularly within and between proteins, are not fully understood. There is a need to clarify how protein structures influence the rates and efficiency of electron tunneling in redox chains. Additionally, some observed electron transfer rates exceed current theoretical predictions, suggesting gaps in existing models.",
    "solution": "The researchers investigated Ru-modified proteins to delineate the distance- and driving-force dependences of intra-protein electron-transfer rates and studied electron transfer across protein-protein interfaces. They developed an experimentally validated timetable for electron tunneling across specified distances in proteins, which aligns well with many natural reactions. For reactions that exceed timetable predictions, they propose that multistep tunneling is responsible for the anomalously rapid charge transfer events.",
    "year": 2003,
    "journal": "Quarterly Reviews of Biophysics",
    "citations": {
      "total": 869,
      "supporting": 37,
      "contradicting": 0,
      "mentioning": 823,
      "unclassified": 9,
      "citingPublications": 639
    }
  },
  {
    "query": "Markov Chains",
    "title": "Maximum Likelihood from Incomplete Data Via the EM Algorithm",
    "doi": "10.1111/j.2517-6161.1977.tb01600.x",
    "problem": "The paper addresses the challenge of computing maximum likelihood estimates when dealing with incomplete data. This is a common issue in statistical analysis, especially in cases involving missing values, grouped, censored, or truncated data. The lack of complete data complicates the estimation process and can affect the accuracy of statistical models.",
    "solution": "The authors propose a broadly applicable algorithm designed to compute maximum likelihood estimates from incomplete data. Theoretical results are provided to demonstrate the monotone behavior of the likelihood function and the convergence of the algorithm. The approach is illustrated through various examples, including applications to mixture models, variance component estimation, and factor analysis.",
    "year": 1977,
    "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
    "citations": {
      "total": 23283,
      "supporting": 27,
      "contradicting": 2,
      "mentioning": 22884,
      "unclassified": 370,
      "citingPublications": 44904
    }
  },
  {
    "query": "Markov Chains",
    "title": "MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space",
    "doi": "10.1093/sysbio/sys029",
    "problem": "There is a need for efficient, accurate, and user-friendly software for Bayesian phylogenetic inference using Markov chain Monte Carlo (MCMC) methods. Previous versions of MrBayes lacked certain features such as advanced convergence diagnostics, parallel analysis capabilities, and support for newer models and faster likelihood calculations. Additionally, there was a demand for improved convergence, expanded output options, and the ability to handle long or interrupted analyses.",
    "solution": "The release of MrBayes version 3.2 addresses these issues by introducing convergence diagnostics, parallel analysis with real-time monitoring, and automatic optimization of tuning parameters. The new version offers significantly faster likelihood calculations using SSE and BEAGLE, supports checkpointing, and includes new models and output options. It also incorporates advanced methods for species tree inference and accurate estimation of marginal model likelihoods, enhancing both performance and usability.",
    "year": 2012,
    "journal": "Systematic Biology",
    "citations": {
      "total": 15517,
      "supporting": 30,
      "contradicting": 0,
      "mentioning": 15422,
      "unclassified": 65,
      "citingPublications": 24637
    }
  },
  {
    "query": "Markov Chains",
    "title": "Mixed effects models and extensions in ecology with R",
    "doi": "10.1007/978-0-387-87458-6",
    "problem": "There are several common and complex issues in the analysis of ecological data, such as nested data structures, heterogeneity of variance, spatial and temporal correlation, and zero-inflated data, that require more detailed description and analysis. Existing resources do not provide sufficient depth or practical guidance on these topics, particularly in the context of applied statistics in ecology.",
    "solution": "The authors have written a comprehensive book that addresses these issues in detail, including a dedicated chapter on Bayesian Monte-Carlo Markov Chain applications in generalized linear modelling. The book combines case studies, theoretical explanations, and practical R code to guide readers through the analysis of ecological data, providing both introductory material and practical advice for writing scientific papers.",
    "year": 2009,
    "journal": "",
    "citations": {
      "total": 13407,
      "supporting": 35,
      "contradicting": 2,
      "mentioning": 13238,
      "unclassified": 132,
      "citingPublications": 17055
    }
  },
  {
    "query": "Markov Chains",
    "title": "Stellar population synthesis at the resolution of 2003",
    "doi": "10.1046/j.1365-8711.2003.06897.x",
    "problem": "Existing models for computing the spectral evolution of stellar populations are limited in age range, wavelength coverage, resolution, and accuracy, particularly for varying metallicities and complex evolutionary phases. There is a need for a model that can accurately reproduce observed spectra and color-magnitude diagrams of star clusters and galaxies across a wide range of ages and metallicities. Additionally, current models struggle to constrain physical parameters such as star formation history, metallicity, and dust content from observed galaxy spectra.",
    "solution": "The authors propose a new model that computes the spectral evolution of stellar populations from 100,000 years to 20 billion years at high resolution across a broad wavelength range, using a newly available library of observed stellar spectra. The model incorporates recent advances in stellar evolution theory and an observationally motivated treatment of thermally-pulsing asymptotic giant branch stars, validated by observations. This model accurately reproduces observed optical and near-infrared color-magnitude diagrams, integrated colors of star clusters, and galaxy spectra, enabling precise studies of absorption-line strengths and the derivation of key physical parameters from galaxy spectra.",
    "year": 2003,
    "journal": "Monthly Notices of the Royal Astronomical Society",
    "citations": {
      "total": 12837,
      "supporting": 255,
      "contradicting": 16,
      "mentioning": 12559,
      "unclassified": 7,
      "citingPublications": 10275
    }
  },
  {
    "query": "Markov Chains",
    "title": "A Simple, Fast, and Accurate Algorithm to Estimate Large Phylogenies by Maximum Likelihood",
    "doi": "10.1080/10635150390235520",
    "problem": "The growing size of data sets and the increasing complexity of probabilistic sequence evolution models have created a need for fast and reliable methods for phylogeny reconstruction. Existing maximum-likelihood methods are often computationally intensive, while distance-based and parsimony approaches may lack accuracy. There is a demand for a method that combines high accuracy with significant reductions in computing time.",
    "solution": "The authors propose a new phylogeny reconstruction approach based on the maximum-likelihood principle, utilizing a simple hill-climbing algorithm that simultaneously adjusts tree topology and branch lengths. Starting from an initial tree generated by a fast distance-based method, the algorithm iteratively improves the tree's likelihood, requiring only a few iterations to reach an optimum. This method, implemented in the PHYML program, achieves accuracy comparable to or better than existing maximum-likelihood programs, with dramatically reduced computing time.",
    "year": 2003,
    "journal": "Systematic Biology",
    "citations": {
      "total": 11613,
      "supporting": 23,
      "contradicting": 1,
      "mentioning": 11526,
      "unclassified": 63,
      "citingPublications": 16147
    }
  },
  {
    "query": "Markov Chains",
    "title": "BEAST: Bayesian evolutionary analysis by sampling trees",
    "doi": "10.1186/1471-2148-7-214",
    "problem": "The evolutionary analysis of molecular sequence variation requires sophisticated statistical methods, as reflected in the growing use of probabilistic models for tasks such as phylogenetic inference, multiple sequence alignment, and molecular population genetics. However, there is a need for flexible and efficient tools that can implement a wide range of stochastic models for these analyses.",
    "solution": "The authors present BEAST, a fast and flexible software architecture designed for Bayesian analysis of molecular sequences related by evolutionary trees. BEAST provides a large number of popular stochastic models of sequence evolution and implements tree-based models suitable for both within- and between-species sequence data.",
    "year": 2007,
    "journal": "BMC Evolutionary Biology",
    "citations": {
      "total": 11154,
      "supporting": 37,
      "contradicting": 1,
      "mentioning": 11057,
      "unclassified": 59,
      "citingPublications": 12317
    }
  },
  {
    "query": "Markov Chains",
    "title": "<b>mice</b>: Multivariate Imputation by <strong class=\"highlight\">Chained</strong> Equations in<i>R</i>",
    "doi": "10.18637/jss.v045.i03",
    "problem": "Handling incomplete multivariate data is a common challenge in statistical analysis, requiring effective imputation methods. Existing tools, such as the original mice package, had limitations in model generality, pooling, and handling of complex data structures. There was also a need for improved methods for imputing categorical data and better support for multilevel data and diagnostic tools.",
    "solution": "The mice 2.9 package extends the original functionality by providing a more general framework for analyzing imputed data and broadening the range of models for pooling. It introduces new features such as multilevel data imputation, automatic predictor selection, enhanced data handling, specialized pooling routines, and improved imputation for categorical data. The article offers a practical, step-by-step guide to applying these new tools to solve real-world incomplete data problems.",
    "year": 2011,
    "journal": "Journal of Statistical Software",
    "citations": {
      "total": 10513,
      "supporting": 23,
      "contradicting": 2,
      "mentioning": 10430,
      "unclassified": 58,
      "citingPublications": 13692
    }
  },
  {
    "query": "Markov Chains",
    "title": "Bayesian Measures of Model Complexity and Fit",
    "doi": "10.1111/1467-9868.00353",
    "problem": "The paper addresses the challenge of comparing complex hierarchical models where the number of parameters is not clearly defined. Traditional model comparison criteria may not be directly applicable in such cases, especially in a Bayesian context. There is a need for a practical and theoretically justified measure to assess model fit and complexity.",
    "solution": "The authors propose an information-theoretic measure, p D, for the effective number of parameters, defined as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters. They introduce the deviance information criterion (DIC), which combines p D with the posterior mean deviance, providing a Bayesian model comparison tool that is easy to compute in Markov chain Monte Carlo analyses. The properties of this measure are explored, and its use is illustrated with examples and comparisons to alternative approaches.",
    "year": 2002,
    "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
    "citations": {
      "total": 9532,
      "supporting": 40,
      "contradicting": 4,
      "mentioning": 9385,
      "unclassified": 103,
      "citingPublications": 11856
    }
  },
  {
    "query": "Markov Chains",
    "title": "Bayesian Phylogenetics with BEAUti and the BEAST 1.7",
    "doi": "10.1093/molbev/mss075",
    "problem": "Computational evolutionary biology, statistical phylogenetics, and coalescent-based population genetics require advanced tools for analyzing and understanding molecular sequence data. Existing methods for Bayesian phylogenetic inference and related analyses are often complex and not easily accessible to non-developers. There is a need for user-friendly software that implements sophisticated Markov chain Monte Carlo algorithms for these applications.",
    "solution": "The BEAST software package version 1.7 is introduced, implementing a family of Markov chain Monte Carlo algorithms for Bayesian phylogenetic inference, divergence time dating, coalescent analysis, and phylogeography. It includes an enhanced graphical user interface, BEAUti, which provides access to advanced evolutionary models and tools for visualizing and summarizing analyses. The package is open source and freely available to the scientific community.",
    "year": 2012,
    "journal": "Molecular Biology and Evolution",
    "citations": {
      "total": 9109,
      "supporting": 18,
      "contradicting": 1,
      "mentioning": 9046,
      "unclassified": 44,
      "citingPublications": 9891
    }
  },
  {
    "query": "Bell Curve",
    "title": "Maps of Dust Infrared Emission for Use in Estimation of Reddening and Cosmic Microwave Background Radiation Foregrounds",
    "doi": "10.1086/305772",
    "problem": "Existing full-sky dust maps suffer from contamination by zodiacal light, cosmic infrared background, and point sources, as well as artifacts from the IRAS scan pattern, leading to inaccuracies in estimating Galactic extinction and dust column density. Previous reddening estimates, such as the Burstein-Heiles maps, are less accurate, especially in regions of low, moderate, and high reddening. There is also a need for improved maps to aid in cosmic microwave background and X-ray studies.",
    "solution": "The authors present a reprocessed full-sky 100 μm map that combines COBE/DIRBE and IRAS/ISSA data, with zodiacal foreground and confirmed point sources removed, and artifacts corrected. They use dust temperature maps derived from DIRBE data to convert the 100 μm map to one proportional to dust column density, and calibrate the maps using the colors of elliptical galaxies. The resulting maps have DIRBE calibration and IRAS resolution, are twice as accurate as previous estimates in low and moderate reddening regions, and are expected to be significantly more accurate in high-reddening areas, with easy public access for general use.",
    "year": 1998,
    "journal": "The Astrophysical Journal",
    "citations": {
      "total": 17520,
      "supporting": 592,
      "contradicting": 71,
      "mentioning": 16836,
      "unclassified": 21,
      "citingPublications": 14540
    }
  },
  {
    "query": "Bell Curve",
    "title": "Maximum entropy modeling of species geographic distributions",
    "doi": "10.1016/j.ecolmodel.2005.03.026",
    "problem": "Predictive modeling of species' environmental requirements and geographic distributions is hindered by the lack of absence data for most species, limiting the use of standard statistical techniques. While detailed presence/absence data exist for some species, most datasets only contain presence information, making accurate modeling challenging.",
    "solution": "The paper introduces the use of the maximum entropy method (Maxent), a machine learning approach, for modeling species distributions using presence-only data. Maxent is shown to provide superior discrimination of suitable versus unsuitable areas compared to existing methods, and can be effectively applied to many presence-only datasets, warranting further research and development.",
    "year": 2006,
    "journal": "Ecological Modelling",
    "citations": {
      "total": 16098,
      "supporting": 39,
      "contradicting": 2,
      "mentioning": 15346,
      "unclassified": 711,
      "citingPublications": 15801
    }
  },
  {
    "query": "Bell Curve",
    "title": "Stellar population synthesis at the resolution of 2003",
    "doi": "10.1046/j.1365-8711.2003.06897.x",
    "problem": "Existing models for computing the spectral evolution of stellar populations are limited in age range, spectral resolution, and wavelength coverage, and often lack accurate treatment of certain stellar evolutionary phases. These limitations hinder the ability to reproduce observed properties of star clusters and galaxies, and to constrain key physical parameters such as star formation history, metallicity, and dust content. There is a need for a model that can accurately simulate the spectral evolution of stellar populations across a wide range of ages and metallicities.",
    "solution": "The authors present a new model that computes the spectral evolution of stellar populations from 100,000 years to 20 billion years at high resolution across a wide wavelength range, using a newly available library of observed stellar spectra. The model incorporates recent advances in stellar evolution theory and an observationally motivated treatment of thermally-pulsing asymptotic giant branch stars, enabling accurate reproduction of observed colour-magnitude diagrams and galaxy spectra. This approach allows for detailed studies of absorption-line strengths and provides constraints on physical parameters such as star formation history, metallicity, and dust content in galaxies.",
    "year": 2003,
    "journal": "Monthly Notices of the Royal Astronomical Society",
    "citations": {
      "total": 12837,
      "supporting": 255,
      "contradicting": 16,
      "mentioning": 12559,
      "unclassified": 7,
      "citingPublications": 10275
    }
  },
  {
    "query": "Bell Curve",
    "title": "The ERA5 global reanalysis",
    "doi": "10.1002/qj.3803",
    "problem": "There is a need for a comprehensive and detailed global reanalysis dataset that accurately represents the atmosphere, land surface, and ocean waves over several decades. The existing ERA-Interim reanalysis, started in 2006 and spanning from 1979 onwards, has limitations in spatial and temporal resolution as well as in data assimilation techniques. Improved accuracy and uncertainty estimates are required for better weather and climate analysis.",
    "solution": "The ERA5 reanalysis, produced by ECMWF within the Copernicus Climate Change Service, replaces ERA-Interim and provides a detailed record from 1950 onwards using advanced model physics, core dynamics, and data assimilation. ERA5 offers significantly enhanced horizontal resolution (31 km), hourly outputs, and uncertainty estimates from an ensemble, resulting in improved fit to observational data and better representation of weather systems. The paper describes ERA5's setup and performance, demonstrating gains in forecast skill and improved agreement with independent datasets.",
    "year": 2020,
    "journal": "Quarterly Journal of the Royal Meteorological Society",
    "citations": {
      "total": 12144,
      "supporting": 117,
      "contradicting": 6,
      "mentioning": 11971,
      "unclassified": 50,
      "citingPublications": 23614
    }
  },
  {
    "query": "Bell Curve",
    "title": "The Chemical Composition of the Sun",
    "doi": "10.1146/annurev.astro.46.060407.145222",
    "problem": "Accurately determining the solar chemical composition is crucial for understanding the Sun and solar system, and for comparing the elemental contents of other astronomical objects. However, existing values for solar elemental abundances, particularly for elements like carbon, nitrogen, oxygen, and neon, may be outdated or inconsistent. There is also a notable conflict between new abundance determinations and standard models of the solar interior as inferred from helioseismology.",
    "solution": "The researchers present a redetermination of nearly all available solar elemental abundances using a new, realistic 3-dimensional, time-dependent hydrodynamical model of the solar atmosphere. They carefully consider atomic data, spectral line selection, and departures from LTE to compile a comprehensive and homogeneous set of solar abundances. This new composition achieves high internal consistency and agreement with external measurements, although it highlights a significant discrepancy with helioseismological models.",
    "year": 2009,
    "journal": "Annual Review of Astronomy and Astrophysics",
    "citations": {
      "total": 10121,
      "supporting": 503,
      "contradicting": 49,
      "mentioning": 9549,
      "unclassified": 20,
      "citingPublications": 9144
    }
  },
  {
    "query": "Bell Curve",
    "title": "A Universal Density Profile from Hierarchical Clustering",
    "doi": "10.1086/304888",
    "problem": "The study addresses the challenge of understanding the equilibrium density profiles of dark matter halos in hierarchically clustering universes, specifically whether these profiles depend on halo mass, the initial density fluctuation spectrum, or cosmological parameters. There is also a need to accurately characterize these profiles and their dependence on formation history and cosmology.",
    "solution": "The authors use high-resolution N-body simulations to show that all dark matter halos share a universal density profile shape, regardless of mass or cosmological parameters. They fit these profiles with a simple formula and demonstrate a strong correlation between halo mass and characteristic density, linked to the universe's density at the time of halo assembly. Additionally, they provide an analytic procedure based on the Press-Schechter formalism to calculate accurate equilibrium profiles for any hierarchical model.",
    "year": 1997,
    "journal": "The Astrophysical Journal",
    "citations": {
      "total": 9518,
      "supporting": 253,
      "contradicting": 24,
      "mentioning": 9219,
      "unclassified": 22,
      "citingPublications": 9947
    }
  },
  {
    "query": "Bell Curve",
    "title": "Adolescence-limited and life-course-persistent antisocial behavior: A developmental taxonomy.",
    "doi": "10.1037/0033-295x.100.4.674",
    "problem": "Antisocial behavior presents a paradox: it remains consistent within individuals over time, yet its overall prevalence in the population increases dramatically during adolescence before declining. This raises questions about the underlying causes and developmental pathways of antisocial behavior.",
    "solution": "The article proposes a dual taxonomy, distinguishing between two groups: a small group with life-course-persistent antisocial behavior due to neuropsychological problems and adverse environments, and a larger group with adolescence-limited antisocial behavior driven by the maturity gap and social mimicry. This framework reconciles the observed continuity and prevalence changes by attributing them to distinct etiologies and developmental courses.",
    "year": 1993,
    "journal": "Psychological Review",
    "citations": {
      "total": 8073,
      "supporting": 570,
      "contradicting": 49,
      "mentioning": 7126,
      "unclassified": 328,
      "citingPublications": 8702
    }
  },
  {
    "query": "Bell Curve",
    "title": "A comparative risk assessment of burden of disease and injury attributable to 67 risk factors and risk factor clusters in 21 regions, 1990–2010: a systematic analysis for the Global Burden of Disease Study 2010",
    "doi": "10.1016/s0140-6736(12)61766-8",
    "problem": "There has not been a comprehensive revision of the global disease burden caused by risk factors since 2000, and previous analyses have not assessed how the burden attributable to these risk factors has changed over time. This gap limits the ability to inform prevention strategies with up-to-date and comparative data on health loss from various risks.",
    "solution": "The study systematically estimated deaths and disability-adjusted life years (DALYs) attributable to 67 risk factors across 21 regions for the years 1990 and 2010, using a combination of exposure data, relative risks, and cause-specific burden estimates. By comparing these values over time and across regions, the analysis provides an updated and comprehensive account of how the contribution of different risk factors to global disease burden has changed.",
    "year": 2012,
    "journal": "The Lancet",
    "citations": {
      "total": 8025,
      "supporting": 98,
      "contradicting": 9,
      "mentioning": 7641,
      "unclassified": 277,
      "citingPublications": 11340
    }
  },
  {
    "query": "Bell Curve",
    "title": "ANFIS: adaptive-network-based fuzzy inference system",
    "doi": "10.1109/21.256541",
    "problem": "The challenge addressed is how to effectively construct input-output mappings for complex systems by leveraging both human knowledge in the form of fuzzy if-then rules and empirical input-output data. Existing approaches such as artificial neural networks and traditional fuzzy modeling have limitations in integrating these two sources of information. There is also a need for improved modeling of nonlinear functions and components, as well as better prediction of chaotic time series.",
    "solution": "The paper proposes ANFIS (Adaptive-Network-based Fuzzy Inference System), which combines the framework of adaptive networks with fuzzy inference systems using a hybrid learning procedure. ANFIS enables the construction of input-output mappings by integrating human knowledge and data-driven learning. The architecture is demonstrated to effectively model nonlinear functions, identify nonlinear system components online, and predict chaotic time series, outperforming previous methods.",
    "year": 1993,
    "journal": "Ieee Transactions on Systems Man and Cybernetics",
    "citations": {
      "total": 7726,
      "supporting": 4,
      "contradicting": 1,
      "mentioning": 7384,
      "unclassified": 337,
      "citingPublications": 14975
    }
  },
  {
    "query": "Bell Curve",
    "title": "Novel methods improve prediction of species’ distributions from occurrence data",
    "doi": "10.1111/j.2006.0906-7590.04596.x",
    "problem": "There is a lack of effective guidance on how to best use the increasing amount of species occurrence data from museums and herbaria for predicting species' distributions, given the numerous available modelling approaches. Existing methods may not be well suited to the noisy or sparse nature of such data. This creates a challenge for applications in ecology, evolution, and conservation science.",
    "solution": "The study compared 16 modelling methods, including both well-established and novel approaches such as machine-learning and community models, across 226 species from 6 regions. By evaluating these methods using presence-only and independent presence-absence data, the research found that novel methods consistently outperformed established ones. This suggests that improved modelling techniques are promising for effectively utilizing museum and herbarium data despite its inherent noise.",
    "year": 2006,
    "journal": "Ecography",
    "citations": {
      "total": 7488,
      "supporting": 121,
      "contradicting": 14,
      "mentioning": 7137,
      "unclassified": 216,
      "citingPublications": 8282
    }
  },
  {
    "query": "SIR",
    "title": "Fiji: an open-source platform for biological-image analysis",
    "doi": "10.1038/nmeth.2019",
    "problem": "There is a need for effective tools to support biological image analysis and to facilitate collaboration between computer science and biology research communities. Existing solutions may lack modern software engineering practices or easy ways to share new algorithms with end users.",
    "solution": "Fiji is proposed as a distribution of ImageJ that integrates powerful software libraries, supports multiple scripting languages, and enables rapid prototyping of image processing algorithms. It also provides an integrated update system for sharing plugins, thus fostering productive collaboration between researchers.",
    "year": 2012,
    "journal": "Nature Chemical Biology",
    "citations": {
      "total": 50316,
      "supporting": 86,
      "contradicting": 5,
      "mentioning": 50104,
      "unclassified": 121,
      "citingPublications": 60999
    }
  },
  {
    "query": "SIR",
    "title": "A short history ofSHELX",
    "doi": "10.1107/s0108767307043930",
    "problem": "The paper addresses the challenge of maintaining and evolving the SHELX system of computer programs, originally designed for outdated hardware and data formats, to remain relevant and effective in modern crystallographic applications. It also examines the limitations, missed opportunities, and areas for improvement in the software. Additionally, it considers why SHELX programs continue to be widely used despite the availability of objectively superior alternatives.",
    "solution": "The solution involves the continuous development and innovation within the SHELX suite, introducing new features and adapting the software for modern use cases such as high-throughput and macromolecular phasing. The paper provides a critical analysis of both successful and less-successful features, and suggests improvements for future releases. The open-source nature and integration of various SHELX programs into crystallographic pipelines ensure their ongoing utility and relevance.",
    "year": 2007,
    "journal": "Acta Crystallographica Section a Foundations of Crystallography",
    "citations": {
      "total": 41114,
      "supporting": 144,
      "contradicting": 2,
      "mentioning": 40783,
      "unclassified": 185,
      "citingPublications": 86469
    }
  },
  {
    "query": "SIR",
    "title": "A pneumonia outbreak associated with a new coronavirus of probable bat origin",
    "doi": "10.1038/s41586-020-2012-7",
    "problem": "The study addresses the need to understand which human cell surface receptors the 2019-nCoV (now known as SARS-CoV-2) virus can use to infect cells. Specifically, it examines the infectivity of the virus in cells expressing different candidate receptors.",
    "solution": "The researchers tested virus infectivity in HeLa cells engineered to express human ACE2, APN, or DPP4 receptors. They used tagged plasmids and immunofluorescence to detect the presence of these receptors and viral proteins, allowing them to analyze which receptors facilitate viral entry.",
    "year": 2020,
    "journal": "Nature",
    "citations": {
      "total": 23058,
      "supporting": 260,
      "contradicting": 15,
      "mentioning": 21997,
      "unclassified": 786,
      "citingPublications": 22452
    }
  },
  {
    "query": "SIR",
    "title": "The Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3)",
    "doi": "10.1001/jama.2016.0287",
    "problem": "Previous definitions of sepsis and septic shock, including the systemic inflammatory response syndrome (SIRS) criteria, were limited by an excessive focus on inflammation, lack of specificity and sensitivity, and inconsistent terminology. These shortcomings led to discrepancies in reported incidence and mortality rates, as well as confusion in clinical and epidemiological settings. The outdated definitions failed to adequately capture the complexity and severity of sepsis and septic shock.",
    "solution": "A task force convened by critical care societies developed updated definitions and clinical criteria for sepsis and septic shock, emphasizing life-threatening organ dysfunction caused by a dysregulated host response to infection. The new criteria include using an increase in SOFA score for organ dysfunction, specific clinical markers for septic shock, and the introduction of the quickSOFA (qSOFA) score for rapid bedside assessment. These updates aim to improve consistency in diagnosis, facilitate earlier recognition and management, and enhance comparability across studies and clinical trials.",
    "year": 2016,
    "journal": "Jama",
    "citations": {
      "total": 22332,
      "supporting": 187,
      "contradicting": 22,
      "mentioning": 20960,
      "unclassified": 1163,
      "citingPublications": 22751
    }
  },
  {
    "query": "SIR",
    "title": "The Hallmarks of Aging",
    "doi": "10.1016/j.cell.2013.05.039",
    "problem": "Aging leads to a progressive loss of physiological integrity, resulting in impaired function and increased vulnerability to death, and is the primary risk factor for major diseases such as cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Understanding the underlying mechanisms of aging is complex due to the involvement of multiple interconnected genetic and biochemical pathways. A major challenge is to determine how these pathways contribute to aging and to identify effective interventions.",
    "solution": "The review proposes a framework of nine tentative hallmarks that represent common features of aging across different organisms, especially mammals. By dissecting the interconnectedness and relative contributions of these hallmarks, researchers aim to identify pharmaceutical targets that can improve human health during aging with minimal side effects.",
    "year": 2013,
    "journal": "Cell",
    "citations": {
      "total": 12934,
      "supporting": 178,
      "contradicting": 13,
      "mentioning": 12532,
      "unclassified": 211,
      "citingPublications": 13279
    }
  },
  {
    "query": "SIR",
    "title": "The Structure and Function of Complex Networks",
    "doi": "10.1137/s003614450342480",
    "problem": "Researchers face challenges in understanding and predicting the behavior of complex networked systems such as the Internet, social networks, and biological networks. These systems exhibit intricate structural and dynamic properties that require specialized models and techniques for analysis.",
    "solution": "The paper reviews recent developments in the study of networked systems, including concepts like the small-world effect, degree distributions, clustering, network correlations, and random graph models. It also discusses models of network growth, preferential attachment, and the analysis of dynamical processes occurring on networks.",
    "year": 2003,
    "journal": "Siam Review",
    "citations": {
      "total": 12904,
      "supporting": 148,
      "contradicting": 11,
      "mentioning": 12318,
      "unclassified": 427,
      "citingPublications": 17092
    }
  },
  {
    "query": "SIR",
    "title": "Overview of theCCP4 suite and current developments",
    "doi": "10.1107/s0907444910045749",
    "problem": "Macromolecular structure determination by X-ray crystallography is a complex process that requires the integration of multiple computational tools and methods. Researchers face challenges in managing diverse software programs, data formats, and workflows necessary for structure solution. There is a need for a unified and flexible software suite to streamline and automate these processes.",
    "solution": "The CCP4 software suite provides a comprehensive collection of programs, data, and libraries for macromolecular structure determination by X-ray crystallography. It connects diverse programs through a common infrastructure of standard file formats, data objects, and graphical interfaces, and includes automation pipelines to facilitate increasingly automated structure solution. The suite is designed to be flexible and user-friendly, enabling researchers to efficiently achieve their aims.",
    "year": 2011,
    "journal": "Acta Crystallographica Section D Biological Crystallography",
    "citations": {
      "total": 10076,
      "supporting": 15,
      "contradicting": 0,
      "mentioning": 10035,
      "unclassified": 26,
      "citingPublications": 12280
    }
  },
  {
    "query": "SIR",
    "title": "Chromatin Modifications and Their Function",
    "doi": "10.1016/j.cell.2007.02.005",
    "problem": "The surface of nucleosomes contains a variety of histone modifications, with at least eight different classes and many modification sites, but the functional significance and mechanisms by which these modifications influence chromatin structure and biological processes are not fully understood.",
    "solution": "The study proposes that histone modifications function by either disrupting chromatin contacts or by affecting the recruitment of nonhistone proteins to chromatin, thereby dictating higher-order chromatin structure and orchestrating the recruitment of enzyme complexes to manipulate DNA. This mechanism allows histone modifications to influence fundamental biological processes, some of which may be epigenetically inherited.",
    "year": 2007,
    "journal": "Cell",
    "citations": {
      "total": 9377,
      "supporting": 101,
      "contradicting": 8,
      "mentioning": 9094,
      "unclassified": 174,
      "citingPublications": 9972
    }
  },
  {
    "query": "SIR",
    "title": "Development of a new resilience scale: The Connor-Davidson Resilience Scale (CD-RISC)",
    "doi": "10.1002/da.10113",
    "problem": "Resilience, as a measure of stress coping ability, is an important target for treatment in anxiety, depression, and stress-related disorders, but there is a need for a reliable and valid tool to assess it. Existing measures may not adequately capture changes in resilience or its relationship to treatment outcomes.",
    "solution": "The authors developed the Connor-Davidson Resilience Scale (CD-RISC), a 25-item rating scale to assess resilience, and evaluated its psychometric properties across various clinical and community samples. The scale demonstrated good reliability, validity, and sensitivity to treatment effects, indicating that resilience is measurable and modifiable through intervention.",
    "year": 2003,
    "journal": "Depression and Anxiety",
    "citations": {
      "total": 8428,
      "supporting": 179,
      "contradicting": 56,
      "mentioning": 7727,
      "unclassified": 466,
      "citingPublications": 9411
    }
  },
  {
    "query": "Logistic/Population",
    "title": "Deep Residual Learning for Image Recognition",
    "doi": "10.1109/cvpr.2016.90",
    "problem": "Training deeper neural networks is challenging, as increasing depth often makes optimization more difficult and can lead to reduced performance. Existing architectures struggle to efficiently learn as network depth increases. This limits the potential accuracy gains from using very deep networks for visual recognition tasks.",
    "solution": "The authors propose a residual learning framework that reformulates network layers to learn residual functions with reference to the layer inputs, rather than unreferenced functions. This approach makes it easier to train substantially deeper networks, enabling successful optimization and improved accuracy. Their residual networks achieve state-of-the-art results on benchmarks such as ImageNet and COCO, demonstrating the effectiveness of extremely deep representations.",
    "year": 2016,
    "journal": "",
    "citations": {
      "total": 120349,
      "supporting": 309,
      "contradicting": 22,
      "mentioning": 119363,
      "unclassified": 655,
      "citingPublications": 206992
    }
  },
  {
    "query": "Logistic/Population",
    "title": "Using thematic analysis in psychology",
    "doi": "10.1191/1478088706qp063oa",
    "problem": "There is a lack of explicit regulation and limited awareness among occupational health and safety professionals regarding chemical exposures from pollutants accumulating in closed environments during the transportation and storage of non-dangerous goods. This leads to inadequate risk assessments and underestimation of potential health effects. The issue is exacerbated by increasing global transportation, climate change, and the need for more frequent fumigation.",
    "solution": "The study suggests that regulations concerning chemical safety in the transportation and storage of non-dangerous goods should be made more specific. Additionally, responsible professionals need improved education and information about the potential workplace hazards associated with chemical pollutants in these environments.",
    "year": 2006,
    "journal": "Qualitative Research in Psychology",
    "citations": {
      "total": 107327,
      "supporting": 187,
      "contradicting": 5,
      "mentioning": 104576,
      "unclassified": 2559,
      "citingPublications": 141314
    }
  },
  {
    "query": "Logistic/Population",
    "title": "Global Cancer Statistics 2020: GLOBOCAN Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries",
    "doi": "10.3322/caac.21660",
    "problem": "The global cancer burden is substantial and rising, with an estimated 19.3 million new cases and 10 million deaths in 2020, and projections indicating a 47% increase in cases by 2040. There are significant disparities in cancer incidence and mortality between transitioned and transitioning countries, with transitioning countries facing higher death rates for certain cancers and expected to experience a larger increase in cancer cases. These trends are driven by demographic changes and increasing risk factors related to globalization and economic growth.",
    "solution": "Building a sustainable infrastructure for the dissemination of cancer prevention measures and the provision of cancer care in transitioning countries is critical. Such efforts are essential for effective global cancer control and to address the growing and uneven cancer burden worldwide.",
    "year": 2021,
    "journal": "Ca a Cancer Journal for Clinicians",
    "citations": {
      "total": 68019,
      "supporting": 246,
      "contradicting": 32,
      "mentioning": 66379,
      "unclassified": 1362,
      "citingPublications": 91700
    }
  },
  {
    "query": "Logistic/Population",
    "title": "Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries",
    "doi": "10.3322/caac.21492",
    "problem": "There is a significant global burden of cancer, with substantial geographic variability in incidence and mortality across different world regions. Accurate and high-quality cancer registry data, essential for planning and implementing effective cancer control programs, are lacking in most low- and middle-income countries. This lack of data hampers the ability to prioritize and evaluate national cancer control efforts effectively.",
    "solution": "The Global Initiative for Cancer Registry Development is an international partnership that supports the improvement of cancer data collection and estimation, particularly in low- and middle-income countries. By enhancing the availability and quality of local cancer registry data, this initiative aims to facilitate better prioritization and evaluation of national cancer control programs.",
    "year": 2018,
    "journal": "Ca a Cancer Journal for Clinicians",
    "citations": {
      "total": 60748,
      "supporting": 282,
      "contradicting": 29,
      "mentioning": 58657,
      "unclassified": 1780,
      "citingPublications": 80114
    }
  },
  {
    "query": "Logistic/Population",
    "title": "Hallmarks of Cancer: The Next Generation",
    "doi": "10.1016/j.cell.2011.02.013",
    "problem": "Cancer is a complex disease characterized by multiple biological capabilities, known as hallmarks, that are acquired during tumor development. These hallmarks, along with underlying factors like genome instability and inflammation, make understanding and treating cancer challenging. Additionally, the tumor microenvironment, composed of recruited normal cells, adds further complexity to the disease.",
    "solution": "The authors propose using the concept of cancer hallmarks as an organizing principle to rationalize the complexities of neoplastic disease. By recognizing and categorizing these hallmarks, including newly identified ones like reprogramming energy metabolism and evading immune destruction, researchers can better understand tumor biology and develop new, more effective cancer treatments.",
    "year": 2011,
    "journal": "Cell",
    "citations": {
      "total": 51167,
      "supporting": 583,
      "contradicting": 21,
      "mentioning": 49690,
      "unclassified": 873,
      "citingPublications": 59730
    }
  },
  {
    "query": "Logistic/Population",
    "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
    "doi": "10.1037/0021-9010.88.5.879",
    "problem": "There is a longstanding issue of method biases affecting results in behavioral sciences research. Despite its significance, there is no comprehensive summary detailing the sources of these biases or how to control them.",
    "solution": "The article aims to examine the impact of method biases on behavioral research, identify their sources, and discuss the cognitive processes involved. It also evaluates various procedural and statistical techniques to control for method biases and provides recommendations for selecting appropriate remedies in different research settings.",
    "year": 2003,
    "journal": "Journal of Applied Psychology",
    "citations": {
      "total": 51016,
      "supporting": 569,
      "contradicting": 162,
      "mentioning": 49580,
      "unclassified": 705,
      "citingPublications": 63567
    }
  },
  {
    "query": "Logistic/Population",
    "title": "The Measurement of Observer Agreement for Categorical Data",
    "doi": "10.2307/2529310",
    "problem": "The paper addresses the challenge of analyzing multivariate categorical data from observer reliability studies, specifically focusing on assessing the extent of agreement among observers. There is a need for statistical methods to test for interobserver bias and to measure interobserver agreement in such studies.",
    "solution": "The authors propose a general statistical methodology that constructs functions of observed proportions to evaluate observer agreement and develops test statistics for related hypotheses. They introduce tests for interobserver bias based on first-order marginal homogeneity and create generalized kappa-type statistics to measure agreement. The methodology is demonstrated using a clinical diagnosis example from epidemiological research.",
    "year": 1977,
    "journal": "Biometrics",
    "citations": {
      "total": 42579,
      "supporting": 791,
      "contradicting": 72,
      "mentioning": 39999,
      "unclassified": 1717,
      "citingPublications": 69054
    }
  },
  {
    "query": "Logistic/Population",
    "title": "Clinical course and risk factors for mortality of adult inpatients with COVID-19 in Wuhan, China: a retrospective cohort study",
    "doi": "10.1016/s0140-6736(20)30566-3",
    "problem": "Although epidemiological and clinical characteristics of COVID-19 patients have been reported, the specific risk factors for mortality and the detailed clinical course of illness, including viral shedding, have not been well described. There is a need to identify which factors are associated with in-hospital death among COVID-19 patients.",
    "solution": "The study conducted a retrospective, multicentre cohort analysis using logistic regression to identify risk factors associated with in-hospital death, finding that older age, higher SOFA score, and elevated d-dimer levels on admission are significant predictors. These findings can help clinicians identify patients at higher risk of poor prognosis early, and the observation of prolonged viral shedding supports strategies for patient isolation and targeted antiviral interventions.",
    "year": 2020,
    "journal": "The Lancet",
    "citations": {
      "total": 34087,
      "supporting": 2196,
      "contradicting": 405,
      "mentioning": 29572,
      "unclassified": 1914,
      "citingPublications": 28372
    }
  },
  {
    "query": "Logistic/Population",
    "title": "ImageNet: A large-scale hierarchical image database",
    "doi": "10.1109/cvpr.2009.5206848",
    "problem": "The rapid growth of image data on the Internet presents an opportunity to develop better models and algorithms for indexing, retrieving, organizing, and interacting with images and multimedia data. However, effectively harnessing and organizing this vast amount of image data remains a significant challenge. Existing image datasets are limited in scale, diversity, and accuracy.",
    "solution": "The authors propose ImageNet, a large-scale ontology of images constructed on the WordNet semantic hierarchy, aiming to populate most of its 80,000 synsets with 500-1000 high-quality images each. They detail the data collection process using Amazon Mechanical Turk and demonstrate ImageNet's superior scale, diversity, and accuracy compared to existing datasets. The paper also illustrates the utility of ImageNet in applications such as object recognition, image classification, and object clustering.",
    "year": 2009,
    "journal": "",
    "citations": {
      "total": 33724,
      "supporting": 59,
      "contradicting": 2,
      "mentioning": 33465,
      "unclassified": 198,
      "citingPublications": 59781
    }
  },
  {
    "query": "Queue Model",
    "title": "MEME SUITE: tools for motif discovery and searching",
    "doi": "10.1093/nar/gkp335",
    "problem": "Researchers face challenges in discovering and analyzing sequence motifs, such as DNA binding sites and protein interaction domains, across large DNA and protein sequence databases. Existing tools may lack unified access, support for motifs with gaps, or integrated downstream analysis capabilities. There is a need for a comprehensive, user-friendly platform that streamlines motif discovery and subsequent analyses.",
    "solution": "The MEME Suite web server provides a unified portal for online motif discovery and analysis, integrating the MEME and GLAM2 algorithms for finding motifs (including those with gaps). It offers multiple sequence scanning tools (MAST, FIMO, GLAM2SCAN), motif comparison (Tomtom), and functional analysis (GOMO), all accessible via a user-friendly web interface with enhanced output and workflow integration. All tools are implemented as web services, with source code and binaries freely available for noncommercial use.",
    "year": 2009,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 7946,
      "supporting": 41,
      "contradicting": 2,
      "mentioning": 7878,
      "unclassified": 25,
      "citingPublications": 9641
    }
  },
  {
    "query": "Queue Model",
    "title": "Complex networks: Structure and dynamics",
    "doi": "10.1016/j.physrep.2005.10.009",
    "problem": "Many real-world systems, such as biological networks, neural networks, and the Internet, consist of numerous highly interconnected dynamical units. Scientists face challenges in characterizing the topology of these complex networks, understanding the principles underlying their structure, and modeling their growth. Additionally, there are significant questions about how the collective dynamics of these interconnected systems emerge from their complex wiring.",
    "solution": "The paper reviews major concepts and recent results in the study of both the structure and dynamics of complex networks. It summarizes how modeling these systems as graphs helps capture their global properties and discusses the application of these ideas across various disciplines, including nonlinear science, biology, statistical mechanics, medicine, and engineering.",
    "year": 2006,
    "journal": "Physics Reports",
    "citations": {
      "total": 7833,
      "supporting": 54,
      "contradicting": 4,
      "mentioning": 7581,
      "unclassified": 194,
      "citingPublications": 10389
    }
  },
  {
    "query": "Queue Model",
    "title": "The Structure of the Potassium Channel: Molecular Basis of K\n            +\n            Conduction and Selectivity",
    "doi": "10.1126/science.280.5360.69",
    "problem": "The mechanism by which potassium channels selectively conduct K+ ions over smaller Na+ ions, despite structural similarities, is not well understood. The physical principles underlying this selective ion conduction in the potassium channel from Streptomyces lividans require elucidation.",
    "solution": "X-ray analysis reveals that the channel's structure, particularly the selectivity filter lined with main chain carbonyl oxygen atoms, is configured to coordinate K+ ions but not Na+ ions. The presence of two K+ ions within the filter, spaced to exploit electrostatic repulsion, facilitates ion conduction while maintaining selectivity. This structural arrangement establishes the physical principles responsible for selective K+ conduction.",
    "year": 1998,
    "journal": "Science",
    "citations": {
      "total": 6459,
      "supporting": 200,
      "contradicting": 16,
      "mentioning": 6190,
      "unclassified": 53,
      "citingPublications": 6561
    }
  },
  {
    "query": "Queue Model",
    "title": "Statistical physics of social dynamics",
    "doi": "10.1103/revmodphys.81.591",
    "problem": "The problem addressed is understanding and describing collective phenomena that emerge from the interactions of individuals within social structures, such as opinion dynamics, cultural and language evolution, crowd behavior, hierarchy formation, human dynamics, and social spreading. These phenomena are complex and traditionally outside the scope of classical physics.",
    "solution": "The solution proposed is to apply the framework of statistical physics to model these social phenomena. The approach involves reviewing and connecting various topics in social dynamics with traditional statistical physics, and emphasizing the comparison of model results with empirical data from social systems.",
    "year": 2009,
    "journal": "Reviews of Modern Physics",
    "citations": {
      "total": 4002,
      "supporting": 43,
      "contradicting": 2,
      "mentioning": 3876,
      "unclassified": 81,
      "citingPublications": 3979
    }
  },
  {
    "query": "Queue Model",
    "title": "Internet of Things: A Survey on Enabling Technologies, Protocols, and Applications",
    "doi": "10.1109/comst.2015.2444095",
    "problem": "Existing IoT architectures that are based on network stack models, such as the three-layer model, do not accurately reflect real IoT environments because their network layers do not encompass all technologies used to transfer data to IoT platforms. These models are also tailored to specific communication media like WSNs and are not well-suited for the diversity of IoT devices. Furthermore, complex layers such as 'Service Composition' can consume significant time and energy on resource-constrained devices.",
    "solution": "The abstract suggests the need for IoT architectures that better accommodate the variety of underlying technologies and the constraints of IoT devices, rather than relying on traditional network stack models. This would involve designing models that are more flexible and efficient for real-world IoT environments, particularly in terms of communication and service integration on resource-limited devices.",
    "year": 2015,
    "journal": "Ieee Communications Surveys & Tutorials",
    "citations": {
      "total": 3711,
      "supporting": 9,
      "contradicting": 1,
      "mentioning": 3486,
      "unclassified": 215,
      "citingPublications": 7647
    }
  },
  {
    "query": "Queue Model",
    "title": "Reinforcement Learning:  A Survey",
    "doi": "10.1613/jair.301",
    "problem": "The paper addresses the problem of how an agent can learn optimal behavior through trial-and-error interactions with a dynamic environment, a challenge known as reinforcement learning. Key issues include balancing exploration and exploitation, learning from delayed reinforcement, and dealing with hidden state. The field also faces challenges in establishing solid theoretical foundations and accelerating learning.",
    "solution": "The paper surveys and summarizes various approaches to reinforcement learning, including the use of Markov decision theory, empirical models to speed up learning, generalization, and hierarchical methods. It discusses practical implementations and assesses the effectiveness of current reinforcement learning techniques. The survey aims to provide a comprehensive overview of the field's foundations, challenges, and practical applications.",
    "year": 1996,
    "journal": "Journal of Artificial Intelligence Research",
    "citations": {
      "total": 3576,
      "supporting": 9,
      "contradicting": 0,
      "mentioning": 3460,
      "unclassified": 107,
      "citingPublications": 7458
    }
  },
  {
    "query": "Queue Model",
    "title": "Understanding normal and impaired word reading: Computational principles in quasi-regular domains.",
    "doi": "10.1037/0033-295x.103.1.56",
    "problem": "The paper addresses the challenge of modeling how skilled readers process quasi-regular domains such as English word reading, where the mapping between written and spoken forms is systematic but contains many exceptions. Previous models struggled particularly with reading nonwords and capturing the effects of word frequency and spelling-sound consistency. The complexity arises from the presence of both regular and exception words, as well as ambiguous spelling patterns.",
    "solution": "The authors propose a connectionist approach with improved orthographic and phonological representations that better capture the structure of written and spoken words. Through simulation experiments, their networks successfully learn to read both regular and exception words, as well as pronounceable nonwords, and reproduce empirical data on naming latencies and impaired reading. This approach supports a graded division-of-labor between semantic and phonological processes, offering a nuanced alternative to standard dual-route models.",
    "year": 1996,
    "journal": "Psychological Review",
    "citations": {
      "total": 2989,
      "supporting": 134,
      "contradicting": 12,
      "mentioning": 2790,
      "unclassified": 53,
      "citingPublications": 2445
    }
  },
  {
    "query": "Queue Model",
    "title": "Watersheds in digital spaces: an efficient algorithm based on immersion simulations",
    "doi": "10.1109/34.87344",
    "problem": "The paper addresses the challenge of efficiently and accurately computing watersheds in digital grayscale images, which is important for tasks such as image segmentation. Existing watershed algorithms are either slow or lack flexibility, and there is a need for methods that can be easily adapted to various digital grids and higher-dimensional data.",
    "solution": "The authors propose a fast and flexible algorithm for computing watersheds based on an immersion process analogy, using a queue of pixels to efficiently simulate flooding. The algorithm is described in detail, shown to be more accurate and faster than existing methods, and can be easily generalized to n-dimensional images and graphs. Applications to image segmentation, including MR imagery and digital elevation models, are demonstrated.",
    "year": 1991,
    "journal": "Ieee Transactions on Pattern Analysis and Machine Intelligence",
    "citations": {
      "total": 2824,
      "supporting": 4,
      "contradicting": 0,
      "mentioning": 2760,
      "unclassified": 60,
      "citingPublications": 5093
    }
  },
  {
    "query": "Queue Model",
    "title": "Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases",
    "doi": "10.1038/s41586-019-1237-9",
    "problem": "Inflammatory bowel diseases such as Crohn’s disease and ulcerative colitis are complex and heterogeneous at multiple biological levels, affecting millions worldwide. Despite extensive research on individual contributing factors, a comprehensive understanding of the integrated molecular and microbial changes during disease activity is lacking.",
    "solution": "The study conducted longitudinal profiling of 132 subjects over one year, generating integrated molecular data from host and microbial sources during disease activity. This approach revealed characteristic shifts in gut microbiome composition and function, identified key dysregulated factors, and made these comprehensive datasets publicly available through the Inflammatory Bowel Disease Multi’omics Database.",
    "year": 2019,
    "journal": "Nature",
    "citations": {
      "total": 2740,
      "supporting": 163,
      "contradicting": 9,
      "mentioning": 2557,
      "unclassified": 11,
      "citingPublications": 2404
    }
  },
  {
    "query": "Queue Model",
    "title": "An Experimental Comparison of Min-cut/Max-flow Algorithms for Energy Minimization in Vision",
    "doi": "10.1007/3-540-44745-8_24",
    "problem": "While minimum cut/maximum flow algorithms are widely used for energy minimization in low-level vision tasks, their practical efficiency has not been thoroughly studied within the context of computer vision. There is a need for an experimental comparison of these algorithms to determine their effectiveness and speed on typical vision-related graphs.",
    "solution": "The paper provides an experimental comparison of the efficiency of several standard min-cut/max-flow algorithms, including both push-relabel and augmenting path methods, as well as a newly developed algorithm. These algorithms are benchmarked on graphs from image restoration, stereo, and interactive segmentation tasks. The new algorithm is shown to be several times faster than existing methods, enabling near real-time performance.",
    "year": 2001,
    "journal": "",
    "citations": {
      "total": 2566,
      "supporting": 4,
      "contradicting": 0,
      "mentioning": 2545,
      "unclassified": 17,
      "citingPublications": 1689
    }
  },
  {
    "query": "Compound Annual Growth Rate",
    "title": "Global Cancer Statistics 2020: GLOBOCAN Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries",
    "doi": "10.3322/caac.21660",
    "problem": "The global cancer burden is rising, with an estimated 19.3 million new cancer cases and almost 10 million deaths in 2020. Incidence and mortality rates are significantly higher in transitioned countries, but death rates for certain cancers like breast and cervical are much higher in transitioning countries. The cancer burden is projected to increase by 47% to 28.4 million cases by 2040, with the largest increases in transitioning countries due to demographic changes and risk factors.",
    "solution": "The article emphasizes the need to build sustainable infrastructure for disseminating cancer prevention measures and providing cancer care, especially in transitioning countries. This approach is critical for effective global cancer control and to address the expected rise in cancer cases and deaths.",
    "year": 2021,
    "journal": "Ca a Cancer Journal for Clinicians",
    "citations": {
      "total": 68019,
      "supporting": 246,
      "contradicting": 32,
      "mentioning": 66379,
      "unclassified": 1362,
      "citingPublications": 91700
    }
  },
  {
    "query": "Compound Annual Growth Rate",
    "title": "Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries",
    "doi": "10.3322/caac.21492",
    "problem": "There is significant geographic variability in cancer incidence and mortality worldwide, with the most common types of cancer and leading causes of cancer death differing across regions and countries. Many low- and middle-income countries lack high-quality cancer registry data, which hampers the planning and implementation of effective, evidence-based cancer control programs.",
    "solution": "The Global Initiative for Cancer Registry Development is an international partnership that supports the improvement of cancer data collection and estimation, particularly in low- and middle-income countries. This initiative aims to enhance the use of local data to better prioritize and evaluate national cancer control efforts.",
    "year": 2018,
    "journal": "Ca a Cancer Journal for Clinicians",
    "citations": {
      "total": 60748,
      "supporting": 282,
      "contradicting": 29,
      "mentioning": 58657,
      "unclassified": 1780,
      "citingPublications": 80114
    }
  },
  {
    "query": "Compound Annual Growth Rate",
    "title": "Hallmarks of Cancer: The Next Generation",
    "doi": "10.1016/j.cell.2011.02.013",
    "problem": "Cancer is a complex disease characterized by multiple biological capabilities, known as hallmarks, that enable tumor development and progression. These hallmarks include processes such as sustained proliferative signaling, evasion of growth suppressors, and resistance to cell death, among others. The complexity is further increased by genetic instability, inflammation, and the involvement of non-cancerous cells in the tumor microenvironment.",
    "solution": "The hallmarks of cancer provide an organizing principle for understanding and rationalizing the complexities of neoplastic disease. Recognizing and conceptualizing these hallmarks and their underlying mechanisms will guide the development of new strategies and therapies for treating human cancer.",
    "year": 2011,
    "journal": "Cell",
    "citations": {
      "total": 51167,
      "supporting": 583,
      "contradicting": 21,
      "mentioning": 49690,
      "unclassified": 873,
      "citingPublications": 59730
    }
  },
  {
    "query": "Compound Annual Growth Rate",
    "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
    "doi": "10.1037/0021-9010.88.5.879",
    "problem": "There is a longstanding issue of method biases affecting results in behavioral sciences research. Despite this, there is no comprehensive summary detailing the sources of these biases or how to control for them. This gap makes it difficult for researchers to effectively address method biases in their studies.",
    "solution": "The article examines the extent to which method biases influence behavioral research, identifies their potential sources, and discusses the cognitive processes through which they affect responses. It evaluates various procedural and statistical techniques to control for method biases and provides recommendations for selecting appropriate remedies tailored to different research settings.",
    "year": 2003,
    "journal": "Journal of Applied Psychology",
    "citations": {
      "total": 51016,
      "supporting": 569,
      "contradicting": 162,
      "mentioning": 49580,
      "unclassified": 705,
      "citingPublications": 63567
    }
  },
  {
    "query": "Compound Annual Growth Rate",
    "title": "Cytoscape: A Software Environment for Integrated Models of Biomolecular Interaction Networks",
    "doi": "10.1101/gr.1239303",
    "problem": "Integrating and analyzing the vast and complex data from biomolecular interaction networks and high-throughput molecular state measurements is challenging. Traditional pathway-specific models are limited in scope and struggle to manage the immense complexity and scale of cellular components and interactions. There is a need for a unified framework to systematically interrogate, visualize, and analyze these data to reveal emergent biological properties.",
    "solution": "Cytoscape is proposed as an open source software platform that integrates biomolecular interaction networks with high-throughput expression data and other molecular states into a unified conceptual framework. Its extensible core provides basic network layout, querying, and visualization functionalities, and supports rapid development of additional analyses through plug-ins. Cytoscape enables researchers to link networks to functional databases, visually integrate diverse molecular data, and manage the complexity of large-scale biological systems.",
    "year": 2003,
    "journal": "Genome Research",
    "citations": {
      "total": 34040,
      "supporting": 38,
      "contradicting": 3,
      "mentioning": 33876,
      "unclassified": 123,
      "citingPublications": 44784
    }
  },
  {
    "query": "Compound Annual Growth Rate",
    "title": "Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.",
    "doi": "10.1037/0003-066x.55.1.68",
    "problem": "Human beings can exhibit either proactive engagement or passive alienation, largely depending on the social conditions in which they develop and function. Non-optimal human functioning, such as apathy and alienation, is widespread and not limited by social or cultural background. This variation in motivation and personal growth suggests that social environments play a significant role in fostering or undermining positive human potentials.",
    "solution": "Research guided by self-determination theory (SDT) investigates the social-contextual conditions that facilitate or hinder self-motivation and healthy psychological development. The theory identifies three innate psychological needs—competence, autonomy, and relatedness—whose satisfaction enhances self-motivation and well-being, while their frustration leads to diminished motivation. Understanding and applying these findings can inform the design of social environments that optimize development, performance, and well-being across various life domains.",
    "year": 2000,
    "journal": "American Psychologist",
    "citations": {
      "total": 30517,
      "supporting": 1088,
      "contradicting": 80,
      "mentioning": 27611,
      "unclassified": 1738,
      "citingPublications": 32868
    }
  },
  {
    "query": "Compound Annual Growth Rate",
    "title": "Global cancer statistics",
    "doi": "10.3322/caac.20107",
    "problem": "The global burden of cancer is increasing due to the aging and growth of the world population, as well as the rising adoption of cancer-causing behaviors, especially in economically developing countries. Cancer incidence and mortality rates are high, with poorer survival rates in developing countries due to late diagnosis and limited access to treatment. Specific cancers such as breast and lung cancer are leading causes of death, and overall cancer mortality rates are similar between developed and developing countries despite lower incidence rates in the latter.",
    "solution": "A significant portion of the global cancer burden could be prevented by applying existing cancer control knowledge and implementing programs such as tobacco control, vaccination for liver and cervical cancers, early detection and treatment, and public health campaigns promoting physical activity and healthier diets. Clinicians, public health professionals, and policymakers are encouraged to actively promote and accelerate the adoption of these interventions worldwide.",
    "year": 2011,
    "journal": "Ca a Cancer Journal for Clinicians",
    "citations": {
      "total": 23973,
      "supporting": 98,
      "contradicting": 23,
      "mentioning": 23312,
      "unclassified": 540,
      "citingPublications": 51549
    }
  },
  {
    "query": "Compound Annual Growth Rate",
    "title": "Cancer incidence and mortality worldwide: Sources, methods and major patterns in GLOBOCAN 2012",
    "doi": "10.1002/ijc.29210",
    "problem": "There is a lack of comprehensive, high-quality, and up-to-date global data on cancer incidence and mortality, particularly in low and middle income countries where cancer burden is rapidly increasing. Existing cancer registries often cover only limited portions of national populations, leading to incomplete or inconsistent data. This makes it challenging to accurately assess and address the global cancer burden.",
    "solution": "The GLOBOCAN project compiles and refines national cancer incidence and mortality estimates using the best available data from 184 countries, providing global, regional, and country-level profiles for 27 major cancers and all cancers combined. It introduces an alphanumeric scoring system to document data quality and availability, and offers tools for tabulation and visualization of the dataset. These efforts aim to guide resource allocation and strategy development to reduce the worldwide cancer burden.",
    "year": 2014,
    "journal": "International Journal of Cancer",
    "citations": {
      "total": 23634,
      "supporting": 136,
      "contradicting": 18,
      "mentioning": 22554,
      "unclassified": 926,
      "citingPublications": 29484
    }
  },
  {
    "query": "Economy curve",
    "title": "Endogenous Technological Change",
    "doi": "10.1086/261725",
    "problem": "The paper addresses the challenge of modeling economic growth driven by technological change, where technology is a nonrival, partially excludable good. This characteristic introduces nonconvexity, making price-taking competition unsustainable. The problem also includes the underinvestment of human capital in research and the factors influencing growth rates, such as market integration and population size.",
    "solution": "The proposed solution is a growth model where technological change results from intentional investment decisions by profit-maximizing agents, leading to an equilibrium characterized by monopolistic competition. The model demonstrates that the stock of human capital is crucial for growth, highlights the underallocation of human capital to research, and suggests that integration into world markets can increase growth rates.",
    "year": 1990,
    "journal": "Journal of Political <strong Class=\"highlight\">economy</Strong>",
    "citations": {
      "total": 11070,
      "supporting": 165,
      "contradicting": 17,
      "mentioning": 9675,
      "unclassified": 1213,
      "citingPublications": 16514
    }
  },
  {
    "query": "Economy curve",
    "title": "Adolescence-limited and life-course-persistent antisocial behavior: A developmental taxonomy.",
    "doi": "10.1037/0033-295x.100.4.674",
    "problem": "Antisocial behavior appears to be both highly continuous across age and yet its prevalence changes dramatically, particularly increasing almost tenfold during adolescence. This presents an incongruity in understanding the development and persistence of antisocial behavior over the lifespan.",
    "solution": "The paper proposes a dual taxonomy that distinguishes between two categories of individuals: a small group exhibiting life-course-persistent antisocial behavior due to neuropsychological problems and adverse environments, and a larger group displaying adolescence-limited antisocial behavior driven by a maturity gap and social mimicry. This framework reconciles the observed continuity and prevalence changes in antisocial behavior by attributing them to distinct etiologies and developmental trajectories.",
    "year": 1993,
    "journal": "Psychological Review",
    "citations": {
      "total": 8073,
      "supporting": 570,
      "contradicting": 49,
      "mentioning": 7126,
      "unclassified": 328,
      "citingPublications": 8702
    }
  },
  {
    "query": "Economy curve",
    "title": "The socio-economic implications of the coronavirus pandemic (COVID-19): A review",
    "doi": "10.1016/j.ijsu.2020.04.018",
    "problem": "The COVID-19 pandemic has caused widespread health impacts and triggered fears of a global economic crisis and recession. Measures such as social distancing, self-isolation, and travel restrictions have reduced workforce participation, led to job losses, and decreased demand for many goods and services. At the same time, certain sectors like medical supplies and food have experienced increased demand.",
    "solution": "The paper provides a summary of the socio-economic effects of COVID-19 on various aspects of the world economy, aiming to inform and contextualize the impact of the pandemic across different sectors.",
    "year": 2020,
    "journal": "International Journal of Surgery",
    "citations": {
      "total": 5204,
      "supporting": 66,
      "contradicting": 3,
      "mentioning": 4852,
      "unclassified": 283,
      "citingPublications": 6334
    }
  },
  {
    "query": "Economy curve",
    "title": "INTEREST AND PRICES: FOUNDATIONS OF A THEORY OF MONETARY POLICY",
    "doi": "10.1017/s1365100505040253",
    "problem": "Monetary policy in the 1980s and early 1990s was plagued by time inconsistency, where discretionary actions by central banks often led to socially costly inflation. Researchers sought to understand and address the challenges of dynamic inconsistency in monetary policy. The need was to develop frameworks that could prevent politically induced inflation and improve social welfare.",
    "solution": "Solutions included appointing conservative central bankers, designing incentive contracts, and imposing inflation targets to mitigate time inconsistency. Over time, the adoption of dynamic stochastic general equilibrium models combined with models of price stickiness provided a robust framework for macroeconomic analysis and monetary policy design. This framework, centered on the expectational IS curve, inflation adjustment equations, and explicit monetary policy rules, has facilitated productive collaboration between academics and central bank economists.",
    "year": 2005,
    "journal": "Macroeconomic Dynamics",
    "citations": {
      "total": 4736,
      "supporting": 120,
      "contradicting": 6,
      "mentioning": 4464,
      "unclassified": 146,
      "citingPublications": 5302
    }
  },
  {
    "query": "Economy curve",
    "title": "Methods for in vitro evaluating antimicrobial activity: A review",
    "doi": "10.1016/j.jpha.2015.11.005",
    "problem": "There is a growing need to discover and develop new antimicrobial agents to address microbial resistance. However, the effectiveness of these agents depends on reliable antimicrobial activity screening and evaluation methods. Some advanced bioassays are underutilized due to the need for specialized equipment and concerns about reproducibility and standardization.",
    "solution": "The review provides an exhaustive list of in vitro antimicrobial susceptibility testing methods, along with detailed information on their advantages and limitations. This comprehensive overview aims to inform researchers about available methods and guide the selection of appropriate assays for antimicrobial evaluation.",
    "year": 2016,
    "journal": "Journal of Pharmaceutical Analysis",
    "citations": {
      "total": 3825,
      "supporting": 28,
      "contradicting": 2,
      "mentioning": 3664,
      "unclassified": 131,
      "citingPublications": 6007
    }
  },
  {
    "query": "Economy curve",
    "title": "Heart Disease and Stroke Statistics—2022 Update: A Report From the American Heart Association",
    "doi": "10.1161/cir.0000000000001052",
    "problem": "There is a need for the most current and comprehensive data on heart disease, stroke, and cardiovascular risk factors, including their clinical conditions, outcomes, and economic costs. Accurate and up-to-date information is essential for understanding the prevalence and impact of cardiovascular diseases and for informing healthcare decisions and policy.",
    "solution": "The American Heart Association, in collaboration with the National Institutes of Health, compiles and publishes an annual Statistical Update that presents the latest data on heart disease, stroke, and related risk factors. This update is produced through continuous monitoring and evaluation of data sources and serves as a critical resource for a wide range of stakeholders seeking reliable information on cardiovascular health.",
    "year": 2022,
    "journal": "Circulation",
    "citations": {
      "total": 3483,
      "supporting": 32,
      "contradicting": 7,
      "mentioning": 3391,
      "unclassified": 53,
      "citingPublications": 4701
    }
  },
  {
    "query": "Economy curve",
    "title": "A spreading-activation theory of semantic processing.",
    "doi": "10.1037/0033-295x.82.6.407",
    "problem": "There is a need to explain and account for a wide range of experimental results in human semantic processing, including issues with existing theories such as Quillian's model and the Smith, Shoben, and Rips model for categorization judgments. Misconceptions about Quillian's theory and its limitations in explaining recent experimental findings also present challenges.",
    "solution": "The paper proposes an extended spreading-activation theory of human semantic processing, building on Quillian's theory and incorporating additional assumptions to address recent experimental results. This extended theory is demonstrated to account for various production, categorization, and sentence-verification experiments, and it provides a critique of alternative models.",
    "year": 1975,
    "journal": "Psychological Review",
    "citations": {
      "total": 3477,
      "supporting": 93,
      "contradicting": 5,
      "mentioning": 3285,
      "unclassified": 94,
      "citingPublications": 7042
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "Some Tests of Specification for Panel Data: <strong class=\"highlight\">Monte</strong> <strong class=\"highlight\">Carlo</strong> Evidence and an Application to Employment Equations",
    "doi": "10.2307/2297968",
    "problem": "The paper addresses the challenge of testing model specifications after estimating dynamic models from panel data using the generalized method of moments (GMM). Specifically, it deals with exploiting all linear moment restrictions under the assumption of no serial correlation in the errors, in models containing individual effects and lagged dependent variables but no strictly exogenous variables.",
    "solution": "The authors propose specification tests, including a test for serial correlation based on GMM residuals, and compare these with Sargan and Hausman specification tests. They also discuss alternative estimators that do not depend on fourth-order moments and are asymptotically equivalent under certain conditions, thereby improving the practical performance of GMM-based procedures.",
    "year": 1991,
    "journal": "The Review of Economic Studies",
    "citations": {
      "total": 21264,
      "supporting": 212,
      "contradicting": 10,
      "mentioning": 20328,
      "unclassified": 714,
      "citingPublications": 27692
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "QUANTUM ESPRESSO: a modular and open-source software project for quantum <strong class=\"highlight\">simulations</strong> of materials",
    "doi": "10.1088/0953-8984/21/39/395502",
    "problem": "Researchers in electronic-structure calculations and materials modeling require efficient, innovative, and user-friendly computational tools that can handle complex simulations, especially on massively parallel architectures. Existing solutions may lack integration, openness, or the ability to easily incorporate new algorithms and user contributions.",
    "solution": "QUANTUM ESPRESSO is an open-source suite of computer codes designed for electronic-structure calculations and materials modeling, based on density-functional theory, plane waves, and pseudopotentials. It emphasizes innovation, efficiency, and user-friendliness, and is structured to support massively parallel computing. The project encourages community participation by enabling researchers to contribute new codes or ideas, fostering a collaborative and interoperable environment.",
    "year": 2009,
    "journal": "Journal of Physics Condensed Matter",
    "citations": {
      "total": 0,
      "supporting": 0,
      "contradicting": 0,
      "mentioning": 0,
      "unclassified": 0,
      "citingPublications": 0
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "The electronic properties of graphene",
    "doi": "10.1103/revmodphys.81.109",
    "problem": "The electronic properties of graphene, a one-atom-thick carbon material with unusual Dirac-like excitations, are not fully understood, particularly regarding how they are influenced by external fields, sample geometry, stacking order, edge termination, disorder, and interactions. There is a need to clarify how these factors affect phenomena such as tunneling, confinement, quantum Hall effect, and transport properties in both single-layer and multilayer graphene.",
    "solution": "The article reviews and analyzes the theoretical aspects of graphene's electronic properties, focusing on the behavior of Dirac electrons under various conditions, including external fields, geometry, stacking, and disorder. It discusses how these factors influence key physical phenomena and presents insights into the effects of electron-electron and electron-phonon interactions in graphene systems.",
    "year": 2009,
    "journal": "Reviews of Modern Physics",
    "citations": {
      "total": 18350,
      "supporting": 319,
      "contradicting": 12,
      "mentioning": 17837,
      "unclassified": 182,
      "citingPublications": 23990
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "A new criterion for assessing discriminant validity in variance-based structural equation modeling",
    "doi": "10.1007/s11747-014-0403-8",
    "problem": "The current dominant approaches for assessing discriminant validity in variance-based structural equation modeling, such as the Fornell-Larcker criterion and examination of cross-loadings, do not reliably detect the lack of discriminant validity in common research situations. This inadequacy can lead to incorrect conclusions about the relationships between latent variables.",
    "solution": "The authors propose an alternative approach based on the multitrait-multimethod matrix, specifically the heterotrait-monotrait ratio of correlations, to assess discriminant validity. They demonstrate its superior performance through a Monte Carlo simulation study and provide guidelines for addressing discriminant validity issues in variance-based structural equation modeling.",
    "year": 2014,
    "journal": "Journal of the Academy of Marketing Science",
    "citations": {
      "total": 18196,
      "supporting": 612,
      "contradicting": 26,
      "mentioning": 17043,
      "unclassified": 515,
      "citingPublications": 25275
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "A smooth particle mesh Ewald method",
    "doi": "10.1063/1.470117",
    "problem": "The particle mesh Ewald method, while effective, is limited by its interpolation technique and computational efficiency, especially for potentials of the form 1/r^p with p ≠ 1. Existing methods also struggle to provide analytic gradients and high accuracy without significant computational cost. This poses challenges for large biomolecular systems requiring efficient and accurate Ewald summation.",
    "solution": "The method is reformulated using efficient B-spline interpolation of the structure factors, replacing Lagrange interpolation. This allows for analytic gradients, significant improvements in accuracy, and extension to a broader class of potentials. The approach achieves arbitrary accuracy independent of system size, with computational cost scaling as N log(N), making it feasible for large biomolecular systems.",
    "year": 1995,
    "journal": "The Journal of Chemical Physics",
    "citations": {
      "total": 16931,
      "supporting": 43,
      "contradicting": 1,
      "mentioning": 16840,
      "unclassified": 47,
      "citingPublications": 21528
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space",
    "doi": "10.1093/sysbio/sys029",
    "problem": "There is a need for efficient and accurate Bayesian phylogenetic inference using Markov chain Monte Carlo (MCMC) methods, with improved convergence diagnostics, computational speed, and support for complex evolutionary models. Existing software versions had limitations in convergence monitoring, computational efficiency, and model flexibility.",
    "solution": "The new version of MrBayes (3.2) addresses these issues by introducing enhanced convergence diagnostics, parallel analysis capabilities, and real-time convergence monitoring. It improves convergence through new proposals and automatic tuning, increases computational speed via SSE and BEAGLE GPU support, and adds new models and output options for greater flexibility and accuracy in phylogenetic inference.",
    "year": 2012,
    "journal": "Systematic Biology",
    "citations": {
      "total": 15517,
      "supporting": 30,
      "contradicting": 0,
      "mentioning": 15422,
      "unclassified": 65,
      "citingPublications": 24637
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "Co-Integration and Error Correction: Representation, Estimation, and Testing",
    "doi": "10.2307/1913236",
    "problem": "The paper addresses the challenge of understanding and testing the relationship between co-integration and error correction models in time series analysis, particularly when the individual series are nonstationary but certain linear combinations are stationary. It also considers the difficulty of estimating such models and testing for co-integration, given the complications of unit roots and unidentified parameters under the null hypothesis.",
    "solution": "The authors extend previous theoretical work to develop new estimation procedures and tests for co-integrated systems, including a simple but asymptotically efficient two-step estimator. They formulate and analyze seven test statistics, calculate their critical values using Monte Carlo simulation, and examine the power properties of these tests, ultimately recommending one test procedure for practical application.",
    "year": 1987,
    "journal": "Econometrica",
    "citations": {
      "total": 14327,
      "supporting": 101,
      "contradicting": 7,
      "mentioning": 13430,
      "unclassified": 789,
      "citingPublications": 26082
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "All-Atom Empirical Potential for Molecular Modeling and Dynamics Studies of Proteins",
    "doi": "10.1021/jp973084f",
    "problem": "Existing all-atom empirical energy functions in the CHARMM program lacked balanced and accurate parameters for modeling proteins, particularly in representing both internal and interaction terms of the force field. Previous parameter sets were insufficient for achieving reliable agreement with experimental data for peptides and proteins in both solution and crystal environments. There was a need for improved parameters that could consistently reproduce structural and dynamic properties observed experimentally.",
    "solution": "The researchers developed new protein parameters for the CHARMM energy function using a self-consistent optimization approach that balanced internal and interaction terms, utilizing experimental data and ab initio results. The parameters were optimized and validated against a wide range of experimental measurements, including gas-phase geometries, vibrational spectra, heats of vaporization, and crystal structures, as well as molecular dynamics simulations. The resulting parameter set provides a consistent and accurate basis for condensed-phase simulations of proteins and other biologically relevant molecules.",
    "year": 1998,
    "journal": "The Journal of Physical Chemistry B",
    "citations": {
      "total": 13611,
      "supporting": 101,
      "contradicting": 6,
      "mentioning": 13443,
      "unclassified": 61,
      "citingPublications": 14135
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "CHARMM: A program for macromolecular energy, minimization, and dynamics calculations",
    "doi": "10.1002/jcc.540040211",
    "problem": "Modeling the structural, equilibrium, and dynamic properties of macromolecular systems is a complex task that requires flexible and accurate computational tools. Existing methods may lack the necessary flexibility or empirical accuracy for such simulations.",
    "solution": "The CHARMM program is introduced as a highly flexible tool that utilizes empirical energy functions to model macromolecular systems. It can build structures, perform energy minimization, run simulations, and analyze the resulting properties, with detailed operational descriptions and sample parameters provided.",
    "year": 1983,
    "journal": "Journal of Computational Chemistry",
    "citations": {
      "total": 13018,
      "supporting": 56,
      "contradicting": 0,
      "mentioning": 12872,
      "unclassified": 90,
      "citingPublications": 15113
    }
  },
  {
    "query": "Monte Carlo Simulation",
    "title": "The Structure and Function of Complex Networks",
    "doi": "10.1137/s003614450342480",
    "problem": "Researchers face challenges in understanding and predicting the behavior of complex networked systems such as the Internet, social networks, and biological networks. These systems exhibit intricate structures and dynamics that require specialized analytical approaches.",
    "solution": "A variety of techniques and models have been developed to analyze and predict network behavior, including concepts like the small-world effect, degree distributions, clustering, network correlations, and random graph models. Additionally, models of network growth, preferential attachment, and studies of dynamical processes on networks have advanced understanding in this field.",
    "year": 2003,
    "journal": "Siam Review",
    "citations": {
      "total": 12904,
      "supporting": 148,
      "contradicting": 11,
      "mentioning": 12318,
      "unclassified": 427,
      "citingPublications": 17092
    }
  },
  {
    "query": "Graph Theory",
    "title": "Gapped BLAST and PSI-BLAST: a new generation of protein database search programs",
    "doi": "10.1093/nar/25.17.3389",
    "problem": "The BLAST programs, while widely used for searching protein and DNA databases for sequence similarities, face challenges in execution speed and sensitivity to weak similarities. There is a need to improve both the efficiency and sensitivity of these searches, particularly for detecting biologically relevant but weak sequence similarities.",
    "solution": "The authors introduce algorithmic and statistical refinements, including a new criterion for extending word hits and a heuristic for generating gapped alignments, resulting in a faster and more sensitive gapped BLAST program. Additionally, they present PSI-BLAST, which automatically combines significant alignments into a position-specific score matrix for iterative database searching, achieving greater sensitivity to weak similarities while maintaining efficient performance.",
    "year": 1997,
    "journal": "Nucleic Acids Research",
    "citations": {
      "total": 50326,
      "supporting": 111,
      "contradicting": 12,
      "mentioning": 49877,
      "unclassified": 326,
      "citingPublications": 69682
    }
  },
  {
    "query": "Graph Theory",
    "title": "Gradient-based learning applied to document recognition",
    "doi": "10.1109/5.726791",
    "problem": "Handwritten character recognition, especially in real-life document processing systems, is challenging due to the high dimensionality and variability of input patterns, such as handwritten digits and characters. Existing methods often require extensive preprocessing and may not be optimized for the overall system performance. There is a need for a more flexible and globally trainable approach that can handle the complexity of multimodule document recognition systems.",
    "solution": "The paper proposes a new learning paradigm called graph transformer networks (GTN's), which enables the global training of multimodule document recognition systems using gradient-based methods. GTNs integrate convolutional neural network character recognizers with global optimization techniques to improve overall system accuracy. The approach is demonstrated on online handwriting recognition tasks and a commercial bank check reading system, achieving record accuracy and high scalability.",
    "year": 1998,
    "journal": "Proceedings of the Ieee",
    "citations": {
      "total": 25812,
      "supporting": 80,
      "contradicting": 8,
      "mentioning": 25415,
      "unclassified": 309,
      "citingPublications": 52110
    }
  },
  {
    "query": "Graph Theory",
    "title": "When to use and how to report the results of PLS-SEM",
    "doi": "10.1108/ebr-11-2018-0203",
    "problem": "Researchers using partial least squares structural equation modeling (PLS-SEM) face challenges in understanding which considerations and metrics are necessary for proper analysis and reporting of results. There is a need for up-to-date guidelines that incorporate both established and newly proposed evaluation criteria and methods. The rapid methodological developments in PLS-SEM further complicate keeping current with best practices.",
    "solution": "This paper provides a comprehensive and concise overview of the key considerations, metrics, and rules of thumb for conducting and reporting PLS-SEM analyses. It summarizes both established and recently introduced guidelines, including novel approaches such as PLSpredict, model comparison metrics, and robustness checks, ensuring researchers are equipped with the latest tools for PLS-SEM evaluation.",
    "year": 2019,
    "journal": "European Business Review",
    "citations": {
      "total": 23047,
      "supporting": 563,
      "contradicting": 27,
      "mentioning": 21451,
      "unclassified": 1006,
      "citingPublications": 17285
    }
  },
  {
    "query": "Graph Theory",
    "title": "PRISMA Extension for Scoping Reviews (PRISMA-ScR): Checklist and Explanation",
    "doi": "10.7326/m18-0850",
    "problem": "The methodological and reporting quality of scoping reviews, which are increasingly being conducted to map evidence and identify key concepts and gaps, needs improvement. There is a lack of standardized guidance for reporting scoping reviews effectively.",
    "solution": "The authors present the PRISMA-ScR checklist, developed by an expert panel, as a standardized reporting guideline for scoping reviews. This checklist includes 20 essential and 2 optional reporting items, along with rationales and examples, to improve the quality, transparency, and understanding of scoping review reporting.",
    "year": 2018,
    "journal": "Annals of Internal Medicine",
    "citations": {
      "total": 21547,
      "supporting": 50,
      "contradicting": 0,
      "mentioning": 20413,
      "unclassified": 1084,
      "citingPublications": 27673
    }
  },
  {
    "query": "Graph Theory",
    "title": "Multiwfn: A multifunctional wavefunction analyzer",
    "doi": "10.1002/jcc.22885",
    "problem": "There is a need for a comprehensive, efficient, and user-friendly tool for wavefunction analysis in quantum chemistry, including functionalities such as real space function visualization, population analysis, bond order analysis, and more. Existing programs may lack efficiency, versatility, or ease of use for both research and teaching purposes.",
    "solution": "Multiwfn is a multifunctional, open-source program that provides a wide range of wavefunction analysis tools, including visualization, population and bond order analysis, and topology analysis. It features a user-friendly interface, built-in graphing capabilities, and optimized, parallelized code for high efficiency, outperforming similar programs. The program is freely available for download, with both precompiled binaries and source code accessible online.",
    "year": 2011,
    "journal": "Journal of Computational Chemistry",
    "citations": {
      "total": 19224,
      "supporting": 218,
      "contradicting": 4,
      "mentioning": 18933,
      "unclassified": 69,
      "citingPublications": 33528
    }
  },
  {
    "query": "Graph Theory",
    "title": "Statistical mechanics of complex networks",
    "doi": "10.1103/revmodphys.74.47",
    "problem": "Traditional models of complex systems, such as biological or technological networks, have relied on random graph theory, which fails to capture the robust organizing principles observed in real-world networks. There is a need to better understand the topology and evolution of these networks, as well as their resilience to failures and attacks.",
    "solution": "The paper reviews recent advances in the statistical mechanics of complex networks, focusing on empirical data, main models (including random graphs, small-world, and scale-free networks), and analytical tools. It also explores the interplay between network topology and robustness, providing a comprehensive overview of the field's current understanding.",
    "year": 2002,
    "journal": "Reviews of Modern Physics",
    "citations": {
      "total": 16007,
      "supporting": 231,
      "contradicting": 20,
      "mentioning": 15477,
      "unclassified": 279,
      "citingPublications": 19423
    }
  },
  {
    "query": "Graph Theory",
    "title": "Fast unfolding of communities in large networks",
    "doi": "10.1088/1742-5468/2008/10/p10008",
    "problem": "Extracting the community structure of large networks is a challenging task, especially when considering computational efficiency and the quality of detected communities. Existing community detection methods often struggle with large-scale networks due to high computation times or insufficient accuracy.",
    "solution": "The authors propose a simple heuristic method based on modularity optimization to detect community structures in large networks. Their method outperforms existing approaches in computation time and delivers high-quality results, as demonstrated on large real-world networks and verified on ad-hoc modular networks.",
    "year": 2008,
    "journal": "Journal of Statistical Mechanics <strong Class=\"highlight\">theory</Strong> and Experiment",
    "citations": {
      "total": 14072,
      "supporting": 61,
      "contradicting": 3,
      "mentioning": 13655,
      "unclassified": 353,
      "citingPublications": 18912
    }
  },
  {
    "query": "Graph Theory",
    "title": "MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification",
    "doi": "10.1038/nbt.1511",
    "problem": "Efficient analysis of very large amounts of raw data for peptide identification and protein quantification is a principal challenge in mass spectrometry (MS)-based proteomics. Existing methods struggle with accurate detection and quantification in high-resolution, quantitative MS data. Achieving high mass accuracy and robust identification in complex datasets remains difficult.",
    "solution": "The authors propose MaxQuant, an integrated suite of algorithms specifically developed for high-resolution, quantitative MS data. MaxQuant uses correlation analysis and graph theory to detect peaks, isotope clusters, and SILAC peptide pairs as three-dimensional objects, and integrates multiple mass measurements with correction for mass offsets to achieve high mass accuracy. This approach enables automatic quantification of hundreds of thousands of peptides and robust identification and quantification of thousands of proteins in complex samples.",
    "year": 2008,
    "journal": "Nature Biotechnology",
    "citations": {
      "total": 13018,
      "supporting": 23,
      "contradicting": 4,
      "mentioning": 12957,
      "unclassified": 34,
      "citingPublications": 14613
    }
  },
  {
    "query": "Graph Theory",
    "title": "The Structure and Function of Complex Networks",
    "doi": "10.1137/s003614450342480",
    "problem": "Understanding and predicting the behavior of complex networked systems such as the Internet, social networks, and biological networks is a significant challenge. These systems exhibit intricate structures and dynamics that are not easily captured by traditional analytical methods.",
    "solution": "Researchers have developed a variety of techniques and models, including concepts like the small-world effect, degree distributions, clustering, network correlations, random graph models, network growth, preferential attachment, and dynamical processes on networks, to better analyze and predict the behavior of these systems.",
    "year": 2003,
    "journal": "Siam Review",
    "citations": {
      "total": 12904,
      "supporting": 148,
      "contradicting": 11,
      "mentioning": 12318,
      "unclassified": 427,
      "citingPublications": 17092
    }
  },
  {
    "query": "Graph Theory",
    "title": "Accurate spin-dependent electron liquid correlation energies for local spin density calculations: a critical analysis",
    "doi": "10.1139/p80-159",
    "problem": "The paper addresses the inadequacies of commonly used approximate forms for the correlation energy per particle in the spin-polarized homogeneous electron gas, particularly in their ability to interpolate accurately between paramagnetic and ferromagnetic states. These approximations are frequently used in the local spin density approximation for the exchange-correlation energy functional, but may not provide sufficient accuracy for real systems.",
    "solution": "The authors recalculate the RPA correlation energy as a function of electron density and spin polarization, and present a new, accurate interpolation formula using a Pade approximant technique. This approach interpolates recent Monte Carlo results for both paramagnetic and ferromagnetic states, and, when combined with RPA spin-dependence, yields a correlation energy formula with an estimated maximum error of 1 mRy, improving the reliability of non-local corrections to the local spin density approximation.",
    "year": 1980,
    "journal": "Canadian Journal of Physics",
    "citations": {
      "total": 12309,
      "supporting": 94,
      "contradicting": 3,
      "mentioning": 12096,
      "unclassified": 116,
      "citingPublications": 20676
    }
  },
  {
    "query": "Game Theory",
    "title": "Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology",
    "doi": "10.2307/249008",
    "problem": "The paper addresses the challenge of understanding which factors enhance the perception of mobile commerce users in Egypt, with the goal of promoting this form of shopping. It seeks to identify and examine the factors influencing technology users' behavior, particularly in the context of mobile communication and electronic shopping.",
    "solution": "The researchers conducted exploratory research to identify and test the most relevant factors for mobile commerce adoption, focusing on social influence, convenience, and hedonic motivations. They found that all three factors significantly affect users' perceptions of usefulness and ease of use, with convenience having the strongest impact and mediating the effects of social influence and hedonic motivation on user perceptions.",
    "year": 1989,
    "journal": "Mis Quarterly",
    "citations": {
      "total": 51456,
      "supporting": 1707,
      "contradicting": 149,
      "mentioning": 45367,
      "unclassified": 4233,
      "citingPublications": 50563
    }
  },
  {
    "query": "Game Theory",
    "title": "Generative adversarial networks",
    "doi": "10.1145/3422622",
    "problem": "The problem being addressed is the generative modeling problem, which involves learning the probability distribution underlying a collection of training examples in order to generate new, similar examples. Traditional generative models based on deep learning face challenges in generating realistic high-resolution images and often rely on optimization-based approaches.",
    "solution": "The proposed solution is the use of Generative Adversarial Networks (GANs), which leverage game theory to model the generative process. GANs consist of two competing networks that learn to generate realistic data by estimating the underlying probability distribution, resulting in highly successful generative models capable of producing realistic high-resolution images.",
    "year": 2020,
    "journal": "Communications of the Acm",
    "citations": {
      "total": 32589,
      "supporting": 44,
      "contradicting": 1,
      "mentioning": 32346,
      "unclassified": 198,
      "citingPublications": 28740
    }
  },
  {
    "query": "Game Theory",
    "title": "Human-level control through deep reinforcement learning",
    "doi": "10.1038/nature14236",
    "problem": "Reinforcement learning agents struggle to operate in real-world complex environments because they must efficiently process high-dimensional sensory inputs and generalize from past experiences to new situations. Traditional approaches have been limited to domains with handcrafted features or low-dimensional, fully observed state spaces. There is a need for agents that can learn effective policies directly from raw sensory data.",
    "solution": "The authors propose a novel artificial agent called a deep Q-network, which leverages recent advances in deep neural networks to learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. This agent was tested on Atari 2600 games, where it received only raw pixels and game scores as input and was able to outperform previous algorithms and match professional human testers across 49 games. This approach bridges the gap between high-dimensional sensory input and action selection, enabling artificial agents to excel at a diverse range of challenging tasks.",
    "year": 2015,
    "journal": "Nature",
    "citations": {
      "total": 18351,
      "supporting": 35,
      "contradicting": 4,
      "mentioning": 18196,
      "unclassified": 116,
      "citingPublications": 27157
    }
  },
  {
    "query": "Game Theory",
    "title": "Statistical mechanics of complex networks",
    "doi": "10.1103/revmodphys.74.47",
    "problem": "Traditional models of complex systems, such as random graphs, fail to capture the robust organizing principles that govern the topology and evolution of real-world networks found in nature and society. There is a need to better understand the statistical mechanics underlying these complex network structures and their dynamic behaviors.",
    "solution": "The paper reviews recent advances in the study of complex networks, focusing on empirical data, main models, and analytical tools. It covers developments in random graphs, small-world and scale-free networks, and examines how network topology influences robustness against failures and attacks.",
    "year": 2002,
    "journal": "Reviews of Modern Physics",
    "citations": {
      "total": 16007,
      "supporting": 231,
      "contradicting": 20,
      "mentioning": 15477,
      "unclassified": 279,
      "citingPublications": 19423
    }
  },
  {
    "query": "Game Theory",
    "title": "The qualitative content analysis process",
    "doi": "10.1111/j.1365-2648.2007.04569.x",
    "problem": "When there are no previous studies on a phenomenon or existing research is fragmented, it becomes challenging to analyze the content effectively. Additionally, there is a need to test existing theories in new situations or compare categories across different time periods.",
    "solution": "Inductive content analysis is proposed for situations lacking prior studies or where research is fragmented. Conversely, a deductive approach is recommended when the goal is to test existing theories in different contexts or to compare categories over time.",
    "year": 2008,
    "journal": "Journal of Advanced Nursing",
    "citations": {
      "total": 14376,
      "supporting": 25,
      "contradicting": 3,
      "mentioning": 13934,
      "unclassified": 414,
      "citingPublications": 18003
    }
  },
  {
    "query": "Game Theory",
    "title": "The need to belong: Desire for interpersonal attachments as a fundamental human motivation.",
    "doi": "10.1037/0033-2909.117.3.497",
    "problem": "The paper addresses the hypothesized need for humans to form and maintain strong, stable interpersonal relationships, known as the belongingness hypothesis. It examines whether this need is fundamental and pervasive, and explores the negative consequences of lacking social attachments.",
    "solution": "The authors evaluate empirical literature to test the belongingness hypothesis, analyzing evidence from emotional, cognitive, health, and behavioral domains. Their findings support the hypothesis, showing that the need to belong is a powerful and fundamental human motivation, with multiple effects across various aspects of well-being.",
    "year": 1995,
    "journal": "Psychological Bulletin",
    "citations": {
      "total": 13585,
      "supporting": 447,
      "contradicting": 19,
      "mentioning": 12681,
      "unclassified": 438,
      "citingPublications": 18235
    }
  },
  {
    "query": "Game Theory",
    "title": "The Structure and Function of Complex Networks",
    "doi": "10.1137/s003614450342480",
    "problem": "Researchers face challenges in understanding and predicting the behavior of complex networked systems such as the Internet, social networks, and biological networks. These systems exhibit intricate structural and dynamic properties that are not easily captured by traditional models.",
    "solution": "To address this, a variety of new techniques and models have been developed, including concepts like the small-world effect, degree distributions, clustering, network correlations, and random graph models. Additionally, models of network growth, preferential attachment, and analyses of dynamical processes on networks are reviewed to provide deeper insights into these systems.",
    "year": 2003,
    "journal": "Siam Review",
    "citations": {
      "total": 12904,
      "supporting": 148,
      "contradicting": 11,
      "mentioning": 12318,
      "unclassified": 427,
      "citingPublications": 17092
    }
  },
  {
    "query": "Game Theory",
    "title": "Adaptation in Natural and Artificial Systems",
    "doi": "10.7551/mitpress/1090.001.0001",
    "problem": "The problem addressed is understanding and modeling adaptation in complex adaptive systems, such as those found in biology, economics, and game theory, where multiple factors interact in nonlinear ways. Traditional mathematical genetics and existing models struggle to account for the complexity and nonlinearity of these systems.",
    "solution": "The solution proposed is a mathematical model based on genetic algorithms that captures the nonlinearity and complexity of adaptive processes. This model is demonstrated to be universal by applying it to various fields, including economics and game theory, and it explains phenomena like coadaptation, coevolution, and the emergence of building blocks (schemata) that drive innovation and improvement.",
    "year": 1992,
    "journal": "",
    "citations": {
      "total": 11316,
      "supporting": 12,
      "contradicting": 1,
      "mentioning": 10661,
      "unclassified": 642,
      "citingPublications": 20531
    }
  },
  {
    "query": "Game Theory",
    "title": "Executive Functions",
    "doi": "10.1146/annurev-psych-113011-143750",
    "problem": "The paper addresses the challenge of understanding executive functions (EFs), their core components, developmental progression, and how they are affected by factors such as stress, sleep, social, emotional, and physical health. It also discusses controversies regarding the relationships between EFs and related cognitive constructs. Additionally, it considers the impact of various impairments on EFs.",
    "solution": "The paper proposes examining the developmental progression and representative measures of EFs, addressing related controversies, and discussing the importance of overall health for cognitive function. It also explores the trainability of EFs and reviews diverse methods that have been tried to improve them through practice.",
    "year": 2013,
    "journal": "Annual Review of Psychology",
    "citations": {
      "total": 10995,
      "supporting": 213,
      "contradicting": 26,
      "mentioning": 9916,
      "unclassified": 840,
      "citingPublications": 10918
    }
  },
  {
    "query": "Deseasonalization",
    "title": "ADVANCED SPECTRAL METHODS FOR CLIMATIC TIME SERIES",
    "doi": "10.1029/2000rg000092",
    "problem": "Analyzing univariate and multivariate time series is essential for understanding and predicting climatic variability, but extracting useful information from these data remains challenging. There is a need for effective methods to enhance signal-to-noise ratio and to interpret time series data in terms of underlying dynamical systems. Additionally, there are open questions regarding the best approaches for spectral analysis and the application of these methods to important climatic indices.",
    "solution": "The paper reviews and describes novel methods for time series analysis, particularly focusing on connections with nonlinear dynamics, signal-to-noise enhancement, and advanced spectral analysis techniques. These methods are demonstrated using the Southern Oscillation Index and sea surface temperature datasets to showcase their application and effectiveness. The review also discusses the advantages and disadvantages of these approaches and highlights open questions and future prospects in the field.",
    "year": 2002,
    "journal": "Reviews of Geophysics",
    "citations": {
      "total": 1892,
      "supporting": 13,
      "contradicting": 0,
      "mentioning": 1856,
      "unclassified": 23,
      "citingPublications": 2134
    }
  },
  {
    "query": "Deseasonalization",
    "title": "The Global Methane Budget 2000–2017",
    "doi": "10.5194/essd-12-1561-2020",
    "problem": "Accurately understanding and quantifying the global methane (CH4) budget is challenging due to increasing atmospheric concentrations, the complexity of geographically overlapping sources, and uncertainties in the destruction of CH4 by hydroxyl radicals. The main sources of uncertainty are natural emissions, particularly from wetlands and inland waters, and discrepancies between top-down and bottom-up emission estimates. These uncertainties hinder the assessment of realistic pathways to mitigate climate change.",
    "solution": "To address these challenges, a consortium of multidisciplinary scientists under the Global Carbon Project has synthesized and regularly updated the global methane budget by integrating top-down atmospheric observations with bottom-up process-based models and inventories. The study presents an updated decadal methane budget, identifies key uncertainties, and proposes priorities for improvement, including high-resolution mapping of methane-emitting habitats, enhanced process-based modeling, intensified local and regional observations, improved transport and sink modeling, and the development of advanced inversion systems using isotopic tracers.",
    "year": 2020,
    "journal": "Earth System Science Data",
    "citations": {
      "total": 1752,
      "supporting": 59,
      "contradicting": 13,
      "mentioning": 1664,
      "unclassified": 16,
      "citingPublications": 2245
    }
  },
  {
    "query": "Deseasonalization",
    "title": "On the Relation between the Expected Value and the Volatility of the Nominal Excess Return on Stocks",
    "doi": "10.1111/j.1540-6261.1993.tb05128.x",
    "problem": "The relationship between risk and return over time, specifically whether investors require a larger risk premium during periods of higher risk, remains unresolved. Previous studies using the standard GARCH-M model have produced conflicting results, with some finding a positive or zero relation and others a negative relation between expected return and conditional variance. The standard GARCH-M model may not adequately capture the time series properties of monthly excess stock returns.",
    "solution": "The authors propose a modified GARCH-M model that incorporates seasonal patterns in volatility, allows positive and negative return innovations to have different impacts on conditional volatility, and uses nominal interest rates to predict conditional variance. By employing this more general specification, they aim to better capture the dynamics of monthly excess returns and resolve the conflicting findings regarding the risk-return relationship.",
    "year": 1993,
    "journal": "The Journal of Finance",
    "citations": {
      "total": 1286,
      "supporting": 36,
      "contradicting": 3,
      "mentioning": 1223,
      "unclassified": 24,
      "citingPublications": 6733
    }
  },
  {
    "query": "Deseasonalization",
    "title": "Methane emissions from wetlands: biogeochemical, microbial, and modeling perspectives from local to global scales",
    "doi": "10.1111/gcb.12131",
    "problem": "Methane (CH4) emissions from wetlands, which are the largest natural source, are a significant contributor to global warming, yet current understanding and modeling of CH4 dynamics are limited. Major uncertainties arise from inadequate incorporation of key biogeochemical controls, errors due to extrapolations from heterogeneous wetland complexes, and a limited number of observations constraining models. These issues hinder accurate estimation of current and future CH4 emissions from natural ecosystems.",
    "solution": "The paper proposes integrating methodologies and spatial scales from biogeochemistry, molecular microbiology, and modeling to better understand CH4 dynamics in wetlands. It aims to synthesize current estimates, summarize major controls, suggest new research frontiers, examine methanogen community relationships, and review CH4 models. This integrative approach is highlighted as a promising direction to address uncertainties and improve predictions of CH4 emissions.",
    "year": 2013,
    "journal": "Global Change Biology",
    "citations": {
      "total": 996,
      "supporting": 24,
      "contradicting": 4,
      "mentioning": 962,
      "unclassified": 6,
      "citingPublications": 1104
    }
  },
  {
    "query": "Deseasonalization",
    "title": "A Nonparametric Trend Test for Seasonal Data With Serial Dependence",
    "doi": "10.1029/wr020i006p00727",
    "problem": "Statistical tests for detecting monotonic trends in seasonal hydrologic time series are often confounded by issues such as nonnormal data, missing values, seasonality, censoring due to detection limits, and serial dependence. These challenges make it difficult to accurately assess trends in such datasets.",
    "solution": "The paper presents an extension of the Mann-Kendall test for trend that is based entirely on ranks, making it robust against nonnormality and censoring. This method can handle seasonality and missing values without theoretical or computational problems, and Monte Carlo experiments show it is generally robust against serial correlation except in cases of strong long-term persistence or very short records.",
    "year": 1984,
    "journal": "Water Resources Research",
    "citations": {
      "total": 887,
      "supporting": 8,
      "contradicting": 1,
      "mentioning": 863,
      "unclassified": 15,
      "citingPublications": 1536
    }
  },
  {
    "query": "Deseasonalization",
    "title": "The global methane budget 2000–2012",
    "doi": "10.5194/essd-8-697-2016",
    "problem": "The global methane budget is not well understood, with unexplained changes in atmospheric methane over the past decade and increasing emissions making it difficult to manage climate change mitigation pathways. Major challenges include the overlapping and diverse sources of methane emissions, as well as uncertainties in methane destruction by hydroxyl radicals. These issues lead to significant uncertainties, especially regarding emissions from wetlands and inland waters.",
    "solution": "A consortium of multi-disciplinary scientists under the Global Carbon Project has been established to synthesize and stimulate research on the methane cycle, producing regular updates of the global methane budget. This approach integrates top-down atmospheric observations and inverse modeling with bottom-up models, inventories, and data-driven methods. The consortium prioritizes improving process-based models for inland-water emissions, enhancing methane observations at various scales, refining estimates of atmospheric loss by OH, and improving transport models in top-down inversions.",
    "year": 2016,
    "journal": "Earth System Science Data",
    "citations": {
      "total": 869,
      "supporting": 40,
      "contradicting": 9,
      "mentioning": 817,
      "unclassified": 3,
      "citingPublications": 1097
    }
  },
  {
    "query": "Deseasonalization",
    "title": "Modeling and Forecasting Realized Volatility",
    "doi": "10.1111/1468-0262.00418",
    "problem": "Traditional methods for modeling and forecasting financial asset return volatilities, correlations, and distributions rely on restrictive and complex parametric multivariate ARCH or stochastic volatility models, which often perform poorly at intraday frequencies. These limitations hinder accurate measurement and forecasting of volatility and return distributions, especially when using high-frequency intraday data.",
    "solution": "The paper proposes a general framework that integrates high-frequency intraday data by constructing realized volatility from intraday returns, allowing the use of traditional time series models for forecasting. By leveraging continuous-time arbitrage-free price process theory and quadratic variation, the authors link realized volatility to the conditional covariance matrix and demonstrate that a long-memory Gaussian vector autoregression for daily realized volatilities provides superior forecasts compared to standard models. This approach, combined with a parametric lognormal-normal mixture distribution, yields well-calibrated density and quantile forecasts for future returns, enhancing practical modeling and risk management applications.",
    "year": 2003,
    "journal": "Econometrica",
    "citations": {
      "total": 843,
      "supporting": 14,
      "contradicting": 1,
      "mentioning": 815,
      "unclassified": 13,
      "citingPublications": 3392
    }
  },
  {
    "query": "Deseasonalization",
    "title": "Recent variability of the global ocean carbon sink",
    "doi": "10.1002/2014gb004853",
    "problem": "Accurately estimating the global oceanic carbon dioxide (CO2) sink and its temporal variations is challenging due to limited observations and spatial coverage, especially at high latitudes. Existing climatologies may not fully capture seasonal or interannual variability, and there are uncertainties in quantifying the ocean's role in the global carbon cycle.",
    "solution": "The researchers developed a new observation-based estimate of the global oceanic CO2 sink using a neural network-based mapping of surface ocean pCO2 observations from the Surface Ocean CO2 Atlas database. This approach provides monthly, 1×1 spatial resolution estimates from 1998 to 2011, showing improved agreement with independent observations and enabling detailed analysis of seasonal and interannual variability in the ocean carbon sink.",
    "year": 2014,
    "journal": "Global Biogeochemical Cycles",
    "citations": {
      "total": 776,
      "supporting": 66,
      "contradicting": 7,
      "mentioning": 700,
      "unclassified": 3,
      "citingPublications": 576
    }
  },
  {
    "query": "Deseasonalization",
    "title": "Statistical and Machine Learning forecasting methods: Concerns and ways forward",
    "doi": "10.1371/journal.pone.0194889",
    "problem": "There is limited evidence regarding the relative performance of machine learning (ML) methods compared to traditional statistical methods for time series forecasting, particularly in terms of accuracy and computational requirements. This lack of comparative analysis makes it difficult to determine the most effective forecasting approach across different horizons.",
    "solution": "The paper evaluates the performance of popular ML methods versus eight traditional statistical methods using a large set of monthly time series from the M3 Competition. By comparing post-sample accuracy and computational requirements, the study finds that statistical methods outperform ML methods in both respects and suggests the need for objective, unbiased testing through large, open competitions.",
    "year": 2018,
    "journal": "Plos One",
    "citations": {
      "total": 642,
      "supporting": 26,
      "contradicting": 2,
      "mentioning": 598,
      "unclassified": 16,
      "citingPublications": 1147
    }
  },
  {
    "query": "Deseasonalization",
    "title": "Good Day Sunshine: Stock Returns and the Weather",
    "doi": "10.1111/1540-6261.00556",
    "problem": "The paper addresses whether morning sunshine in the city of a country's leading stock exchange affects daily market index returns, a relationship that challenges the traditional efficient markets hypothesis. It questions if sunlight, by influencing mood, can impact stock prices in modern economies where agriculture is not dominant. The issue is whether such mood effects, unrelated to economic fundamentals, can explain stock market movements.",
    "solution": "The study empirically examines the correlation between sunshine and stock returns across 26 countries from 1982 to 1997, finding a strong and significant relationship. It controls for other weather variables like rain and snow, which are found to be unrelated to returns. The findings suggest that weather-based trading strategies could be optimal with low transaction costs, but the effect disappears with modest costs, indicating that mood influenced by sunshine may irrationally affect stock prices.",
    "year": 2003,
    "journal": "The Journal of Finance",
    "citations": {
      "total": 618,
      "supporting": 42,
      "contradicting": 6,
      "mentioning": 562,
      "unclassified": 8,
      "citingPublications": 1572
    }
  }
]