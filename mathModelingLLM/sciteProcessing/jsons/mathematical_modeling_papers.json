{
  "TOPSIS": {
    "query": "TOPSIS",
    "count": 10,
    "papers": [
      {
        "title": "CMS Collaboration",
        "doi": "10.1016/s0375-9474(14)00602-2",
        "year": 2014,
        "journal": "Nuclear Physics A",
        "abstract": "The Scientific and Technical Research Council of Turkey, and Turkish Atomic Energy Authorit",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2974,
          "supporting": 146,
          "contradicting": 6,
          "mentioning": 2819,
          "unclassified": 3,
          "citingPublications": 1279
        }
      },
      {
        "title": "Compromise solution by MCDM methods: A comparative analysis of VIKOR and <strong class=\"highlight\">TOPSIS</strong>",
        "doi": "10.1016/s0377-2217(03)00020-1",
        "year": 2004,
        "journal": "European Journal of Operational Research",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2247,
          "supporting": 5,
          "contradicting": 0,
          "mentioning": 2115,
          "unclassified": 127,
          "citingPublications": 4233
        }
      },
      {
        "title": "CMS Collaboration",
        "doi": "10.1016/s0375-9474(14)00570-3",
        "year": 2014,
        "journal": "Nuclear Physics A",
        "abstract": "The Scientific and Technical Research Council of Turkey, and Turkish Atomic Energy Authorit",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2225,
          "supporting": 97,
          "contradicting": 1,
          "mentioning": 2122,
          "unclassified": 5,
          "citingPublications": 913
        }
      },
      {
        "title": "Extensions of the <strong class=\"highlight\">TOPSIS</strong> for group decision-making under fuzzy environment",
        "doi": "10.1016/s0165-0114(97)00377-1",
        "year": 2000,
        "journal": "Fuzzy Sets and Systems",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2099,
          "supporting": 4,
          "contradicting": 0,
          "mentioning": 1942,
          "unclassified": 153,
          "citingPublications": 3427
        }
      },
      {
        "title": "Rapid, Low-Cost Detection of Zika Virus Using Programmable Biomolecular Components",
        "doi": "10.1016/j.cell.2016.04.059",
        "year": 2016,
        "journal": "Cell",
        "abstract": "The recent Zika virus outbreak highlights the need for low-cost diagnostics that can be rapidly developed for distribution and use in pandemic regions. Here, we report a pipeline for the rapid design, assembly, and validation of cell-free, paper-based sensors for the detection of the Zika virus RNA genome. By linking isothermal RNA amplification to toehold switch RNA sensors, we detect clinically relevant concentrations of Zika virus sequences and demonstrate specificity against closely related Dengue virus sequences. When coupled with a novel CRISPR/Cas9-based module, our sensors can discriminate between viral strains with single-base resolution. We successfully demonstrate a simple, field-ready sample-processing workflow and detect Zika virus from the plasma of a viremic macaque. Our freeze-dried biomolecular platform resolves important practical limitations to the deployment of molecular diagnostics in the field and demonstrates how synthetic biology can be used to develop diagnostic tools for confronting global health crises. PAPERCLIP.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1368,
          "supporting": 9,
          "contradicting": 1,
          "mentioning": 1356,
          "unclassified": 2,
          "citingPublications": 1287
        }
      },
      {
        "title": "Measurements of the Higgs boson production and decay rates and constraints on its couplings from a combined ATLAS and CMS analysis of the LHC pp collision data at s = 7 $$ \\sqrt{s}=7 $$ and 8 TeV",
        "doi": "10.1007/jhep08(2016)045",
        "year": 2016,
        "journal": "Journal of High Energy Physics",
        "abstract": "Measurements of the Higgs boson production and decay rates and constraints on its couplings from a combined ATLAS and CMS analysis of the LHC pp collision data at √ s = 7 and 8 TeV The ATLAS and CMS collaborations Abstract: Combined ATLAS and CMS measurements of the Higgs boson production and decay rates, as well as constraints on its couplings to vector bosons and fermions, are presented. The combination is based on the analysis of five production processes, namely gluon fusion, vector boson fusion, and associated production with a W or a Z boson or a pair of top quarks, and of the six decay modes H → ZZ, W W , γγ, τ τ, bb, and µµ. All results are reported assuming a value of 125.09 GeV for the Higgs boson mass, the result of the combined measurement by the ATLAS and CMS experiments. The analysis uses the CERN LHC proton-proton collision data recorded by the ATLAS and CMS experiments in 2011 and 2012, corresponding to integrated luminosities per experiment of approximately 5 fb −1 at √ s = 7 TeV and 20 fb −1 at √ s = 8 TeV. The Higgs boson production and decay rates measured by the two experiments are combined within the context of three generic parameterisations: two based on cross sections and branching fractions, and one on ratios of coupling modifiers. Several interpretations of the measurements with more model-dependent parameterisations are also given. The combined signal yield relative to the Standard Model prediction is measured to be 1.09 ± 0.11. The combined measurements lead to observed significances for the vector boson fusion production process and for the H → τ τ decay of 5.4 and 5.5 standard deviations, respectively. The data are consistent with the Standard Model predictions for all parameterisations considered. Production Event generator process ATLAS CMS ggF Powheg [80-84] Powheg VBF Powheg Powheg W H Pythia8 [85] Pythia6.4 [86] ZH (qq → ZH or qg → ZH) Pythia8 Pythia6.4 ggZH (gg → ZH) Powheg See text ttH Powhel [88] Pythia6.4 where all κ j values equal unity in the SM; here, by construction, the SM cross sections and branching fractions include the best available higher-order QCD and EW corrections. This higher-order accuracy is not necessarily preserved for κ j values different from unity, but the dominant higher-order QCD corrections factorise to a large extent from any rescaling of the coupling strengths and are therefore assumed to remain valid over the entire range of κ j values considered in this paper. Different production processes and decay modes probe different coupling modifiers, as can be visualised from the Feynman diagrams shown in figures 1-6. Individual coupling modifiers, corresponding to tree-level Higgs boson couplings to the different particles, are introduced, as well as two effective coupling modifiers, κ g and κ γ , which describe the loop processes for ggF production and H → γγ decay. This is possible because BSM particles that might be present in these loops are not expected to appreciably change the kinematics of the corresponding process. The gg → H and H → γγ loop p...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1269,
          "supporting": 65,
          "contradicting": 1,
          "mentioning": 1200,
          "unclassified": 3,
          "citingPublications": 1288
        }
      },
      {
        "title": "Extension of <strong class=\"highlight\">TOPSIS</strong> to Multiple Criteria Decision Making with Pythagorean Fuzzy Sets",
        "doi": "10.1002/int.21676",
        "year": 2014,
        "journal": "International Journal of Intelligent Systems",
        "abstract": "Recently, a new model based on Pythagorean fuzzy set (PFS) has been presented to manage the uncertainty in real‐world decision‐making problems. PFS has much stronger ability than intuitionistic fuzzy set to model such uncertainty. In this paper, we define some novel operational laws of PFSs and discuss their desirable properties. For the multicriteria decision‐making problems with PFSs, we propose an extended technique for order preference by similarity to ideal solution method to deal effectively with them. In this approach, we first propose a score function based comparison method to identify the Pythagorean fuzzy positive ideal solution and the Pythagorean fuzzy negative ideal solution. Then, we define a distance measure to calculate the distances between each alternative and the Pythagorean fuzzy positive ideal solution as well as the Pythagorean fuzzy negative ideal solution, respectively. Afterward, a revised closeness is introduced to identify the optimal alternative. At length, a practical example is given to illustrate the developed method and to make a comparative analysis.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1110,
          "supporting": 5,
          "contradicting": 2,
          "mentioning": 1099,
          "unclassified": 4,
          "citingPublications": 1396
        }
      },
      {
        "title": "Supply chain risk management: a literature review",
        "doi": "10.1080/00207543.2015.1030467",
        "year": 2015,
        "journal": "International Journal of Production Research",
        "abstract": "Risk management plays a vital role in effectively operating supply chains in the presence of a variety of uncertainties. Over the years, many researchers have focused on supply chain risk management (SCRM) by contributing in the areas of defining, operationalizing, and mitigating risks. In this paper, we review and synthesize the extant literature in SCRM in the past decade in a comprehensive manner. The purpose of this paper is threefold. First, we present and categorize SCRM research appearing between 2003 and 2013. Second, we undertake a detailed review associated with research developments in supply chain risk definitions, risk types, risk factors, and risk management/mitigation strategies. Third, we analyze the SCRM literature in exploring potential gaps.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1042,
          "supporting": 12,
          "contradicting": 0,
          "mentioning": 959,
          "unclassified": 71,
          "citingPublications": 1178
        }
      },
      {
        "title": "Combined Measurement of the Higgs Boson Mass inppCollisions ats=7and 8 TeV with the ATLAS and CMS Experiments",
        "doi": "10.1103/physrevlett.114.191803",
        "year": 2015,
        "journal": "Physical Review Letters",
        "abstract": "A measurement of the Higgs boson mass is presented based on the combined data samples of the ATLAS and CMS experiments at the CERN LHC in the H → γγ and H → ZZ → 4l decay channels. The results are obtained from a simultaneous fit to the reconstructed invariant mass peaks in the two channels and for the two experiments. The measured masses from the individual channels and the two experiments are found to be consistent among themselves. The combined measured mass of the Higgs boson is m H ¼ 125.09 AE 0.21 ðstatÞ AE 0.11 ðsystÞ GeV. DOI: 10.1103/PhysRevLett.114.191803  PACS numbers: 14.80.Bn, 13.85.Qk    The study of the mechanism of electroweak symmetry breaking is one of the principal goals of the CERN LHC program. In the standard model (SM), this symmetry breaking is achieved through the introduction of a complex doublet scalar field, leading to the prediction of the Higgs boson H [1-6], whose mass m H is, however, not predicted by the theory. In 2012, the ATLAS and CMS Collaborations at the LHC announced the discovery of a particle with Higgs-boson-like properties and a mass of about 125 GeV [7][8][9]. The discovery was based primarily on mass peaks observed in the γγ and ZZ → l þ l − l 0þ l 0−(denoted H → ZZ → 4l for simplicity) decay channels, where one or both of the Z bosons can be off shell and where l and l 0 denote an electron or muon. With m H known, all properties of the SM Higgs boson, such as its production cross section and partial decay widths, can be predicted. Increasingly precise measurements [10][11][12][13] have established that all observed properties of the new particle, including its spin, parity, and coupling strengths to SM particles are consistent within the uncertainties with those expected for the SM Higgs boson.The ATLAS and CMS Collaborations have independently measured m H using the samples of proton-proton collision data collected in 2011 and 2012, commonly referred to as LHC Run 1. The analyzed samples correspond to approximately 5 fb −1 of integrated luminosity at ffiffi ffi s p ¼ 7 TeV, and 20 fb −1 at ffiffi ffi s p ¼ 8 TeV, for each experiment. Combined results in the context of the separate experiments, as well as those in the individual channels, are presented in Refs. [12,[14][15][16].This Letter describes a combination of the Run 1 data from the two experiments, leading to improved precision for m H . Besides its intrinsic importance as a fundamental parameter, improved knowledge of m H yields more precise predictions for the other Higgs boson properties. Furthermore, the combined mass measurement provides a first step towards combinations of other quantities, such as the couplings. In the SM, m H is related to the values of the masses of the W boson and top quark through loopinduced effects. Taking into account other measured SM quantities, the comparison of the measurements of the Higgs boson, W boson, and top quark masses can be used to directly test the consistency of the SM [17] and thus to search for evidence of physics beyond the SM.The combination is performed usin...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1040,
          "supporting": 39,
          "contradicting": 0,
          "mentioning": 992,
          "unclassified": 9,
          "citingPublications": 1376
        }
      },
      {
        "title": "A state-of the-art survey of <strong class=\"highlight\">TOPSIS</strong> applications",
        "doi": "10.1016/j.eswa.2012.05.056",
        "year": 2012,
        "journal": "Expert Systems With Applications",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 966,
          "supporting": 3,
          "contradicting": 0,
          "mentioning": 925,
          "unclassified": 38,
          "citingPublications": 2120
        }
      }
    ]
  },
  "GRA": {
    "query": "GRA",
    "count": 10,
    "papers": [
      {
        "title": "Dye-Sensitized Solar Cells",
        "doi": "10.1021/cr900356p",
        "year": 2010,
        "journal": "Chemical Reviews",
        "abstract": "He obtained his Ph.D. in 1993 at Uppsala University and was a postdoctoral fellow with Prof. Michael <strong class=\"highlight\">Gra</strong> ¨tzel (1993-1994) at EPFL, Switzerland. His research focuses on physical chemical characterization of mesoporous electrodes for different types of optoelectronic devices, specifically dye-sensitized solar cells. He has about 200 scientific publications and 8 patent applications. He is a member of the Royal Swedish Academy of Engineering Sciences (IVA), Stockholm, and a visiting professor at the",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 7955,
          "supporting": 73,
          "contradicting": 8,
          "mentioning": 7778,
          "unclassified": 96,
          "citingPublications": 8692
        }
      },
      {
        "title": "Solar Water Splitting Cells",
        "doi": "10.1021/cr1002326",
        "year": 2010,
        "journal": "Chemical Reviews",
        "abstract": "earned a B.S. degree in chemistry from the University of Dayton in 2001 and as an undergraduate worked on conductive polymer syntheses at the Air Force Research Laboratory at Wright Patterson Air Force Base. He completed an M.S. degree in 2004 and Ph.D. degree in 2008 at Portland State University and joined the Lewis group at Caltech in 2008. He is currently an NSF-ACCF postdoctoral fellow (2009) and has been studying the electrical characteristics of inorganic semiconductors in contact with conductive polymers. His research interests include molecular semiconductors for solar energy conversion, porphyrin macrocycles for optoelectronic applications, and catalyst materials for photoelectrolysis. Emily L. Warren received a B.S. in chemical engineering at Cornell University in 2005. She received an M.Phil in Engineering for Sustainable Development from Cambridge University in 2006. She is currently a graduate student in Chemical Engineering at the California Institute of Technology. Her research interests include semiconductor photoelectrochemistry, solar energy conversion, and semiconductor nanowires. She is currently a graduate student in Chemical Engineering at the California Institute of Technology, working under Nathan S. Lewis. James R. McKone is in his third year of graduate studies in the Division of Chemistry and Chemical Engineering at the California Institute of Technology, working under Nathan S. Lewis and Harry B. Gray. In 2008 he graduated from Saint Olaf College with a Bachelor of Arts degree, double-majoring in music and chemistry. His current research focuses on semiconductor-coupled heterogeneous catalysis of the hydrogen evolution reaction using mixtures of earth-abundant transition metals. Shannon W. Boettcher earned his B.A. degree in chemistry from the University of Oregon, Eugene (2003), and, working with Galen Stucky, his Ph.D. in Inorganic Chemistry from the University of California, Santa Barbara (2008). Following postdoctoral work with Nate Lewis and Harry Atwater at the California Institute of Technology (2008-2010), he returned to the University of Oregon to join the faculty as an Assistant Professor. His research interests span synthesis and physical measurement with the goal of designing and understanding solid-state inorganic material architectures for use in solar-energy conversion and storage.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 7554,
          "supporting": 40,
          "contradicting": 2,
          "mentioning": 7489,
          "unclassified": 23,
          "citingPublications": 9000
        }
      },
      {
        "title": "Titanium Dioxide Nanomaterials:  Synthesis, Properties, Modifications, and Applications",
        "doi": "10.1021/cr0500535",
        "year": 2007,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 6699,
          "supporting": 80,
          "contradicting": 2,
          "mentioning": 6515,
          "unclassified": 102,
          "citingPublications": 10198
        }
      },
      {
        "title": "Bounding the role of black carbon in the climate system: A scientific assessment",
        "doi": "10.1002/jgrd.50171",
        "year": 2013,
        "journal": "Journal of Geophysical Research Atmospheres",
        "abstract": "[1] Black carbon aerosol plays a unique and important role in Earth's climate system. Black carbon is a type of carbonaceous material with a unique combination of physical properties. This assessment provides an evaluation of black-carbon climate forcing that is comprehensive in its inclusion of all known and relevant processes and that is quantitative in providing best estimates and uncertainties of the main forcing terms: direct solar absorption; influence on liquid, mixed phase, and ice clouds; and deposition on snow and ice. These effects are calculated with climate models, but when possible, they are evaluated with both microphysical measurements and field observations. Predominant sources are combustion related, namely, fossil fuels for transportation, solid fuels for industrial and residential uses, and open burning of biomass. Total global emissions of black carbon using bottom-up inventory methods are 7500 Gg yr À1 in the year 2000 with an uncertainty range of 2000 to 29000. However, global atmospheric absorption attributable to black",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 5422,
          "supporting": 147,
          "contradicting": 20,
          "mentioning": 5212,
          "unclassified": 43,
          "citingPublications": 6266
        }
      },
      {
        "title": "Semiconductor-based Photocatalytic Hydrogen Generation",
        "doi": "10.1021/cr1001645",
        "year": 2010,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4745,
          "supporting": 22,
          "contradicting": 1,
          "mentioning": 4686,
          "unclassified": 36,
          "citingPublications": 7477
        }
      },
      {
        "title": "Prospects of Colloidal Nanocrystals for Electronic and Optoelectronic Applications",
        "doi": "10.1021/cr900137k",
        "year": 2009,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3953,
          "supporting": 28,
          "contradicting": 3,
          "mentioning": 3896,
          "unclassified": 26,
          "citingPublications": 3993
        }
      },
      {
        "title": "Applications of ionic liquids in the chemical industry",
        "doi": "10.1039/b006677j",
        "year": 2008,
        "journal": "Chemical Society Reviews",
        "abstract": "In contrast to a recently expressed, and widely cited, view that \"Ionic liquids are starting to leave academic labs and find their way into a wide variety of industrial applications\", we demonstrate in this critical review that there have been parallel and collaborative exchanges between academic research and industrial developments since the materials were first reported in 1914 (148 references).",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3843,
          "supporting": 15,
          "contradicting": 0,
          "mentioning": 3782,
          "unclassified": 46,
          "citingPublications": 5382
        }
      },
      {
        "title": "Chemical Routes for the Transformation of Biomass into Chemicals",
        "doi": "10.1021/cr050989d",
        "year": 2007,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3560,
          "supporting": 14,
          "contradicting": 0,
          "mentioning": 3496,
          "unclassified": 50,
          "citingPublications": 5666
        }
      },
      {
        "title": "Lithium metal anodes for rechargeable batteries",
        "doi": "10.1039/c3ee40795k",
        "year": 2014,
        "journal": "Energy & Environmental Science",
        "abstract": "Lithium (Li) metal is an ideal anode material for rechargeable batteries due to its extremely high theoretical specific capacity (3860 mA h g À1 ), low density (0.59 g cm À3 ) and the lowest negative electrochemical potential (À3.040 V vs. the standard hydrogen electrode). Unfortunately, uncontrollable dendritic Li growth and limited Coulombic efficiency during Li deposition/stripping inherent in these batteries have prevented their practical applications over the past 40 years. With the emergence of post-Li-ion batteries, safe and efficient operation of Li metal anodes has become an enabling technology which may determine the fate of several promising candidates for the next generation energy storage systems, including rechargeable Li-air batteries, Li-S batteries, and Li metal batteries which utilize intercalation compounds as cathodes. In this paper, various factors that affect the morphology and Coulombic efficiency of Li metal anodes have been analyzed. Technologies utilized to characterize the morphology of Li deposition and the results obtained by modelling of Li dendrite growth have also been reviewed.Finally, recent development and urgent need in this field are discussed.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3367,
          "supporting": 24,
          "contradicting": 1,
          "mentioning": 3327,
          "unclassified": 15,
          "citingPublications": 4326
        }
      },
      {
        "title": "Cell Movement Is Guided by the Rigidity of the Substrate",
        "doi": "10.1016/s0006-3495(00)76279-5",
        "year": 2000,
        "journal": "Biophysical Journal",
        "abstract": "Directional cell locomotion is critical in many physiological processes, including morphogenesis, the immune response, and wound healing. It is well known that in these processes cell movements can be guided by gradients of various chemical signals. In this study, we demonstrate that cell movement can also be guided by purely physical interactions at the cell-substrate interface. We cultured National Institutes of Health 3T3 fibroblasts on flexible polyacrylamide sheets coated with type I collagen. A transition in rigidity was introduced in the central region of the sheet by a discontinuity in the concentration of the bis-acrylamide cross-linker. Cells approaching the transition region from the soft side could easily migrate across the boundary, with a concurrent increase in spreading area and traction forces. In contrast, cells migrating from the stiff side turned around or retracted as they reached the boundary. We call this apparent preference for a stiff substrate \"durotaxis.\" In addition to substrate rigidity, we discovered that cell movement could also be guided by manipulating the flexible substrate to produce mechanical strains in the front or rear of a polarized cell. We conclude that changes in tissue rigidity and strain could play an important controlling role in a number of normal and pathological processes involving cell locomotion.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3124,
          "supporting": 156,
          "contradicting": 17,
          "mentioning": 2938,
          "unclassified": 13,
          "citingPublications": 3133
        }
      }
    ]
  },
  "AHP Weights": {
    "query": "AHP Weights",
    "count": 10,
    "papers": [
      {
        "title": "Global and regional diabetes prevalence estimates for 2019 and projections for 2030 and 2045: Results from the International Diabetes Federation Diabetes Atlas, 9th edition",
        "doi": "10.1016/j.diabres.2019.107843",
        "year": 2019,
        "journal": "Diabetes Research and Clinical Practice",
        "abstract": "To provide global estimates of diabetes prevalence for 2019 and projections for 2030 and 2045. Methods: A total of 255 high-quality data sources, published between 1990 and 2018 and representing 138 countries were identified. For countries without high quality in-country data, estimates were extrapolated from similar countries matched by economy, ethnicity, geography and language. Logistic regression was used to generate smoothed age-specific diabetes prevalence estimates (including previously undiagnosed diabetes) in adults aged 20-79 years. Results: The global diabetes prevalence in 2019 is estimated to be 9.3% (463 million people), rising to 10.2% (578 million) by 2030 and 10.9% (700 million) by 2045. The prevalence is higher in urban (10.8%) than rural (7.2%) areas, and in high-income (10.4%) than lowincome countries (4.0%). One in two (50.1%) people living with diabetes do not know that they have diabetes. The global prevalence of impaired glucose tolerance is estimated to",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 6165,
          "supporting": 39,
          "contradicting": 10,
          "mentioning": 5923,
          "unclassified": 193,
          "citingPublications": 8885
        }
      },
      {
        "title": "GABAergic Interneurons in the Neocortex: From Cellular Properties to Circuits",
        "doi": "10.1016/j.neuron.2016.06.033",
        "year": 2016,
        "journal": "Neuron",
        "abstract": "Cortical networks are composed of glutamatergic excitatory projection neurons and local GABAergic inhibitory interneurons which gate signal flow and sculpt network dynamics. Although they represent a minority of the total neocortical neuronal population, GABAergic interneurons are highly heterogeneous, forming functional classes based on their morphological, electrophysiological and molecular features as well as connectivity and in vivo patterns of activity. Here we review our current understanding of neocortical interneuron diversity and the properties that distinguish among cell types. We then discuss how the involvement of multiple cell types, each with a specific set of cellular properties, plays a crucial role in diversifying and increasing the computational power of a relatively small number of simple circuit motifs forming cortical networks. We illustrate how recent advances in the field have shed light onto the mechanisms by which GABAergic inhibition contributes to network operations.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2381,
          "supporting": 111,
          "contradicting": 6,
          "mentioning": 2243,
          "unclassified": 21,
          "citingPublications": 1958
        }
      },
      {
        "title": "Metabotropic Glutamate Receptors: Physiology, Pharmacology, and Disease",
        "doi": "10.1146/annurev.pharmtox.011008.145533",
        "year": 2010,
        "journal": "The Annual Review of Pharmacology and Toxicology",
        "abstract": "AbstractThe metabotropic glutamate receptors (mGluRs) are family C G-protein-coupled receptors that participate in the modulation of synaptic transmission and neuronal excitability throughout the central nervous system. The mGluRs bind glutamate within a large extracellular domain and transmit signals through the receptor protein to intracellular signaling partners. A great deal of progress has been made in determining the mechanisms by which mGluRs are activated, proteins with which they interact, and orthosteric and allosteric ligands that can modulate receptor activity. The widespread expression of mGluRs makes these receptors particularly attractive drug targets, and recent studies continue to validate the therapeutic utility of mGluR ligands in neurological and psychiatric disorders such as Alzheimer's disease, Parkinson's disease, anxiety, depression, and schizophrenia.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1715,
          "supporting": 26,
          "contradicting": 1,
          "mentioning": 1682,
          "unclassified": 6,
          "citingPublications": 1691
        }
      },
      {
        "title": "Molecular Physiology of Low-Voltage-Activated T-type Calcium Channels",
        "doi": "10.1152/physrev.00018.2002",
        "year": 2003,
        "journal": "Physiological Reviews",
        "abstract": "T-type Ca2+ channels were originally called low-voltage-activated (LVA) channels because they can be activated by small depolarizations of the plasma membrane. In many neurons Ca2+ influx through LVA channels triggers low-threshold spikes, which in turn triggers a burst of action potentials mediated by Na+ channels. Burst firing is thought to play an important role in the synchronized activity of the thalamus observed in absence epilepsy, but may also underlie a wider range of thalamocortical dysrhythmias. In addition to a pacemaker role, Ca2+ entry via T-type channels can directly regulate intracellular Ca2+ concentrations, which is an important second messenger for a variety of cellular processes. Molecular cloning revealed the existence of three T-type channel genes. The deduced amino acid sequence shows a similar four-repeat structure to that found in high-voltage-activated (HVA) Ca2+ channels, and Na+ channels, indicating that they are evolutionarily related. Hence, the α1-subunits of T-type channels are now designated Cav3. Although mRNAs for all three Cav3 subtypes are expressed in brain, they vary in terms of their peripheral expression, with Cav3.2 showing the widest expression. The electrophysiological activities of recombinant Cav3 channels are very similar to native T-type currents and can be differentiated from HVA channels by their activation at lower voltages, faster inactivation, slower deactivation, and smaller conductance of Ba2+. The Cav3 subtypes can be differentiated by their kinetics and sensitivity to block by Ni2+. The goal of this review is to provide a comprehensive description of T-type currents, their distribution, regulation, pharmacology, and cloning.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1583,
          "supporting": 67,
          "contradicting": 5,
          "mentioning": 1492,
          "unclassified": 19,
          "citingPublications": 1574
        }
      },
      {
        "title": "Recombinant protein expression in Escherichia coli: advances and challenges",
        "doi": "10.3389/fmicb.2014.00172",
        "year": 2014,
        "journal": "Frontiers in Microbiology",
        "abstract": "Escherichia coli is one of the organisms of choice for the production of recombinant proteins. Its use as a cell factory is well-established and it has become the most popular expression platform. For this reason, there are many molecular tools and protocols at hand for the high-level production of heterologous proteins, such as a vast catalog of expression plasmids, a great number of engineered strains and many cultivation strategies. We review the different approaches for the synthesis of recombinant proteins in E. coli and discuss recent progress in this ever-growing field.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1572,
          "supporting": 14,
          "contradicting": 1,
          "mentioning": 1508,
          "unclassified": 49,
          "citingPublications": 2227
        }
      },
      {
        "title": "Multi-criteria decision making approaches for supplier evaluation and selection: A literature review",
        "doi": "10.1016/j.ejor.2009.05.009",
        "year": 2010,
        "journal": "European Journal of Operational Research",
        "abstract": "a b s t r a c tSupplier evaluation and selection problem has been studied extensively. Various decision making approaches have been proposed to tackle the problem. In contemporary supply chain management, the performance of potential suppliers is evaluated against multiple criteria rather than considering a single factor-cost. This paper reviews the literature of the multi-criteria decision making approaches for supplier evaluation and selection. Related articles appearing in the international journals from 2000 to 2008 are gathered and analyzed so that the following three questions can be answered: (i) Which approaches were prevalently applied? (ii) Which evaluating criteria were paid more attention to? (iii) Is there any inadequacy of the approaches? Based on the inadequacy, if any, some improvements and possible future work are recommended. This research not only provides evidence that the multi-criteria decision making approaches are better than the traditional cost-based approach, but also aids the researchers and decision makers in applying the approaches effectively.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1153,
          "supporting": 19,
          "contradicting": 4,
          "mentioning": 1069,
          "unclassified": 61,
          "citingPublications": 1939
        }
      },
      {
        "title": "Reactive oxygen and nitrogen intermediates in the relationship between mammalian hosts and microbial pathogens",
        "doi": "10.1073/pnas.97.16.8841",
        "year": 2000,
        "journal": "Proceedings of the National Academy of Sciences",
        "abstract": "This review summarizes recent evidence from knock-out mice on the role of reactive oxygen intermediates and reactive nitrogen intermediates (RNI) in mammalian immunity. Reflections on redundancy in immunity help explain an apparent paradox: the phagocyte oxidase and inducible nitric oxide synthase are each nonredundant, and yet also mutually redundant, in host defense. In combination, the contribution of these two enzymes appears to be greater than previously appreciated. The remainder of this review focuses on a relatively new field, the basis of microbial resistance to RNI. Experimental tuberculosis provides an important example of an extended, dynamic balance between host and pathogen in which RNI play a major role. In diseases such as tuberculosis, a molecular understanding of host-pathogen interactions requires characterization of the defenses used by microbes against RNI, analogous to our understanding of defenses against reactive oxygen intermediates. Genetic and biochemical approaches have identified candidates for RNI-resistance genes in Mycobacterium tuberculosis and other pathogens.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1118,
          "supporting": 17,
          "contradicting": 0,
          "mentioning": 1077,
          "unclassified": 24,
          "citingPublications": 1358
        }
      },
      {
        "title": "Cysteine-Mediated Redox Signaling: Chemistry, Biology, and Tools for Discovery",
        "doi": "10.1021/cr300163e",
        "year": 2013,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1048,
          "supporting": 6,
          "contradicting": 0,
          "mentioning": 1024,
          "unclassified": 18,
          "citingPublications": 1128
        }
      },
      {
        "title": "Supply chain risk management: a literature review",
        "doi": "10.1080/00207543.2015.1030467",
        "year": 2015,
        "journal": "International Journal of Production Research",
        "abstract": "Risk management plays a vital role in effectively operating supply chains in the presence of a variety of uncertainties. Over the years, many researchers have focused on supply chain risk management (SCRM) by contributing in the areas of defining, operationalizing, and mitigating risks. In this paper, we review and synthesize the extant literature in SCRM in the past decade in a comprehensive manner. The purpose of this paper is threefold. First, we present and categorize SCRM research appearing between 2003 and 2013. Second, we undertake a detailed review associated with research developments in supply chain risk definitions, risk types, risk factors, and risk management/mitigation strategies. Third, we analyze the SCRM literature in exploring potential gaps.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1042,
          "supporting": 12,
          "contradicting": 0,
          "mentioning": 959,
          "unclassified": 71,
          "citingPublications": 1178
        }
      },
      {
        "title": "Therapeutic siRNA: state of the art",
        "doi": "10.1038/s41392-020-0207-x",
        "year": 2020,
        "journal": "Signal Transduction and Targeted Therapy",
        "abstract": "RNA interference (RNAi) is an ancient biological mechanism used to defend against external invasion. It theoretically can silence any disease-related genes in a sequence-specific manner, making small interfering RNA (siRNA) a promising therapeutic modality. After a two-decade journey from its discovery, two approvals of siRNA therapeutics, ONPATTRO ® (patisiran) and GIVLAARI™ (givosiran), have been achieved by Alnylam Pharmaceuticals. Reviewing the long-term pharmaceutical history of human beings, siRNA therapy currently has set up an extraordinary milestone, as it has already changed and will continue to change the treatment and management of human diseases. It can be administered quarterly, even twice-yearly, to achieve therapeutic effects, which is not the case for small molecules and antibodies. The drug development process was extremely hard, aiming to surmount complex obstacles, such as how to efficiently and safely deliver siRNAs to desired tissues and cells and how to enhance the performance of siRNAs with respect to their activity, stability, specificity and potential off-target effects. In this review, the evolution of siRNA chemical modifications and their biomedical performance are comprehensively reviewed. All clinically explored and commercialized siRNA delivery platforms, including the GalNAc (N-acetylgalactosamine)-siRNA conjugate, and their fundamental design principles are thoroughly discussed. The latest progress in siRNA therapeutic development is also summarized. This review provides a comprehensive view and roadmap for general readers working in the field.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1040,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 1029,
          "unclassified": 11,
          "citingPublications": 1192
        }
      }
    ]
  },
  "Entropic Weights": {
    "query": "Entropic Weights",
    "count": 10,
    "papers": [
      {
        "title": "The Amber biomolecular simulation programs",
        "doi": "10.1002/jcc.20290",
        "year": 2005,
        "journal": "Journal of Computational Chemistry",
        "abstract": "AbstractWe describe the development, current features, and some directions for future development of the Amber package of computer programs. This package evolved from a program that was constructed in the late 1970s to do Assisted Model Building with Energy Refinement, and now contains a group of programs embodying a number of powerful tools of modern computational chemistry, focused on molecular dynamics and free energy calculations of proteins, nucleic acids, and carbohydrates.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 10638,
          "supporting": 33,
          "contradicting": 1,
          "mentioning": 10553,
          "unclassified": 51,
          "citingPublications": 10494
        }
      },
      {
        "title": "ff14SB: Improving the Accuracy of Protein Side Chain and Backbone Parameters from ff99SB",
        "doi": "10.1021/acs.jctc.5b00255",
        "year": 2015,
        "journal": "Journal of Chemical Theory and Computation",
        "abstract": "Molecular mechanics is powerful for its speed in atomistic simulations, but an accurate force field is required. The Amber ff99SB force field improved protein secondary structure balance and dynamics from earlier force fields like ff99, but weaknesses in side chain rotamer and backbone secondary structure preferences have been identified. Here, we performed a complete refit of all amino acid side chain dihedral parameters, which had been carried over from ff94. The training set of conformations included multidimensional dihedral scans designed to improve transferability of the parameters. Improvement in all amino acids was obtained as compared to ff99SB. Parameters were also generated for alternate protonation states of ionizable side chains. Average errors in relative energies of pairs of conformations were under 1.0 kcal/mol as compared to QM, reduced 35% from ff99SB. We also took the opportunity to make empirical adjustments to the protein backbone dihedral parameters as compared to ff99SB. Multiple small adjustments of φ and ψ parameters were tested against NMR scalar coupling data and secondary structure content for short peptides. The best results were obtained from a physically motivated adjustment to the φ rotational profile that compensates for lack of ff99SB QM training data in the β-ppII transition region. Together, these backbone and side chain modifications (hereafter called ff14SB) not only better reproduced their benchmarks, but improved secondary structure content in small peptides, and reproduction of NMR χ1 scalar coupling measurements for proteins in solution. We also discuss the Amber ff12SB parameter set, a preliminary version of ff14SB that includes most of its improvements.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 8572,
          "supporting": 32,
          "contradicting": 0,
          "mentioning": 8529,
          "unclassified": 11,
          "citingPublications": 9943
        }
      },
      {
        "title": "Automated docking using a Lamarckian genetic algorithm and an empirical binding free energy function",
        "doi": "10.1002/(sici)1096-987x(19981115)19:14<1639::aid-jcc10>3.0.co;2-b",
        "year": 1998,
        "journal": "Journal of Computational Chemistry",
        "abstract": "A novel and robust automated docking method that predicts the bound conformations of flexible ligands to macromolecular targets has been developed and tested, in combination with a new scoring function that estimates the free energy change upon binding. Interestingly, this method applies a Lamarckian model of genetics, in which environmental adaptations of an individual's phenotype are reverse transcribed into its genotype and become Ž . heritable traits sic . We consider three search methods, Monte Carlo simulated annealing, a traditional genetic algorithm, and the Lamarckian genetic algorithm, and compare their performance in dockings of seven protein᎐ligand test systems having known three-dimensional structure. We show that both the traditional and Lamarckian genetic algorithms can handle ligands with more degrees of freedom than the simulated annealing method used in earlier versions of AUTODOCK, and that the Lamarckian genetic algorithm is the most efficient, reliable, and successful of the three. The empirical free energy function was calibrated using a set of 30 structurally known protein᎐ligand complexes with experimentally determined binding constants. Linear regression analysis of the observed binding constants in terms of a wide variety of structure-derived molecular properties was performed. The final model had a residual standard y1 Ž y1 . error of 9.11 kJ mol 2.177 kcal mol and was chosen as the new energy",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 7888,
          "supporting": 43,
          "contradicting": 1,
          "mentioning": 7776,
          "unclassified": 68,
          "citingPublications": 10570
        }
      },
      {
        "title": "Titanium Dioxide Nanomaterials:  Synthesis, Properties, Modifications, and Applications",
        "doi": "10.1021/cr0500535",
        "year": 2007,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 6699,
          "supporting": 80,
          "contradicting": 2,
          "mentioning": 6515,
          "unclassified": 102,
          "citingPublications": 10198
        }
      },
      {
        "title": "ViennaRNA Package 2.0",
        "doi": "10.1186/1748-7188-6-26",
        "year": 2011,
        "journal": "Algorithms for Molecular Biology",
        "abstract": "BackgroundSecondary structure forms an important intermediate level of description of nucleic acids that encapsulates the dominating part of the folding energy, is often well conserved in evolution, and is routinely used as a basis to explain experimental findings. Based on carefully measured thermodynamic parameters, exact dynamic programming algorithms can be used to compute ground states, base pairing probabilities, as well as thermodynamic properties.ResultsThe  Package has been a widely used compilation of RNA secondary structure related computer programs for nearly two decades. Major changes in the structure of the standard energy model, the Turner 2004 parameters, the pervasive use of multi-core CPUs, and an increasing number of algorithmic variants prompted a major technical overhaul of both the underlying  and the interactive user programs. New features include an expanded repertoire of tools to assess RNA-RNA interactions and restricted ensembles of structures, additional output information such as centroid structures and maximum expected accuracy structures derived from base pairing probabilities, or z-scores for locally stable secondary structures, and support for input in  format. Updates were implemented without compromising the computational efficiency of the core algorithms and ensuring compatibility with earlier versions.ConclusionsThe , supporting concurrent computations , can be downloaded from http://www.tbi.univie.ac.at/RNA.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4913,
          "supporting": 21,
          "contradicting": 0,
          "mentioning": 4885,
          "unclassified": 7,
          "citingPublications": 4694
        }
      },
      {
        "title": "Cellulose Nanocrystals: Chemistry, Self-Assembly, and Applications",
        "doi": "10.1021/cr900339w",
        "year": 2010,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4342,
          "supporting": 63,
          "contradicting": 1,
          "mentioning": 4191,
          "unclassified": 87,
          "citingPublications": 5421
        }
      },
      {
        "title": "Carbon Dioxide Capture in Metal–Organic Frameworks",
        "doi": "10.1021/cr2003272",
        "year": 2011,
        "journal": "Chemical Reviews",
        "abstract": "Introduction 724 1.1. Carbon Dioxide Emission from Anthropogenic Sources 725 1.2. CO 2 Capture at Stationary Point Sources 726 1.3. Options for CO 2 Sequestration 727 1.4. Current CO 2 Capture Materials 727 1.4.1. Aqueous Alkanolamine Absorbents 728 1.4.2. Solid Porous Adsorbent Materials 729 1.5. MetalÀOrganic Frameworks 731 1.5.1. Synthesis and Structural Features 731 1.5.2. Physical Properties 732 2. CO 2 Adsorption in MetalÀOrganic Frameworks 733 2.1. Capacity for CO 2 733 2.2. Enthalpy of Adsorption 733 2.3. Selectivity for CO 2 739 2.3.1. Estimation from Single-Component Isotherms 741 2.3.2. Ideal Adsorbed Solution Theory (IAST) 742 2.3.3. Gas Mixtures and Breakthrough Experiments 742 2.4. In Situ Characterization of Adsorbed CO 2 742 2.4.1. Structural Observations 743 2.4.2. Infrared Spectroscopy 744 2.5. Computational Modeling of CO 2 Capture 745 3. Post-combustion Capture 746 3.1. MetalÀOrganic Frameworks for CO 2 /N 2 Separation 746 3.2. Enhancing CO 2 /N 2 Selectivity via Surface Functionalization 746 3.2.1. Pores Functionalized by Nitrogen Bases 746 3.2.2. Other Strongly Polarizing Organic Functional Groups 749 3.2.3. Exposed Metal Cation Sites 750 3.3. Considerations for Application 752 3.3.1. Stability to Water Vapor 752 3.3.2. Other Minor Components of Flue Gas 754 4. Pre-combustion Capture 754 4.1. Considerations for Pre-combustion CO 2 Capture 755 4.1.1. Advantages of Pre-combustion Capture 755 4.1.2. Hydrogen Purification 755 4.1.3. Metrics for Evaluating Adsorbents 755 4.1.4. Non-CO 2 Impurities in CO 2 /H 2 Streams 756 4.1.5. MetalÀOrganic Framework-Containing Membranes for Pre-combustion CO 2 Capture 756 4.2. MetalÀOrganic Frameworks as Adsorbents 756 4.2.1. Investigations Based on Single-Component Isotherms 757 4.2.2. Computational Studies 757 5. Oxy-fuel Combustion 760 5.1.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4324,
          "supporting": 70,
          "contradicting": 3,
          "mentioning": 4227,
          "unclassified": 24,
          "citingPublications": 6117
        }
      },
      {
        "title": "Emerging applications of stimuli-responsive polymer materials",
        "doi": "10.1038/nmat2614",
        "year": 2010,
        "journal": "Nature Materials",
        "abstract": "Responsive polymer materials can adapt to surrounding environments, regulate transport of ions and molecules, change wettability and adhesion of different species on external stimuli, or convert chemical and biochemical signals into optical, electrical, thermal and mechanical signals, and vice versa. These materials are playing an increasingly important part in a diverse range of applications, such as drug delivery, diagnostics, tissue engineering and 'smart' optical systems, as well as biosensors, microelectromechanical systems, coatings and textiles. We review recent advances and challenges in the developments towards applications of stimuli-responsive polymeric materials that are self-assembled from nanostructured building blocks. We also provide a critical outline of emerging developments.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4166,
          "supporting": 13,
          "contradicting": 2,
          "mentioning": 4125,
          "unclassified": 26,
          "citingPublications": 5408
        }
      },
      {
        "title": "Prospects of Colloidal Nanocrystals for Electronic and Optoelectronic Applications",
        "doi": "10.1021/cr900137k",
        "year": 2009,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3953,
          "supporting": 28,
          "contradicting": 3,
          "mentioning": 3896,
          "unclassified": 26,
          "citingPublications": 3993
        }
      },
      {
        "title": "The Halogen Bond",
        "doi": "10.1021/acs.chemrev.5b00484",
        "year": 2016,
        "journal": "Chemical Reviews",
        "abstract": "The\nhalogen bond occurs when there is evidence of a net attractive\ninteraction between an electrophilic region associated with a halogen\natom in a molecular entity and a nucleophilic region in another, or\nthe same, molecular entity. In this fairly extensive review, after\na brief history of the interaction, we will provide the reader with\na snapshot of where the research on the halogen bond is now, and,\nperhaps, where it is going. The specific advantages brought up by\na design based on the use of the halogen bond will be demonstrated\nin quite different fields spanning from material sciences to biomolecular\nrecognition and drug design.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3589,
          "supporting": 88,
          "contradicting": 2,
          "mentioning": 3461,
          "unclassified": 38,
          "citingPublications": 3518
        }
      }
    ]
  },
  "Pearson Correlation": {
    "query": "Pearson Correlation",
    "count": 10,
    "papers": [
      {
        "title": "G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences",
        "doi": "10.3758/bf03193146",
        "year": 2007,
        "journal": "Behavior Research Methods",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 34746,
          "supporting": 149,
          "contradicting": 17,
          "mentioning": 34085,
          "unclassified": 495,
          "citingPublications": 54836
        }
      },
      {
        "title": "BEDTools: a flexible suite of utilities for comparing genomic features",
        "doi": "10.1093/bioinformatics/btq033",
        "year": 2010,
        "journal": "Bioinformatics",
        "abstract": "Motivation: Testing for <strong class=\"highlight\">correlations</strong> between different sets of genomic features is a fundamental task in genomics research. However, searching for overlaps between features with existing web-based methods is complicated by the massive datasets that are routinely produced with current sequencing technologies. Fast and flexible tools are therefore required to ask complex questions of these data in an efficient manner.Results: This article introduces a new software suite for the comparison, manipulation and annotation of genomic features in Browser Extensible Data (BED) and General Feature Format (GFF) format. BEDTools also supports the comparison of sequence alignments in BAM format to both BED and GFF features. The tools are extremely efficient and allow the user to compare large datasets (e.g. next-generation sequencing data) with both public and custom genome annotation tracks. BEDTools can be combined with one another as well as with standard UNIX commands, thus facilitating routine genomics tasks as well as pipelines that can quickly answer intricate questions of large genomic datasets.Availability and implementation: BEDTools was written in C++. Source code and a comprehensive user manual are freely available at http://code.google.com/p/bedtoolsContact: aaronquinlan@gmail.com; imh4y@virginia.eduSupplementary information: Supplementary data are available at Bioinformatics online.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23025,
          "supporting": 13,
          "contradicting": 2,
          "mentioning": 22991,
          "unclassified": 19,
          "citingPublications": 25525
        }
      },
      {
        "title": "A power primer.",
        "doi": "10.1037/0033-2909.112.1.155",
        "year": 1992,
        "journal": "Psychological Bulletin",
        "abstract": "One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided here. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for eight standard statistical tests: (a) the difference between independent means, (b) the significance of a product-moment <strong class=\"highlight\">correlation</strong>, (c) the difference between independent rs, (d) the sign test, (e) the difference between independent proportions, (f) chi-square tests for goodness of fit and contingency tables, (g) one-way analysis of variance, and (h) the significance of a multiple or multiple partial correlation.The preface to the first edition of my power handbook (Cohen, 1969) \nbegins:During my first dozen years of teaching and consulting on applied statistics with behavioral scientists, 1 became increasingly impressed with the importance of statistical power analysis, an importance which was increased an order of magnitude by its neglect in our textbooks and curricula. The case for its importance is easily made: What behavioral scientist would view with equanimity the question of the probability that his investigation would lead to statistically significant results, i.e., its power? (p. vii)",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 21231,
          "supporting": 414,
          "contradicting": 71,
          "mentioning": 19919,
          "unclassified": 827,
          "citingPublications": 37823
        }
      },
      {
        "title": "A Guideline of Selecting and Reporting Intraclass <strong class=\"highlight\">Correlation</strong> Coefficients for Reliability Research",
        "doi": "10.1016/j.jcm.2016.02.012",
        "year": 2016,
        "journal": "Journal of Chiropractic Medicine",
        "abstract": "AbstractObjective: Intraclass <strong class=\"highlight\">correlation</strong> coefficient (ICC) is a widely used reliability index in testretest, intrarater, and interrater reliability analyses. This article introduces the basic concept of ICC in the content of reliability analysis. Discussion for Researchers: There are 10 forms of ICCs. Because each form involves distinct assumptions in their calculation and will lead to different interpretations, researchers should explicitly specify the ICC form they used in their calculation. A thorough review of the research design is needed in selecting the appropriate form of ICC to evaluate reliability. The best practice of reporting ICC should include software information, \"model,\" \"type,\" and \"definition\" selections. Discussion for Readers: When coming across an article that includes ICC, readers should first check whether information about the ICC form has been reported and if an appropriate ICC form was used. Based on the 95% confident interval of the ICC estimate, values less than 0.5, between 0.5 and 0.75, between 0.75 and 0.9, and greater than 0.90 are indicative of poor, moderate, good, and excellent reliability, respectively. Conclusion: This article provides a practical guideline for clinical researchers to choose the correct form of ICC and suggests the best practice of reporting ICC parameters in scientific publications. This article also gives readers an appreciation for what to look for when coming across ICC while reading an article.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Has erratum",
            "date": "2017-12-1",
            "noticeDoi": "10.1016/j.jcm.2017.10.001",
            "doi": "10.1016/j.jcm.2016.02.012",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16865,
          "supporting": 288,
          "contradicting": 20,
          "mentioning": 16329,
          "unclassified": 228,
          "citingPublications": 21656
        }
      },
      {
        "title": "Maximum entropy modeling of species geographic distributions",
        "doi": "10.1016/j.ecolmodel.2005.03.026",
        "year": 2006,
        "journal": "Ecological Modelling",
        "abstract": "The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of the method, here we perform a continental-scale case study using two Neotropical mammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus. We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC curve (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16098,
          "supporting": 39,
          "contradicting": 2,
          "mentioning": 15346,
          "unclassified": 711,
          "citingPublications": 15801
        }
      },
      {
        "title": "Comparing the Areas under Two or More <strong class=\"highlight\">Correlated</strong> Receiver Operating Characteristic Curves: A Nonparametric Approach",
        "doi": "10.2307/2531595",
        "year": 1988,
        "journal": "Biometrics",
        "abstract": "Methods of evaluating and comparing the performance of diagnostic tests are of increasing importance as new tests are developed and marketed. When a test is based on an observed variable that lies on a continuous or graded scale, an assessment of the overall value of the test can be made through the use of a receiver operating characteristic (ROC) curve. The curve is constructed by varying the cutpoint used to determine which values of the observed variable will be considered abnormal and then plotting the resulting sensitivities against the corresponding false positive rates. When two or more empirical curves are constructed based on tests performed on the same individuals, statistical analysis on differences between curves must take into account the <strong class=\"highlight\">correlated</strong> nature of the data. This paper presents a nonparametric approach to the analysis of areas under <strong class=\"highlight\">correlated</strong> ROC curves, by using the theory on generalized U-statistics to generate an estimated covariance matrix.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 13881,
          "supporting": 66,
          "contradicting": 12,
          "mentioning": 13698,
          "unclassified": 105,
          "citingPublications": 19685
        }
      },
      {
        "title": "STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets",
        "doi": "10.1093/nar/gky1131",
        "year": 2018,
        "journal": "Nucleic Acids Research",
        "abstract": "AbstractProteins and their functional interactions form the backbone of the cellular machinery. Their connectivity network needs to be considered for the full understanding of biological phenomena, but the available information on protein–protein associations is incomplete and exhibits varying levels of annotation granularity and reliability. The STRING database aims to collect, score and integrate all publicly available sources of protein–protein interaction information, and to complement these with computational predictions. Its goal is to achieve a comprehensive and objective global network, including direct (physical) as well as indirect (functional) interactions. The latest version of STRING (11.0) more than doubles the number of organisms it covers, to 5090. The most important new feature is an option to upload entire, genome-wide datasets as input, allowing users to visualize subsets as interaction networks and to perform gene-set enrichment analysis on the entire input. For the enrichment analysis, STRING implements well-known classification systems such as Gene Ontology and KEGG, but also offers additional, new classification systems based on high-throughput text-mining as well as on a hierarchical clustering of the association network itself. The STRING resource is available online at https://string-db.org/.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 13336,
          "supporting": 61,
          "contradicting": 2,
          "mentioning": 13248,
          "unclassified": 25,
          "citingPublications": 15963
        }
      },
      {
        "title": "The Hallmarks of Aging",
        "doi": "10.1016/j.cell.2013.05.039",
        "year": 2013,
        "journal": "Cell",
        "abstract": "Aging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12934,
          "supporting": 178,
          "contradicting": 13,
          "mentioning": 12532,
          "unclassified": 211,
          "citingPublications": 13279
        }
      },
      {
        "title": "The Structure and Function of Complex Networks",
        "doi": "10.1137/s003614450342480",
        "year": 2003,
        "journal": "Siam Review",
        "abstract": "Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network <strong class=\"highlight\">correlations</strong>, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12904,
          "supporting": 148,
          "contradicting": 11,
          "mentioning": 12318,
          "unclassified": 427,
          "citingPublications": 17092
        }
      },
      {
        "title": "Simple Combinations of Lineage-Determining Transcription Factors Prime cis-Regulatory Elements Required for Macrophage and B Cell Identities",
        "doi": "10.1016/j.molcel.2010.05.004",
        "year": 2010,
        "journal": "Molecular Cell",
        "abstract": "Summary\nGenome-scale studies have revealed extensive, cell type-specific co-localization of transcription factors, but the mechanisms underlying this phenomenon remain poorly understood. Here we demonstrate in macrophages and B cells that collaborative interactions of the common factor PU.1 with small sets of macrophage- or B celllineage-determining transcription factors establish cell-specific binding sites that are associated with the majority of promoter-distal H3K4me1-marked genomic regions. PU.1 binding initiates nucleosome remodeling followed by H3K4 monomethylation at large numbers of genomic regions associated with both broadly and specifically expressed genes. These locations serve as beacons for additional factors, exemplified by liver X receptors, which drive both cell-specific gene expression and signal-dependent responses. Together with analyses of transcription factor binding and H3K4me1 patterns in other cell types, these studies suggest that simple combinations of lineage-determining transcription factors can specify the genomic sites ultimately responsible for both cell identity and cell type-specific responses to diverse signaling inputs.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12712,
          "supporting": 175,
          "contradicting": 4,
          "mentioning": 12520,
          "unclassified": 13,
          "citingPublications": 12556
        }
      }
    ]
  },
  "Granger Causality": {
    "query": "Granger Causality",
    "count": 10,
    "papers": [
      {
        "title": "Co-Integration and Error Correction: Representation, Estimation, and Testing",
        "doi": "10.2307/1913236",
        "year": 1987,
        "journal": "Econometrica",
        "abstract": "The relationship between co-integration and error correction models, first suggested in <strong class=\"highlight\">Granger</strong> (1981), is here extended and used to develop estimation procedures, tests, and empirical examples. If each element of a vector of time series x, first achieves stationarity after differencing, but a linear combination a'x, is already stationary, the time series x, are said to be co-integrated with co-integrating vector a. There may be several such co-integrating vectors so that a becomes a matrix. Interpreting a'x, = 0 as a long run equilibrium, co-integration implies that deviations from equilibrium are stationary, with finite variance, even though the series themselves are nonstationary and have infinite variance. The paper presents a representation theorem based on <strong class=\"highlight\">Granger</strong> (1983), which connects the moving average, autoregressive, and error correction representations for co-integrated systems. A vector autoregression in differenced variables is incompatible with these representations. Estimation of these models is discussed and a simple but asymptotically efficient two-step estimator is proposed. Testing for co-integration combines the problems of unit root tests and tests with parameters unidentified under the null. Seven statistics are formulated and analyzed. The critical values of these statistics are calculated based on a Monte Carlo simulation. Using these critical values, the power properties of the tests are examined and one test procedure is recommended for application. In a veries of examples it is found that consumption and income are co-integrated, wages and prices are not, short and long interest rates are, and nominal GNP is co-integrated with M2, but not Ml, M3, or aggregate liquid assets.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 14327,
          "supporting": 101,
          "contradicting": 7,
          "mentioning": 13430,
          "unclassified": 789,
          "citingPublications": 26082
        }
      },
      {
        "title": "Investigating <strong class=\"highlight\">Causal</strong> Relations by Econometric Models and Cross-spectral Methods",
        "doi": "10.2307/1912791",
        "year": 1969,
        "journal": "Econometrica",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 8963,
          "supporting": 47,
          "contradicting": 6,
          "mentioning": 8235,
          "unclassified": 675,
          "citingPublications": 20602
        }
      },
      {
        "title": "FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data",
        "doi": "10.1155/2011/156869",
        "year": 2011,
        "journal": "Computational Intelligence and Neuroscience",
        "abstract": "This paper describes FieldTrip, an open source software package that we developed for the analysis of MEG, EEG, and other electrophysiological data. The software is implemented as a MATLAB toolbox and includes a complete set of consistent and user-friendly high-level functions that allow experimental neuroscientists to analyze experimental data. It includes algorithms for simple and advanced analysis, such as time-frequency analysis using multitapers, source reconstruction using dipoles, distributed sources and beamformers, connectivity analysis, and nonparametric statistical permutation tests at the channel and source level. The implementation as toolbox allows the user to perform elaborate and structured analyses of large data sets using the MATLAB command line and batch scripting. Furthermore, users and developers can easily extend the functionality and implement new algorithms. The modular design facilitates the reuse in other software packages.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 8065,
          "supporting": 13,
          "contradicting": 2,
          "mentioning": 8035,
          "unclassified": 15,
          "citingPublications": 10212
        }
      },
      {
        "title": "Causation, Prediction, and Search",
        "doi": "10.7551/mitpress/1754.001.0001",
        "year": 2001,
        "journal": "",
        "abstract": "We obtained a new perspective on the enterprise from Judea Pearl's Probabilistic Reasoning in Intelligent Systems, which appeared the next year. Although not principally concerned with discovery, Pearl's book showed us how to connect conditional independence with <strong class=\"highlight\">causal</strong> structure quite generally, and that connection proved essential to establishing general, reliable discovery procedures. We have since profited from correspondence and conversation with Pearl and with Dan Geiger and Thomas Verma, and from several of their papers. Pearl's work drew on the papers of Wermuth (1980), Kiiveri and Speed (1982), Lauritzen (1983), andCarlin (1984), which in the early 1980s had already provided the foundations for a rigorous study of <strong class=\"highlight\">causal</strong> inference. Paul Holland introduced one of us to the Rubin framework some years ago, but we only recently realized it's logical connections with directed graphical models. We were further helped by J. Whittaker's (1990) excellent account of the properties of undirected graphical models.We have learned a great deal from Gregory Cooper at the University of Pittsburgh who provided us with data, comments, Bayesian algorithms and the picture and description of the ALARM network which we consider in several places. Over the years we have learned useful things from Kenneth Bollen. Chris Meek provided essential help in obtaining an important theorem that derives various claims made by Rubin, Pratt and Schlaifer from axioms on directed graphical models.Steve Fienberg and several students from Carnegie Mellon's department of statistics joined with us in a seminar on graphical models from which we learned a great deal. We are indebted to him for his openness, intelligence and helpfulness in our research, and to Elizabeth Slate for guiding us through several papers in the Rubin framework. We are obliged to Nancy Cartwright for her courteous but salient criticism of the approach taken in our previous book and continued here. Her comments prompted our work on parameters in Causation, Prediction, and Search Chapter 4. We are indebted to Brian Skyrms for his interest and encouragement over many years, and to Marek Druzdzel for helpful comments and encouragement. We have also been",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 5352,
          "supporting": 10,
          "contradicting": 1,
          "mentioning": 5291,
          "unclassified": 50,
          "citingPublications": 5525
        }
      },
      {
        "title": "Saliency, switching, attention and control: a network model of insula function",
        "doi": "10.1007/s00429-010-0262-0",
        "year": 2010,
        "journal": "Brain Structure and Function",
        "abstract": "The insula is a brain structure implicated in disparate cognitive, affective, and regulatory functions, including interoceptive awareness, emotional responses, and empathic processes. While classically considered a limbic region, recent evidence from network analysis suggests a critical role for the insula, particularly the anterior division, in high-level cognitive control and attentional processes. The crucial insight and view we present here is of the anterior insula as an integral hub in mediating dynamic interactions between other large-scale brain networks involved in externally oriented attention and internally oriented or self-related cognition. The model we present postulates that the insula is sensitive to salient events, and that its core function is to mark such events for additional processing and initiate appropriate control signals. The anterior insula and the anterior cingulate cortex form a “salience network” that functions to segregate the most relevant among internal and extrapersonal stimuli in order to guide behavior. Within the framework of our network model, the disparate functions ascribed to the insula can be conceptualized by a few basic mechanisms: (1) bottom–up detection of salient events, (2) switching between other large-scale networks to facilitate access to attention and working memory resources when a salient event is detected, (3) interaction of the anterior and posterior insula to modulate autonomic reactivity to salient stimuli, and (4) strong functional coupling with the anterior cingulate cortex that facilitates rapid access to the motor system. In this manner, with the insula as its integral hub, the salience network assists target brain regions in the generation of appropriate behavioral responses to salient stimuli. We suggest that this framework provides a parsimonious account of insula function in neurotypical adults, and may provide novel insights into the neural basis of disorders of affective and social cognition.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4766,
          "supporting": 300,
          "contradicting": 13,
          "mentioning": 4434,
          "unclassified": 19,
          "citingPublications": 5129
        }
      },
      {
        "title": "A critique of the cross-lagged panel model.",
        "doi": "10.1037/a0038889",
        "year": 2015,
        "journal": "Psychological Methods",
        "abstract": "The cross-lagged panel model (CLPM) is believed by many to overcome the problems associated with the use of cross-lagged correlations as a way to study <strong class=\"highlight\">causal</strong> influences in longitudinal panel data. The current article, however, shows that if stability of constructs is to some extent of a trait-like, time-invariant nature, the autoregressive relationships of the CLPM fail to adequately account for this. As a result, the lagged parameters that are obtained with the CLPM do not represent the actual within-person relationships over time, and this may lead to erroneous conclusions regarding the presence, predominance, and sign of <strong class=\"highlight\">causal</strong> influences. In this article we present an alternative model that separates the within-person process from stable between-person differences through the inclusion of random intercepts, and we discuss how this model is related to existing structural equation models that include cross-lagged relationships. We derive the analytical relationship between the cross-lagged parameters from the CLPM and the alternative model, and use simulations to demonstrate the spurious results that may arise when using the CLPM to analyze data that include stable, trait-like individual differences. We also present a modeling strategy to avoid this pitfall and illustrate this using an empirical data set. The implications for both existing and future cross-lagged panel research are discussed.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4166,
          "supporting": 40,
          "contradicting": 7,
          "mentioning": 4105,
          "unclassified": 14,
          "citingPublications": 3144
        }
      },
      {
        "title": "The Reorienting System of the Human Brain: From Environment to Theory of Mind",
        "doi": "10.1016/j.neuron.2008.04.017",
        "year": 2008,
        "journal": "Neuron",
        "abstract": "Survival can depend on the ability to change a current course of action to respond to potentially advantageous or threatening stimuli. This \"reorienting\" response involves the coordinated action of a right hemisphere dominant ventral frontoparietal network that interrupts and resets ongoing activity and a dorsal frontoparietal network specialized for selecting and linking stimuli and responses. At rest, each network is distinct and internally correlated, but when attention is focused, the ventral network is suppressed to prevent reorienting to distracting events. These different patterns of recruitment may reflect inputs to the ventral attention network from the locus coeruleus/norepinephrine system. While originally conceptualized as a system for redirecting attention from one object to another, recent evidence suggests a more general role in switching between networks, which may explain recent evidence of its involvement in functions such as social cognition.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3850,
          "supporting": 319,
          "contradicting": 25,
          "mentioning": 3486,
          "unclassified": 20,
          "citingPublications": 3722
        }
      },
      {
        "title": "Investigating <strong class=\"highlight\">Causal</strong> Relations by Econometric Models and Cross-Spectral Methods",
        "doi": "10.1017/cbo9780511753978.002",
        "year": null,
        "journal": "",
        "abstract": "INVESTIGATING <strong class=\"highlight\">CAUSAL</strong> RELATIONS BY ECONOMETRIC MODELS AND CROSS-SPECTRAL METHODS There occurs on some occasions a difficulty in deciding the direction of <strong class=\"highlight\">causality</strong> between two related variables and also whether or not feedback is occurring. Testable definitions of <strong class=\"highlight\">causality</strong> and feedback are proposed and illustrated by use of simple two-variable models. The important problem of apparent instantaneous <strong class=\"highlight\">causality</strong> is discussed and it is suggested that the problem often arises due to slowness in recording information or because a sufficiently wide class of possible <strong class=\"highlight\">causal</strong> variables has not been used. It can be shown that the cross spectrum between two variables can be decomposed into two parts, each relating to a single <strong class=\"highlight\">causal</strong> arm of a feedback situation. Measures of <strong class=\"highlight\">causal</strong> lag and <strong class=\"highlight\">causal</strong> strength can then be constructed. A generalisation of this result with the partial cross spectrum is suggested.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3801,
          "supporting": 9,
          "contradicting": 1,
          "mentioning": 3469,
          "unclassified": 322,
          "citingPublications": 4291
        }
      },
      {
        "title": "Financial Development and Economic Growth: Views and Agenda",
        "doi": "10.1596/1813-9450-1678",
        "year": 1999,
        "journal": "",
        "abstract": "Levine argues that the preponderance of theoreticalWithout minimizing the role of institutions, Levine reasoning and empirical evidence suggests a positive, advocates a functional approach to understanding the first-order relationship between financial development role of financial systems in economic growth. This and economic growth. There is even evidence that the approach focuses on the ties between growth and the level of financial development is a good predictor of quality of the functions provided by the financial system. future rates of economic growth, capital accumulation, Levine discourages a narrow focus on one financial and technological change.instrument, such as money, or a particular institution, Moreover, cross-country, case-study, industry-level, such as banks. Instead, he addresses the more and firm-level analyses document extensive periods when comprehensive, and difficult, question: What is the financial development (or the lack thereof) crucially relationship between financial structure and the affects the speed and pattern of economic development.functioning of the financial system? Levine explains what the financial system does and More research is needed on financial development. how it affects, and is affected by, economic growth.Why does financial structure change as countries grow, Theory suggests that financial instruments, markets, and for example? Why do countries at similar stages of institutions arise to mitigate the effects of information economic development have different looking financial and transaction costs. A growing literature shows that systems? Are there long-run economic growth advantages differences in how well financial systems reduce to adopting legal and policy changes that create one type information and transaction costs influence savings rates, of financial structure rather than another? investment decisions, technological innovation, and longrun growth rates.A less developed theoretical literature shows how changes in economic activity can influence financial systems.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3720,
          "supporting": 73,
          "contradicting": 5,
          "mentioning": 3263,
          "unclassified": 379,
          "citingPublications": 3240
        }
      },
      {
        "title": "Testing for <strong class=\"highlight\">Granger</strong> non-<strong class=\"highlight\">causality</strong> in heterogeneous panels",
        "doi": "10.1016/j.econmod.2012.02.014",
        "year": 2012,
        "journal": "Economic Modelling",
        "abstract": "AbstractThis paper proposes a very simple test of <strong class=\"highlight\">Granger</strong> (1969) non-<strong class=\"highlight\">causality</strong> for heterogeneous panel data models. Our test statistic is based on the individual Wald statistics of <strong class=\"highlight\">Granger</strong> non <strong class=\"highlight\">causality</strong> averaged across the cross-section units. First, this statistic is shown to converge sequentially to a standard normal distribution. Second, the semiasymptotic distribution of the average statistic is characterized for a fixed T sample. A standardized statistic based on an approximation of the moments of Wald statistics is hence proposed. Third, Monte Carlo experiments show that our standardized panel statistics have very good small sample properties, even in the presence of cross-sectional dependence.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3487,
          "supporting": 30,
          "contradicting": 1,
          "mentioning": 3260,
          "unclassified": 196,
          "citingPublications": 4798
        }
      }
    ]
  },
  "Kendall Tau": {
    "query": "Kendall Tau",
    "count": 10,
    "papers": [
      {
        "title": "Conservation of resources: A new attempt at conceptualizing stress.",
        "doi": "10.1037/0003-066x.44.3.513",
        "year": 1989,
        "journal": "American Psychologist",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 19054,
          "supporting": 1127,
          "contradicting": 36,
          "mentioning": 17352,
          "unclassified": 539,
          "citingPublications": 12777
        }
      },
      {
        "title": "Trim and Fill: A Simple Funnel‐Plot–Based Method of Testing and Adjusting for Publication Bias in Meta‐Analysis",
        "doi": "10.1111/j.0006-341x.2000.00455.x",
        "year": 2000,
        "journal": "Biometrics",
        "abstract": "We study recently developed nonparametric methods for estimating the number of missing studies that might exist in a meta-analysis and the effect that these studies might have had on its outcome. These are simple rank-based data augmentation techniques, which formalize the use of funnel plots. We show that they provide effective and relatively powerful tests for evaluating the existence of such publication bias. After adjusting for missing studies, we find that the point estimate of the overall effect size is approximately correct and coverage of the effect size confidence intervals is substantially improved, in many cases recovering the nominal confidence levels entirely. We illustrate the trim and fill method on existing meta-analyses of studies in clinical trials and psychometrics.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 8752,
          "supporting": 17,
          "contradicting": 3,
          "mentioning": 8697,
          "unclassified": 35,
          "citingPublications": 12325
        }
      },
      {
        "title": "Estimates of the Regression Coefficient Based on <strong class=\"highlight\">Kendall's</strong> <strong class=\"highlight\">Tau</strong>",
        "doi": "10.1080/01621459.1968.10480934",
        "year": 1968,
        "journal": "Journal of the American Statistical Association",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4949,
          "supporting": 22,
          "contradicting": 0,
          "mentioning": 4801,
          "unclassified": 126,
          "citingPublications": 11347
        }
      },
      {
        "title": "Global observed changes in daily climate extremes of temperature and precipitation",
        "doi": "10.1029/2005jd006290",
        "year": 2006,
        "journal": "Journal of Geophysical Research Atmospheres",
        "abstract": "[1] A suite of climate change indices derived from daily temperature and precipitation data, with a primary focus on extreme events, were computed and analyzed. By setting an exact formula for each index and using specially designed software, analyses done in different countries have been combined seamlessly. This has enabled the presentation of the most up-to-date and comprehensive global picture of trends in extreme temperature and precipitation indices using results from a number of workshops held in data-sparse regions and high-quality station data supplied by numerous scientists world wide. Seasonal and annual indices for the period 1951-2003 were gridded. Trends in the gridded fields were computed and tested for statistical significance. Results showed widespread significant changes in temperature extremes associated with warming, especially for those indices derived from daily minimum temperature. Over 70% of the global land area sampled showed a significant decrease in the annual occurrence of cold nights and a significant increase in the annual occurrence of warm nights. Some regions experienced a more than doubling of these indices. This implies a positive shift in the distribution of daily minimum temperature throughout the globe. Daily maximum temperature indices showed similar changes but with smaller magnitudes. Precipitation changes showed a widespread and significant increase, but the changes are much less spatially coherent compared with temperature change. Probability distributions of indices derived from approximately 200 temperature and 600 precipitation stations, with nearcomplete data for 1901-2003 and covering a very large region of the Northern Hemisphere midlatitudes (and parts of Australia for precipitation) were analyzed for the periods 1901-1950, 1951-1978 and 1979-2003. Results indicate a significant warming throughout the 20th century. Differences in temperature indices distributions are particularly pronounced between the most recent two periods and for those indices related to minimum temperature. An analysis of those indices for which seasonal time series are available shows that these changes occur for all seasons although they are generally least pronounced for September to November. Precipitation indices show a tendency toward wetter conditions throughout the 20th century.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3382,
          "supporting": 236,
          "contradicting": 16,
          "mentioning": 3019,
          "unclassified": 111,
          "citingPublications": 4138
        }
      },
      {
        "title": "Interrater Reliability of a Modified Ashworth Scale of Muscle Spasticity",
        "doi": "10.1093/ptj/67.2.206",
        "year": 1987,
        "journal": "Physical Therapy",
        "abstract": "We undertook this investigation to determine the interrater reliability of manual tests of elbow flexor muscle spasticity graded on a modified Ashworth scale. We each independently graded the elbow flexor muscle spasticity of 30 patients with intracranial lesions. We agreed on 86.7% of our ratings. The <strong class=\"highlight\">Kendall's</strong> <strong class=\"highlight\">tau</strong> correlation between our grades was .847 (p less than .001). Thus, the relationship between the raters' judgments was significant and the reliability was good. Although the results were limited to the elbow flexor muscle group, we believe them to be positive enough to encourage further trials of the modified Ashworth scale for grading spasticity.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3075,
          "supporting": 15,
          "contradicting": 5,
          "mentioning": 2952,
          "unclassified": 103,
          "citingPublications": 4915
        }
      },
      {
        "title": "Revised International Prognostic Scoring System for Myelodysplastic Syndromes",
        "doi": "10.1182/blood-2012-03-420489",
        "year": 2012,
        "journal": "Blood",
        "abstract": "AbstractThe International Prognostic Scoring Sytem (IPSS) is an important standard for ssessing prognosis of primary untreated adult patients with myelodysplastic syndromes (MDS). To refine the IPSS, MDS patient databases from international institutions were coalesced to assemble a much larger combined database (Revised-IPSS [IPSS-R], n = 7012, IPSS, n = 816) for analysis. Multiple statistically weighted clinical features were used to generate a prognostic categorization model. Bone marrow cytogenetics, marrow blast percentage, and cytopenias remained the basis of the new system. Novel components of the current analysis included: 5 rather than 3 cytogenetic prognostic subgroups with specific and new classifications of a number of less common cytogenetic subsets, splitting the low marrow blast percentage value, and depth of cytopenias. This model defined 5 rather than the 4 major prognostic categories that are present in the IPSS. Patient age, performance status, serum ferritin, and lactate dehydrogenase were significant additive features for survival but not for acute myeloid leukemia transformation. This system comprehensively integrated the numerous known clinical features into a method analyzing MDS patient prognosis more precisely than the initial IPSS. As such, this IPSS-R should prove beneficial for predicting the clinical outcomes of untreated MDS patients and aiding design and analysis of clinical trials in this disease.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2854,
          "supporting": 49,
          "contradicting": 17,
          "mentioning": 2715,
          "unclassified": 73,
          "citingPublications": 3016
        }
      },
      {
        "title": "Functional Neuroimaging of Anxiety: A Meta-Analysis of Emotional Processing in PTSD, Social Anxiety Disorder, and Specific Phobia",
        "doi": "10.1176/appi.ajp.2007.07030504",
        "year": 2007,
        "journal": "American Journal of Psychiatry",
        "abstract": "AbstractObjective-The study of human anxiety disorders has benefited greatly from functional neuroimaging approaches. Individual studies, however, vary greatly in their findings. The authors searched for common and disorder-specific functional neurobiological deficits in several anxiety disorders. The authors also compared these deficits to the neural systems engaged during anticipatory anxiety in healthy subjects.Method-Functional magnetic resonance imaging and positron emission tomography studies of posttraumatic stress disorder (PTSD), social anxiety disorder, specific phobia, and fear conditioning in healthy individuals were compared by quantitative meta-analysis. Included studies compared negative emotional processing to baseline, neutral, or positive emotion conditions.Results-Patients with any of the three disorders consistently showed greater activity than matched comparison subjects in the amygdala and insula, structures linked to negative emotional responses. A similar pattern was observed during fear conditioning in healthy subjects. Hyperactivation in the amygdala and insula were, of interest, more frequently observed in social anxiety disorder and specific phobia than in PTSD. By contrast, only patients with PTSD showed hypoactivation in the dorsal and rostral anterior cingulate cortices and the ventromedial prefrontal cortex-structures linked to the experience and regulation of emotion.Conclusions-This meta-analysis allowed us to synthesize often disparate findings from individual studies and thereby provide neuroimaging evidence for common brain mechanisms in anxiety disorders and normal fear. Effects unique to PTSD furthermore suggested a mechanism for the emotional dysregulation symptoms in PTSD that extend beyond an exaggerated fear response. Therefore, these findings help refine our understanding of anxiety disorders and their interrelationships.Fear and avoidance of trigger cues are common to many anxiety disorders (1) and resemble the arousal and avoidance responses shown by normal subjects to conditioned fear cues (2). Thus, a common element of anxiety disorders may be an abnormally elevated fear response. Based on animal models of fear learning (3, 4), this hypothesis leads to the prediction that amygdalar dysfunction is common to a variety of anxiety disorders. Indeed, amygdalar hyperactivity has been observed during symptom provocation or negative emotional processing in patients with posttraumatic stress disorder (PTSD) (5-8), social anxiety disorder (9-14), specific phobia (15-18), panic disorder (19), and obsessive-compulsive disorder (OCD) (19,20). However, because of the low statistical power of individual studies Address correspondence and reprint requests to Dr. Etkin, Department of Psychiatry and Behavioral Sciences, Stanford University School of Medicine, 401 Quarry Rd., Stanford, CA 94305; amitetkin@stanford.edu. All authors report no competing interests.\nNIH Public Access\nAuthor ManuscriptAm J Psychiatry. Author manuscript; available in PMC 2012 April 4. \nNIH-PA Author Manuscr...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2670,
          "supporting": 176,
          "contradicting": 18,
          "mentioning": 2452,
          "unclassified": 24,
          "citingPublications": 3148
        }
      },
      {
        "title": "What is Twitter, a social network or a news media?",
        "doi": "10.1145/1772690.1772751",
        "year": 2010,
        "journal": "",
        "abstract": "ABSTRACTTwitter, a microblogging service less than three years old, commands more than 41 million users as of July 2009 and is growing fast. Twitter users tweet about any topic within the 140-character limit and follow others to receive their tweets. The goal of this paper is to study the topological characteristics of Twitter and its power as a new medium of information sharing.We have crawled the entire Twitter site and obtained 41.7 million user profiles, 1.47 billion social relations, 4, 262 trending topics, and 106 million tweets. In its follower-following topology analysis we have found a non-power-law follower distribution, a short effective diameter, and low reciprocity, which all mark a deviation from known characteristics of human social networks [28]. In order to identify influentials on Twitter, we have ranked users by the number of followers and by PageRank and found two rankings to be similar. Ranking by retweets differs from the previous two rankings, indicating a gap in influence inferred from the number of followers and that from the popularity of one's tweets. We have analyzed the tweets of top trending topics and reported on their temporal behavior and user participation. We have classified the trending topics based on the active period and the tweets and show that the majority (over 85%) of topics are headline news or persistent news in nature. A closer look at retweets reveals that any retweeted tweet is to reach an average of 1, 000 users no matter what the number of followers is of the original tweet. Once retweeted, a tweet gets retweeted almost instantly on next hops, signifying fast diffusion of information after the 1st retweet.To the best of our knowledge this work is the first quantitative study on the entire Twittersphere and information diffusion on it.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2584,
          "supporting": 91,
          "contradicting": 7,
          "mentioning": 2399,
          "unclassified": 87,
          "citingPublications": 5865
        }
      },
      {
        "title": "Prevention of relapse/recurrence in major depression by mindfulness-based cognitive therapy.",
        "doi": "10.1037/0022-006x.68.4.615",
        "year": 2000,
        "journal": "Journal of Consulting and Clinical Psychology",
        "abstract": "This study evaluated mindfulness-based cognitive therapy (MBCT), a group intervention designed to train recovered recurrently depressed patients to disengage from dysphoria-activated depressogenic thinking that may mediate relapse/recurrence. Recovered recurrently depressed patients (n = 145) were randomized to continue with treatment as usual or, in addition, to receive MBCT. Relapse/recurrence to major depression was assessed over a 60-week study period. For patients with 3 or more previous episodes of depression (77% of the sample), MBCT significantly reduced risk of relapse/recurrence. For patients with only 2 previous episodes, MBCT did not reduce relapse/recurrence. MBCT offers a promising cost-efficient psychological approach to preventing relapse/recurrence in recovered recurrently depressed patients.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2156,
          "supporting": 75,
          "contradicting": 19,
          "mentioning": 1965,
          "unclassified": 97,
          "citingPublications": 2611
        }
      },
      {
        "title": "The effect of mindfulness-based therapy on anxiety and depression: A meta-analytic review.",
        "doi": "10.1037/a0018555",
        "year": 2010,
        "journal": "Journal of Consulting and Clinical Psychology",
        "abstract": "AbstractBACKGROUND-Although mindfulness-based therapy has become a popular treatment, little is known about its efficacy.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2156,
          "supporting": 113,
          "contradicting": 10,
          "mentioning": 1946,
          "unclassified": 87,
          "citingPublications": 3410
        }
      }
    ]
  },
  "Divided Differences": {
    "query": "Divided Differences",
    "count": 10,
    "papers": [
      {
        "title": "Deep Residual Learning for Image Recognition",
        "doi": "10.1109/cvpr.2016.90",
        "year": 2016,
        "journal": "",
        "abstract": "AbstractDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 120349,
          "supporting": 309,
          "contradicting": 22,
          "mentioning": 119363,
          "unclassified": 655,
          "citingPublications": 206992
        }
      },
      {
        "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2",
        "doi": "10.1186/s13059-014-0550-8",
        "year": 2014,
        "journal": "Genome Biology",
        "abstract": "In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html.Electronic supplementary materialThe online version of this article (doi:10.1186/s13059-014-0550-8) contains supplementary material, which is available to authorized users.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 63870,
          "supporting": 97,
          "contradicting": 8,
          "mentioning": 63699,
          "unclassified": 66,
          "citingPublications": 80597
        }
      },
      {
        "title": "MUSCLE: multiple sequence alignment with high accuracy and high throughput",
        "doi": "10.1093/nar/gkh340",
        "year": 2004,
        "journal": "Nucleic Acids Research",
        "abstract": "We describe MUSCLE, a new computer program for creating multiple alignments of protein sequences. Elements of the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy of MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets of reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each of these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest of the tested methods for large numbers of sequences, aligning 5000 sequences of average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5. com/muscle.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 32338,
          "supporting": 41,
          "contradicting": 1,
          "mentioning": 32123,
          "unclassified": 173,
          "citingPublications": 42368
        }
      },
      {
        "title": "Bias in meta-analysis detected by a simple, graphical test",
        "doi": "10.1136/bmj.315.7109.629",
        "year": 1997,
        "journal": "BMJ",
        "abstract": "AbstractObjective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38%) journal meta-analyses and 5 (13%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 30448,
          "supporting": 69,
          "contradicting": 13,
          "mentioning": 30201,
          "unclassified": 165,
          "citingPublications": 49041
        }
      },
      {
        "title": "When to use and how to report the results of PLS-SEM",
        "doi": "10.1108/ebr-11-2018-0203",
        "year": 2019,
        "journal": "European Business Review",
        "abstract": "Purpose\nThe purpose of this paper is to provide a comprehensive, yet concise, overview of the considerations and metrics required for partial least squares structural equation modeling (PLS-SEM) analysis and result reporting. Preliminary considerations are summarized first, including reasons for choosing PLS-SEM, recommended sample size in selected contexts, distributional assumptions, use of secondary data, statistical power and the need for goodness-of-fit testing. Next, the metrics as well as the rules of thumb that should be applied to assess the PLS-SEM results are covered. Besides presenting established PLS-SEM evaluation criteria, the overview includes the following new guidelines: PLSpredict (i.e., a novel approach for assessing a model’s out-of-sample prediction), metrics for model comparisons, and several complementary methods for checking the results’ robustness.\n\n\nDesign/methodology/approach\nThis paper provides an overview of previously and recently proposed metrics as well as rules of thumb for evaluating the research results based on the application of PLS-SEM.\n\n\nFindings\nMost of the previously applied metrics for evaluating PLS-SEM results are still relevant. Nevertheless, scholars need to be knowledgeable about recently proposed metrics (e.g. model comparison criteria) and methods (e.g. endogeneity assessment, latent class analysis and PLSpredict), and when and how to apply them to extend their analyses.\n\n\nResearch limitations/implications\nMethodological developments associated with PLS-SEM are rapidly emerging. The metrics reported in this paper are useful for current applications, but must always be up to date with the latest developments in the PLS-SEM method.\n\n\nOriginality/value\nIn light of more recent research and methodological developments in the PLS-SEM domain, guidelines for the method’s use need to be continuously extended and updated. This paper is the most current and comprehensive summary of the PLS-SEM method and the metrics applied to assess its solutions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23047,
          "supporting": 563,
          "contradicting": 27,
          "mentioning": 21451,
          "unclassified": 1006,
          "citingPublications": 17285
        }
      },
      {
        "title": "Densely Connected Convolutional Networks",
        "doi": "10.1109/cvpr.2017.243",
        "year": 2017,
        "journal": "",
        "abstract": "AbstractRecent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network hasdirect connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 22724,
          "supporting": 69,
          "contradicting": 6,
          "mentioning": 22499,
          "unclassified": 150,
          "citingPublications": 42146
        }
      },
      {
        "title": "ImageNet Large Scale Visual Recognition Challenge",
        "doi": "10.1007/s11263-015-0816-y",
        "year": 2015,
        "journal": "International Journal of Computer Vision",
        "abstract": "Abstract The ImageNet Large Scale Visual RecognitionChallenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-ofthe-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 22215,
          "supporting": 63,
          "contradicting": 2,
          "mentioning": 21960,
          "unclassified": 190,
          "citingPublications": 38692
        }
      },
      {
        "title": "Common risk factors in the returns on stocks and bonds",
        "doi": "10.1016/0304-405x(93)90023-5",
        "year": 1993,
        "journal": "Journal of Financial Economics",
        "abstract": "This paper identities five common risk factors in the returns on stocks and bonds. There are three stock-market factors: an overall market factor and factors related to firm size and book-to-market equity. There are two bond-market factors. related to maturity and default risks. Stock returns have shared variation due to the stock-market factors, and they are linked to bond returns through shared variation in the bond-market factors. Except for low-grade corporates. the bond-market factors capture the common variation in bond returns. Most important. the five factors seem to explain average returns on stocks and bonds.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 21343,
          "supporting": 613,
          "contradicting": 88,
          "mentioning": 20232,
          "unclassified": 410,
          "citingPublications": 24694
        }
      },
      {
        "title": "Coefficient Alpha and the Internal Structure of Tests",
        "doi": "10.1007/bf02310555",
        "year": 1951,
        "journal": "Psychometrika",
        "abstract": "A general formula (a) of which a special case is the KuderRichardson coefficient of equivalence is shown to be the mean of all split-half coefficients resulting from <strong class=\"highlight\">different</strong> splittings of a test. is therefore an estimate of the correlation between two random samples of items from a universe of items like those in the test. ~ is found to be an appropriate index of equivalence and, except for very short tests, of the first-factor concentration in the test. Tests divisible into distinct subtests should be so <strong class=\"highlight\">divided</strong> before using the formula. The index ~j, derived from a, is shown to be an index of inter-item homogeneity. Comparison is made to the Guttmau and Loevinger approaches. Parallel split coefficients are shown to be unnecessary for tests of common types. In designing tests, maximum interpretability of scores is obtained by increasing the firat-facter concentration in any separately-scored subtest and avoiding substantial group-factor clusters within a subtest. Scalability is not a requisite.\nI. Historical Resum~Any research based on measurement must be concerned with the accuracy or dependability or, as we usually call it, reliability of measurement. A reliability coefficient demonstrates whether the test designer was correct in expecting a certain collection of items to yield interpretable statements about individual <strong class=\"highlight\">differences</strong> (25).Even those investigators who regard reliability as a pale shadow of the more vital matter of validity cannot avoid considering the reliability of their measures. No validity coefficient and no factor analysis can be interpreted without some appropriate estimate of the magnitude of the error of measurement. The preferred way to find out how accurate one's measures are is to make two independent measurements and compare them. In practice, psychologists and educators have often not had the opportunity to recapture their subjects for a second test. Clinical tests, or those used for vocational guidance, are generally worked into a crowded schedule, and there is always a de-*The assistance of Dora Damrin and Willard Warrington is gratefully acknowledged. Miss Damrin took major responsibility for the empirical studies reported. This research was supported by the Bureau of Research and Service, College of Education. 297 298 PSYCHOMETRIKA sire to give additional tests if any extra time becomes available. Purely scientific investigations fare little better. It is hard enough to schedule twenty tests for a factorial study, let alone scheduling another twenty just to determine reliability.This difficulty was first circumvented by the invention of the splithail approach, whereby the test is rescored, half the items at a time, to get two estimates. The Spearman-Brown formula is then applied to get a coefficient similar to the correlation between two forms. The split-half Spearman-Brown procedure has been a standard method of test analysis for forty years. Alternative formulas have been developed, some of which have advantages over the original. In the course of our development, we shall re...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 21166,
          "supporting": 259,
          "contradicting": 17,
          "mentioning": 19149,
          "unclassified": 1741,
          "citingPublications": 37389
        }
      },
      {
        "title": "Quantifying heterogeneity in a meta‐analysis",
        "doi": "10.1002/sim.1186",
        "year": 2002,
        "journal": "Statistics in Medicine",
        "abstract": "The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic <strong class=\"highlight\">divided</strong> by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing <strong class=\"highlight\">different</strong> amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 20514,
          "supporting": 41,
          "contradicting": 6,
          "mentioning": 20359,
          "unclassified": 108,
          "citingPublications": 32366
        }
      }
    ]
  },
  "ARIMA": {
    "query": "ARIMA",
    "count": 10,
    "papers": [
      {
        "title": "A Pacific Interdecadal Climate Oscillation with Impacts on Salmon Production",
        "doi": "10.1175/1520-0477(1997)078<1069:apicow>2.0.co;2",
        "year": 1997,
        "journal": "Bulletin of the American Meteorological Society",
        "abstract": "Evidence gleaned from the instrumental record of climate data identifies a robust, recurring pattern of ocean-atmosphere climate variability centered over the midlatitude North Pacific basin. Over the past century, the amplitude of this climate pattern has varied irregularly at interannual-to-interdecadal timescales. There is evidence of reversals in the prevailing polarity of the oscillation occurring around 1925, 1947, and 1977; the last two reversals correspond to dramatic shifts in salmon production regimes in the North Pacific Ocean. This climate pattern also affects coastal sea and continental surface air temperatures, as well as streamflow in major west coast river systems, from Alaska to California.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 6134,
          "supporting": 262,
          "contradicting": 19,
          "mentioning": 5725,
          "unclassified": 128,
          "citingPublications": 6702
        }
      },
      {
        "title": "A New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle",
        "doi": "10.2307/1912559",
        "year": 1989,
        "journal": "Econometrica",
        "abstract": "This paper proposes a very tractable approach to modeling changes in regime. The parameters of an autoregression are viewed as the outcome of a discrete-state Markov process. For example, the mean growth rate of a nonstationary series may be subject to occasional, discrete shifts. The econometrician is presumed not to observe these shifts directly, but instead must draw probabilistic inference about whether and when they may have occurred based on the observed behavior of the series. The paper presents an algorithm for drawing such probabilistic inference in the form of a nonlinear iterative filter. The filter also permits estimation of population parameters by the method of maximum likelihood and provides the foundation for forecasting future values of the series. An empirical application of this technique to postwar U.S. real GNP suggests that the periodic shift from a positive growth rate to a negative growth rate is a recurrent feature of the U.S. business cycle, and indeed could be used as an objective criterion for defining and measuring economic recessions. The estimated parameter values suggest that a typical economic recession is associated with a 3% permanent drop in the level of GNP.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 6102,
          "supporting": 56,
          "contradicting": 9,
          "mentioning": 5692,
          "unclassified": 345,
          "citingPublications": 8168
        }
      },
      {
        "title": "The effects of feedback interventions on performance: A historical review, a meta-analysis, and a preliminary feedback intervention theory.",
        "doi": "10.1037/0033-2909.119.2.254",
        "year": 1996,
        "journal": "Psychological Bulletin",
        "abstract": "Since the beginning of the century, feedback interventions (FIs) produced negative-but largely ignored-effects on performance. A meta-analysis (607 effect sizes; 23,663 observations) suggests that FIs improved performance on average (d = .41) but that over '/3 of the FIs decreased performance. This finding cannot be explained by sampling error, feedback sign, or existing theories. The authors proposed a preliminary FI theory (FIT) and tested it with moderator analyses. The central assumption of FIT is that FIs change the locus of attention among 3 general and hierarchically organized levels of control: task learning, task motivation, and meta-tasks (including self-related) processes. The results suggest that FI effectiveness decreases as attention moves up the hierarchy closer to the self and away from the task. These findings are further moderated by task characteristics that are still poorly understood.To relate feedback directly to behavior is very confusing. Results are contradictory and seldom straight-forward.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4614,
          "supporting": 154,
          "contradicting": 13,
          "mentioning": 4281,
          "unclassified": 166,
          "citingPublications": 4931
        }
      },
      {
        "title": "Quantum Theory of Angular Momentum",
        "doi": "10.1142/0270",
        "year": 1988,
        "journal": "",
        "abstract": "This book deals with one of the basic topics of quantum mechanics: the theory of angular momentum and irreducible tensors. Being rather versatile, the mathematical apparatus of this theory is widely used in atomic and molecular physics, in nuclear physics and elementary particle theory. It enables one to calculate atomic, molecular and nuclear structures, energies of ground and excited states, fine and hyperfine splittings, etc. The apparatus is also very handy for evaluating the probabilities of radiative transitions, cross sections of various processes such as elastic and nonelastic scattering, different decays and reactions (both chemical and nuclear) and for studying angular distributions and polarizations of particles.Today this apparatus is finding ever increasing use in solving practical problems relating to quantum chemistry, kinetics, plasma physics, quantum optics, radiophysics and astrophysics.The basic ideas of the theory of angular momentum were first put forward by M. Born, P. Dirac, W. Heisenberg and W. Pauli. However, the modern version of its mathematical apparatus was developed mainly in the works of E. Wigner, J. Racah, L. Biedenharn and others who applied group theoretical methods to problems in quantum mechanics. At present a number of good books on the theory of angular momentum have been already published. The general principles and results of the theory may be found in the books by M. Rose [31], A. Edmonds [16], U. Fano and G. Racah [18], A. P. Yutsis, I. B. Levinson and V. V. Vanagas [44], A. P. Yutsis and A. A. Bandzaitis [45], D. Brink and G. Satcher [9]. Nevertheless, many formulas and relationships essential for practical calculations have escaped these books and are either scattered in various editions, or included as appendices in papers discussing somewhat disparate topics, making them generally inaccessible. Even greater difficulties arise when one tries to use the results, as each author employs his own phase conventions, initial definitions and symbols.The authors of this book aimed at collecting and compiling ample material on the quantum theory of angular momentum within the framework of a single system of phases and definitions. This is why, in addition to the basic theoretical results, the book also includes a great number of formulas and relationships essential for practical applications.This edition is the translated version of our book published in the USSR in 1975. In the course of its preparation we have tried to comply with a number of suggestions from our readers. For instance, each chapter opens with a comprehensive listing of its contents to ease the search for information needed. We also included some new results relating to different aspects of angular momentum theory which have recently appeared in journals. Unfortunately the limited volume of the present book prevented us from covering all the aforementioned results. We offer sincere apologies to the authors whose results we failed to include.The monograph is a kind of handbook. Consequently the material is pres...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4592,
          "supporting": 30,
          "contradicting": 1,
          "mentioning": 4533,
          "unclassified": 28,
          "citingPublications": 4455
        }
      },
      {
        "title": "Magnetic control of ferroelectric polarization",
        "doi": "10.1038/nature02018",
        "year": 2003,
        "journal": "Nature",
        "abstract": "The magnetoelectric effect--the induction of magnetization by means of an electric field and induction of polarization by means of a magnetic field--was first presumed to exist by Pierre Curie, and subsequently attracted a great deal of interest in the 1960s and 1970s (refs 2-4). More recently, related studies on magnetic ferroelectrics have signalled a revival of interest in this phenomenon. From a technological point of view, the mutual control of electric and magnetic properties is an attractive possibility, but the number of candidate materials is limited and the effects are typically too small to be useful in applications. Here we report the discovery of ferroelectricity in a perovskite manganite, TbMnO3, where the effect of spin frustration causes sinusoidal antiferromagnetic ordering. The modulated magnetic structure is accompanied by a magnetoelastically induced lattice modulation, and with the emergence of a spontaneous polarization. In the magnetic ferroelectric TbMnO3, we found gigantic magnetoelectric and magnetocapacitance effects, which can be attributed to switching of the electric polarization induced by magnetic fields. Frustrated spin systems therefore provide a new area to search for magnetoelectric media.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3756,
          "supporting": 147,
          "contradicting": 19,
          "mentioning": 3566,
          "unclassified": 24,
          "citingPublications": 4667
        }
      },
      {
        "title": "Colossal magnetoresistant materials: the key role of phase separation",
        "doi": "10.1016/s0370-1573(00)00121-6",
        "year": 2001,
        "journal": "Physics Reports",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3234,
          "supporting": 128,
          "contradicting": 11,
          "mentioning": 3075,
          "unclassified": 20,
          "citingPublications": 3693
        }
      },
      {
        "title": "The Economic Costs of Conflict: A Case Study of the Basque Country",
        "doi": "10.1257/000282803321455188",
        "year": 2003,
        "journal": "American Economic Review",
        "abstract": "This article investigates the economic effects of conflict, using the terrorist conflict in the Basque Country as a case study. We find that, after the outbreak of terrorism in the late 1960's, per capita GDP in the Basque Country declined about 10 percentage points relative to a synthetic control region without terrorism. In addition, we use the 1998-1999 truce as a natural experiment. We find that stocks of firms with a significant part of their business in the Basque Country showed a positive relative performance when truce became credible, and a negative relative performance at the end of the cease-fire.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3216,
          "supporting": 33,
          "contradicting": 2,
          "mentioning": 3127,
          "unclassified": 54,
          "citingPublications": 4113
        }
      },
      {
        "title": "The Interacting Boson Model",
        "doi": "10.1017/cbo9780511895517",
        "year": 1987,
        "journal": "",
        "abstract": "The interacting boson model was introduced in 1974 as an attempt to describe collective properties of nuclei in a unified way. Since 1974, the model has been the subject of many investigations and it has been extended to cover most aspects of nuclear structure. This book gives an account of the properties of the interacting boson model. In particular, this book presents the mathematical techniques used to analyze the structure of the model. It also collects in a single, easily accessible reference all the formulas that have been developed throughout the years to account for collective properties of nuclei. Suitable for both theorists and experimentalists.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2598,
          "supporting": 59,
          "contradicting": 3,
          "mentioning": 2527,
          "unclassified": 9,
          "citingPublications": 2058
        }
      },
      {
        "title": "Automatic Time Series Forecasting: TheforecastPackage forR",
        "doi": "10.18637/jss.v027.i03",
        "year": 2008,
        "journal": "Journal of Statistical Software",
        "abstract": "AbstractAutomatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with <strong class=\"highlight\">ARIMA</strong> models. The algorithms are applicable to both seasonal and non-seasonal data, and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2476,
          "supporting": 1,
          "contradicting": 0,
          "mentioning": 2382,
          "unclassified": 93,
          "citingPublications": 3392
        }
      },
      {
        "title": "Time series forecasting using a hybrid <strong class=\"highlight\">ARIMA</strong> and neural network model",
        "doi": "10.1016/s0925-2312(01)00702-0",
        "year": 2003,
        "journal": "Neurocomputing",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2152,
          "supporting": 20,
          "contradicting": 0,
          "mentioning": 1943,
          "unclassified": 189,
          "citingPublications": 3797
        }
      }
    ]
  },
  "ADFuller": {
    "query": "ADFuller",
    "count": 10,
    "papers": [
      {
        "title": "Host lifestyle affects human microbiota on daily timescales",
        "doi": "10.1186/gb-2014-15-7-r89",
        "year": 2014,
        "journal": "Genome Biology",
        "abstract": "BackgroundDisturbance to human microbiota may underlie several pathologies. Yet, we lack a comprehensive understanding of how lifestyle affects the dynamics of human-associated microbial communities.ResultsHere, we link over 10,000 longitudinal measurements of human wellness and action to the daily gut and salivary microbiota dynamics of two individuals over the course of one year. These time series show overall microbial communities to be stable for months. However, rare events in each subjects’ life rapidly and broadly impacted microbiota dynamics. Travel from the developed to the developing world in one subject led to a nearly two-fold increase in the Bacteroidetes to Firmicutes ratio, which reversed upon return. Enteric infection in the other subject resulted in the permanent decline of most gut bacterial taxa, which were replaced by genetically similar species. Still, even during periods of overall community stability, the dynamics of select microbial taxa could be associated with specific host behaviors. Most prominently, changes in host fiber intake positively correlated with next-day abundance changes among 15% of gut microbiota members.ConclusionsOur findings suggest that although human-associated microbial communities are generally stable, they can be quickly and profoundly altered by common human actions and experiences.Electronic supplementary materialThe online version of this article (doi:10.1186/gb-2014-15-7-r89) contains supplementary material, which is available to authorized users.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Has correction",
            "date": "2016-5-31",
            "noticeDoi": "10.1186/s13059-016-0988-y",
            "doi": "10.1186/gb-2014-15-7-r89",
            "urls": null
          },
          {
            "status": "Has erratum",
            "date": "2016-5-31",
            "noticeDoi": "10.1186/s13059-016-0988-y",
            "doi": "10.1186/gb-2014-15-7-r89",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 976,
          "supporting": 52,
          "contradicting": 2,
          "mentioning": 919,
          "unclassified": 3,
          "citingPublications": 944
        }
      },
      {
        "title": "Ecological Stability Emerges at the Level of Strains in the Human Gut Microbiome",
        "doi": "10.1128/mbio.02502-22",
        "year": 2023,
        "journal": "Mbio",
        "abstract": "To date, there has been an intense focus on the ecological dynamics of the human gut microbiome at the species level. However, there is considerable genetic diversity within species at the strain level, and these intraspecific differences can have important phenotypic effects on the host, impacting the ability to digest certain foods and metabolize drugs.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 60,
          "supporting": 3,
          "contradicting": 0,
          "mentioning": 57,
          "unclassified": 0,
          "citingPublications": 46
        }
      },
      {
        "title": "Forecasting E-Commerce Products Prices by Combining an Autoregressive Integrated Moving Average (ARIMA) Model and Google Trends Data",
        "doi": "10.3390/fi11010005",
        "year": 2018,
        "journal": "Future Internet",
        "abstract": "E-commerce is becoming more and more the main instrument for selling goods to the mass market. This led to a growing interest in algorithms and techniques able to predict products future prices, since they allow us to define smart systems able to improve the quality of life by suggesting more affordable goods and services. The joint use of time series, reputation and sentiment analysis clearly represents one important approach to this research issue. In this paper we present Price Probe, a suite of software tools developed to perform forecasting on products’ prices. Its primary aim is to predict the future price trend of products generating a customized forecast through the exploitation of autoregressive integrated moving average (ARIMA) model. We experimented the effectiveness of the proposed approach on one of the biggest E-commerce infrastructure in the world: Amazon. We used specific APIs and dedicated crawlers to extract and collect information about products and their related prices over time and, moreover, we extracted information from social media and Google Trends that we used as exogenous features for the ARIMA model. We fine-estimated ARIMA’s parameters and tried the different combinations of the exogenous features and noticed through experimental analysis that the presence of Google Trends information significantly improved the predictions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 33,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 29,
          "unclassified": 4,
          "citingPublications": 60
        }
      },
      {
        "title": "Sentinel node approach to monitoring online COVID-19 misinformation",
        "doi": "10.1038/s41598-022-12450-8",
        "year": 2022,
        "journal": "Scientific Reports",
        "abstract": "AbstractUnderstanding how different online communities engage with COVID-19 misinformation is critical for public health response. For example, misinformation confined to a small, isolated community of users poses a different public health risk than misinformation being consumed by a large population spanning many diverse communities. Here we take a longitudinal approach that leverages tools from network science to study COVID-19 misinformation on Twitter. Our approach provides a means to examine the breadth of misinformation engagement using modest data needs and computational resources. We identify a subset of accounts from different Twitter communities discussing COVID-19, and follow these ‘sentinel nodes’ longitudinally from July 2020 to January 2021. We characterize sentinel nodes in terms of a linked domain preference score, and use a standardized similarity score to examine alignment of tweets within and between communities. We find that media preference is strongly correlated with the amount of misinformation propagated by sentinel nodes. Engagement with sensationalist misinformation topics is largely confined to a cluster of sentinel nodes that includes influential conspiracy theorist accounts. By contrast, misinformation relating to COVID-19 severity generated widespread engagement across multiple communities. Our findings indicate that misinformation downplaying COVID-19 severity is of particular concern for public health response. We conclude that the sentinel node approach can be an effective way to assess breadth and depth of online misinformation penetration.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 21,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 21,
          "unclassified": 0,
          "citingPublications": 4
        }
      },
      {
        "title": "Twitter conversations predict the daily confirmed COVID-19 cases",
        "doi": "10.1016/j.asoc.2022.109603",
        "year": 2022,
        "journal": "Applied Soft Computing",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 13,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 13,
          "unclassified": 0,
          "citingPublications": 24
        }
      },
      {
        "title": "Kernel-based joint independence tests for multivariate stationary and non-stationary time series",
        "doi": "10.1098/rsos.230857",
        "year": 2023,
        "journal": "Royal Society Open Science",
        "abstract": "Multivariate time-series data that capture the temporal evolution of interconnected systems are ubiquitous in diverse areas. Understanding the complex relationships and potential dependencies among co-observed variables is crucial for the accurate statistical modelling and analysis of such systems. Here, we introduce kernel-based statistical tests of joint independence in multivariate time series by extending the\n            d\n            -variable Hilbert–Schmidt independence criterion to encompass both stationary and non-stationary processes, thus allowing broader real-world applications. By leveraging resampling techniques tailored for both single- and multiple-realization time series, we show how the method robustly uncovers significant higher-order dependencies in synthetic examples, including frequency mixing data and logic gates, as well as real-world climate, neuroscience and socio-economic data. Our method adds to the mathematical toolbox for the analysis of multivariate time series and can aid in uncovering high-order interactions in data.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 2,
          "unclassified": 0,
          "citingPublications": 2
        }
      },
      {
        "title": "The Complex Nature of Magnetic Element Transport in the Quiet Sun: The Lévy-walk Character",
        "doi": "10.3847/1538-4357/ab1be2",
        "year": 2019,
        "journal": "The Astrophysical Journal",
        "abstract": "The study of the dynamic properties of small-scale magnetic fields in the solar photosphere (magnetic elements, MEs) provides a fundamental tool to investigate some still unknown aspects of turbulent convection, and gain information on the spatial and temporal scales of evolution of the magnetic field in the quiet Sun. We track the MEs in a set of magnetogram long-time series acquired by the Hinode mission, and take advantage of a method based on entropy (the diffusion entropy analysis, DEA) to detect their dynamic regime, under the assumption that MEs are passively transported by the photospheric plasma flow. DEA has been proven to perform better than other standard techniques, and for the first time it is successfully used to provide the scaling properties of the displacement of MEs in the quiet Sun. The main results of this work, which represents an extension of the analysis presented in previous literature, can be summarized as two points: (i) MEs in the quiet Sun undergo a common dynamic turbulent regime independent of the local environment; (ii) the displacement of MEs exhibits a complex transport dynamics that is consistent with a Lévy walk.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 2,
          "unclassified": 0,
          "citingPublications": 6
        }
      },
      {
        "title": "Benchmark of Holt-Winters and SARIMA Methods in Predicting Jakarta Climate",
        "doi": "10.20944/preprints202204.0295.v1",
        "year": 2022,
        "journal": "",
        "abstract": "As its capital, Jakarta plays a critical role in boosting Indonesia&amp;rsquo;s economic growth and setting the precedent for broader change outside of the city. One crucial avenue of inquiry to better understand, and prepare for, the future of a country so heavily impacted by disastrous weather events is understanding the effects of climate change through data. This study investigates meteorological data collected from 1996 to 2021 and compares the application of the SARIMA and the Holt-Winters methods to predict the future influence of climatic parameters on Jakarta&amp;rsquo;s weather. The performance of the SARIMA method is proven to provide better results than the Holt-Winter models and both methods showed the best performances when forecasting the humidity data. The results of the forecast are able to demonstrate the characteristic of the climate in Jakarta, with dry season ranging from May to October and wet season ranging from November to April.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 1,
          "unclassified": 0,
          "citingPublications": 3
        }
      },
      {
        "title": "MONITORING SYSTEM FOR LTE-A CELLULAR COMMUNICATION NETWORK ACCESSIBILITY INDICATORS",
        "doi": "10.36724/2072-8735-2021-15-3-4-16",
        "year": 2021,
        "journal": "T-Comm",
        "abstract": "In this paper we consider two accessibility indicators, namely E-RAB (E-UTRAN Radio Access Bearer) and E-RRC (Evolved Radio Resource Control) failure rates, of the LTE-A communication network belonging to one of the regional operators in Russian Federation. The aim of this study is to find the proper algorithms for accessibility indicators prediction, and performance estimation of these algorithms. During the study, we provide temporal dynamics of the indicators and possible failure reasons, behind these indicators. Then the percentage of the time series values is shown, which are corresponding to the abnormal situations (incidents). After that, the stationarity of the inspected time series using augmented Dickey-Fuller (<strong class=\"highlight\">ADFuller</strong>), and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) methods is analyzed. Next, the ETS decomposition is performed. In order to predict the future values of the indicators, we utilize SARIMA model, triple Exponential Smoothing (Holt-Winters method), Facebook Prophet, Prony decomposition based model and XGBoost algorithm. Performance estimation is obtained in two ways: by test-sequence- and cross-validation-based Median Absolute Error (MAE). Also, the architecture for the monitoring system, that collects, analyzes and visualizes the required metrics within the infrastructure of the considered operator, is proposed in this paper. Herein, we analyze the possibilities of the open-source solution deployment on each stage of the monitoring process from data mining and preparation up to predictive model learning.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 0,
          "unclassified": 1,
          "citingPublications": 3
        }
      },
      {
        "title": "Modeling the Direction and Volume of Trade Flows in Global Crisis, COVID-19",
        "doi": "10.1007/s40031-021-00560-2",
        "year": 2021,
        "journal": "Journal of the Institution of Engineers (India) Series B",
        "abstract": "The massive spread of COVID-19 has disrupted trading activities worldwide plunging the economy of a nation; particularly, trade-dependent nations are severely affected by the restriction in exports and imports. This paper aims to evaluate the implications of COVID-19 on the trade economy of New Zealand by exploratory data analysis and ARIMA modeling. Based on a comprehensive strategy of analysis and prediction, data were processed to notice the impact of the pandemic on trade sales. ARIMA (Auto-Regressive Integrated Moving Average) model was implemented to assess and determine the total imports and exports value of New Zealand. The efficacy of the results was tested by employing standard error analytical techniques. Analysis of the results demonstrated that the trade economy of New Zealand shows plunge with the rise of pandemic and is likely to decline in future. On the basis, it is recommended that allowing trade for essential goods as a key factor in balancing the economy of trade in New Zealand. Future scope of the research is focused to identify other factors that could stimulate the economy of New Zealand during the pandemic.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 1,
          "unclassified": 0,
          "citingPublications": 4
        }
      }
    ]
  },
  "F Test": {
    "query": "F Test",
    "count": 10,
    "papers": [
      {
        "title": "Deep Residual Learning for Image Recognition",
        "doi": "10.1109/cvpr.2016.90",
        "year": 2016,
        "journal": "",
        "abstract": "AbstractDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet <strong class=\"highlight\">test</strong> set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 120349,
          "supporting": 309,
          "contradicting": 22,
          "mentioning": 119363,
          "unclassified": 655,
          "citingPublications": 206992
        }
      },
      {
        "title": "ImageNet classification with deep convolutional neural networks",
        "doi": "10.1145/3065386",
        "year": 2017,
        "journal": "Communications of the Acm",
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the <strong class=\"highlight\">test</strong> data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 <strong class=\"highlight\">test</strong> error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 72304,
          "supporting": 171,
          "contradicting": 14,
          "mentioning": 71464,
          "unclassified": 655,
          "citingPublications": 71009
        }
      },
      {
        "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2",
        "doi": "10.1186/s13059-014-0550-8",
        "year": 2014,
        "journal": "Genome Biology",
        "abstract": "In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html.Electronic supplementary materialThe online version of this article (doi:10.1186/s13059-014-0550-8) contains supplementary material, which is available to authorized users.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 63870,
          "supporting": 97,
          "contradicting": 8,
          "mentioning": 63699,
          "unclassified": 66,
          "citingPublications": 80597
        }
      },
      {
        "title": "The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations.",
        "doi": "10.1037/0022-3514.51.6.1173",
        "year": 1986,
        "journal": "Journal of Personality and Social Psychology",
        "abstract": "In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators.The purpose of this analysis is to distinguish between the properties of moderator and mediator variables in such a way as to clarify the different ways in which conceptual variables may account for differences in peoples' behavior. Specifically, we differentiate between two often-confused functions of third variables: (a) the moderator function of third variables, which partitions a focal independent variable into subgroups that establish its domains of maximal effectiveness in regard to a given dependent variable, and (b) the mediator function of a third variable, which represents the generative mechanism through which the focal independent variable is able to influence the dependent variable of interest.Although these two functions of third variables have a relatively long tradition in the social sciences, it is not at all uncommon for social psychological researchers to u, the terms moderator and mediator interchangeably. For example, Harkins, Latan6, and Williams 0980) first summarized the impact of identifiability on social loafing by observing that it \"moderates social loafing\" (p. 303) and then within the same paragraph proposed \"that identifiability is an important mediator of social loafing: ' Similarly, Findley and Cooper (1983), intending a moderator interpretation, labeled gender, age, race, and socioeconomic level as mediators of the relation between locus of control and academic achievement. Thus, one largely pedagogiThis research was supported in part by National Science Foundation Grant BNS-8210137 and National Institute of Mental Health Grant R01 MH-40295-01 to the second author. Support was also given to him during his sabbatical year  by the MacArthur Foundation at the Center for Advanced Studies in the Behavioral Sciences, Stanford, California.Thanks are due to Judith Harackiewicz, Charles Judd, Stephen West, and Harris Cooper, who provided comments on an earlier version of this article. Stephen P. Needel was instrumental in the beginning stages of this work.Correspondence concerning this article should be addressed to Reuben M. Baron, Department of Psychology U-20, University of Connecticut, Storrs, Connecticut 06268. cal functi...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 61948,
          "supporting": 1324,
          "contradicting": 120,
          "mentioning": 59057,
          "unclassified": 1447,
          "citingPublications": 73718
        }
      },
      {
        "title": "Fitting Linear Mixed-Effects Models Usinglme4",
        "doi": "10.18637/jss.v067.i01",
        "year": 2015,
        "journal": "Journal of Statistical Software",
        "abstract": "AbstractMaximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed-and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 61400,
          "supporting": 67,
          "contradicting": 8,
          "mentioning": 61004,
          "unclassified": 321,
          "citingPublications": 84077
        }
      },
      {
        "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
        "doi": "10.1037/0021-9010.88.5.879",
        "year": 2003,
        "journal": "Journal of Applied Psychology",
        "abstract": "Interest in the problem of method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist. Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51016,
          "supporting": 569,
          "contradicting": 162,
          "mentioning": 49580,
          "unclassified": 705,
          "citingPublications": 63567
        }
      },
      {
        "title": "Gapped BLAST and PSI-BLAST: a new generation of protein database search programs",
        "doi": "10.1093/nar/25.17.3389",
        "year": 1997,
        "journal": "Nucleic Acids Research",
        "abstract": "The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 50326,
          "supporting": 111,
          "contradicting": 12,
          "mentioning": 49877,
          "unclassified": 326,
          "citingPublications": 69682
        }
      },
      {
        "title": "The Measurement of Observer Agreement for Categorical Data",
        "doi": "10.2307/2529310",
        "year": 1977,
        "journal": "Biometrics",
        "abstract": "This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of <strong class=\"highlight\">test</strong> statistics for hypotheses involving these functions. <strong class=\"highlight\">Tests</strong> for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 42579,
          "supporting": 791,
          "contradicting": 72,
          "mentioning": 39999,
          "unclassified": 1717,
          "citingPublications": 69054
        }
      },
      {
        "title": "A consistent and accurateab initioparametrization of density functional dispersion correction (DFT-D) for the 94 elements H-Pu",
        "doi": "10.1063/1.3382344",
        "year": 2010,
        "journal": "The Journal of Chemical Physics",
        "abstract": "Perspective: Explicitly correlated electronic structure theory for complex systems Perspective: Computing (ro-)vibrational spectra of molecules with more than four atoms A consistent and accurate ab initio parametrization of density functional dispersion correction (DFT-D) for the 94 elements H Today's quantum chemistry methods are extremely powerful but rely upon complex quantities such as the massively multidimensional wavefunction or even the simpler electron density. Consequently, chemical insight and a chemist's intuition are often lost in this complexity leaving the results obtained difficult to rationalize. To handle this overabundance of information, computational chemists have developed tools and methodologies that assist in composing a more intuitive picture that permits better understanding of the intricacies of chemical behavior. In particular, the fundamental comprehension of phenomena governed by non-covalent interactions is not easily achieved in terms of either the total wavefunction or the total electron density, but can be accomplished using more informative quantities. This perspective provides an overview of these tools and methods that have been specifically developed or used to analyze, identify, quantify, and visualize non-covalent interactions. These include the quantitative energy decomposition analysis schemes and the more qualitative class of approaches such as the Non-covalent Interaction index, the Density Overlap Region Indicator, or quantum theory of atoms in molecules. Aside from the enhanced knowledge gained from these schemes, their strengths, limitations, as well as a roadmap for expanding their capabilities are emphasized. ©2017Author(s).All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 38834,
          "supporting": 439,
          "contradicting": 14,
          "mentioning": 38203,
          "unclassified": 178,
          "citingPublications": 49100
        }
      },
      {
        "title": "G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences",
        "doi": "10.3758/bf03193146",
        "year": 2007,
        "journal": "Behavior Research Methods",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 34746,
          "supporting": 149,
          "contradicting": 17,
          "mentioning": 34085,
          "unclassified": 495,
          "citingPublications": 54836
        }
      }
    ]
  },
  "Chi-Squared Test": {
    "query": "Chi-Squared Test",
    "count": 10,
    "papers": [
      {
        "title": "Deep Residual Learning for Image Recognition",
        "doi": "10.1109/cvpr.2016.90",
        "year": 2016,
        "journal": "",
        "abstract": "AbstractDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet <strong class=\"highlight\">test</strong> set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 120349,
          "supporting": 309,
          "contradicting": 22,
          "mentioning": 119363,
          "unclassified": 655,
          "citingPublications": 206992
        }
      },
      {
        "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2",
        "doi": "10.1186/s13059-014-0550-8",
        "year": 2014,
        "journal": "Genome Biology",
        "abstract": "In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html.Electronic supplementary materialThe online version of this article (doi:10.1186/s13059-014-0550-8) contains supplementary material, which is available to authorized users.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 63870,
          "supporting": 97,
          "contradicting": 8,
          "mentioning": 63699,
          "unclassified": 66,
          "citingPublications": 80597
        }
      },
      {
        "title": "The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations.",
        "doi": "10.1037/0022-3514.51.6.1173",
        "year": 1986,
        "journal": "Journal of Personality and Social Psychology",
        "abstract": "In this article, we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. First, we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, both conceptually and strategically, the many ways in which moderators and mediators differ. We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, including control and stress, attitudes, and personality traits. We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, both separately and in terms of a broader causal system that includes both moderators and mediators.The purpose of this analysis is to distinguish between the properties of moderator and mediator variables in such a way as to clarify the different ways in which conceptual variables may account for differences in peoples' behavior. Specifically, we differentiate between two often-confused functions of third variables: (a) the moderator function of third variables, which partitions a focal independent variable into subgroups that establish its domains of maximal effectiveness in regard to a given dependent variable, and (b) the mediator function of a third variable, which represents the generative mechanism through which the focal independent variable is able to influence the dependent variable of interest.Although these two functions of third variables have a relatively long tradition in the social sciences, it is not at all uncommon for social psychological researchers to u, the terms moderator and mediator interchangeably. For example, Harkins, Latan6, and Williams 0980) first summarized the impact of identifiability on social loafing by observing that it \"moderates social loafing\" (p. 303) and then within the same paragraph proposed \"that identifiability is an important mediator of social loafing: ' Similarly, Findley and Cooper (1983), intending a moderator interpretation, labeled gender, age, race, and socioeconomic level as mediators of the relation between locus of control and academic achievement. Thus, one largely pedagogiThis research was supported in part by National Science Foundation Grant BNS-8210137 and National Institute of Mental Health Grant R01 MH-40295-01 to the second author. Support was also given to him during his sabbatical year  by the MacArthur Foundation at the Center for Advanced Studies in the Behavioral Sciences, Stanford, California.Thanks are due to Judith Harackiewicz, Charles Judd, Stephen West, and Harris Cooper, who provided comments on an earlier version of this article. Stephen P. Needel was instrumental in the beginning stages of this work.Correspondence concerning this article should be addressed to Reuben M. Baron, Department of Psychology U-20, University of Connecticut, Storrs, Connecticut 06268. cal functi...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 61948,
          "supporting": 1324,
          "contradicting": 120,
          "mentioning": 59057,
          "unclassified": 1447,
          "citingPublications": 73718
        }
      },
      {
        "title": "Fitting Linear Mixed-Effects Models Usinglme4",
        "doi": "10.18637/jss.v067.i01",
        "year": 2015,
        "journal": "Journal of Statistical Software",
        "abstract": "AbstractMaximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed-and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 61400,
          "supporting": 67,
          "contradicting": 8,
          "mentioning": 61004,
          "unclassified": 321,
          "citingPublications": 84077
        }
      },
      {
        "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
        "doi": "10.1037/0021-9010.88.5.879",
        "year": 2003,
        "journal": "Journal of Applied Psychology",
        "abstract": "Interest in the problem of method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist. Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51016,
          "supporting": 569,
          "contradicting": 162,
          "mentioning": 49580,
          "unclassified": 705,
          "citingPublications": 63567
        }
      },
      {
        "title": "The Measurement of Observer Agreement for Categorical Data",
        "doi": "10.2307/2529310",
        "year": 1977,
        "journal": "Biometrics",
        "abstract": "This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of <strong class=\"highlight\">test</strong> statistics for hypotheses involving these functions. <strong class=\"highlight\">Tests</strong> for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 42579,
          "supporting": 791,
          "contradicting": 72,
          "mentioning": 39999,
          "unclassified": 1717,
          "citingPublications": 69054
        }
      },
      {
        "title": "A short history ofSHELX",
        "doi": "10.1107/s0108767307043930",
        "year": 2007,
        "journal": "Acta Crystallographica Section a Foundations of Crystallography",
        "abstract": "An account is given of the development of the SHELX system of computer programs from SHELX-76 to the present day. In addition to identifying useful innovations that have come into general use through their implementation in SHELX, a critical analysis is presented of the less-successful features, missed opportunities and desirable improvements for future releases of the software. An attempt is made to understand how a program originally designed for photographic intensity data, punched cards and computers over 10 000 times slower than an average modern personal computer has managed to survive for so long. SHELXL is the most widely used program for small-molecule refinement and SHELXS and SHELXD are often employed for structure solution despite the availability of objectively superior programs. SHELXL also finds a niche for the refinement of macromolecules against high-resolution or twinned data; SHELXPRO acts as an interface for macromolecular applications. SHELXC, SHELXD and SHELXE are proving useful for the experimental phasing of macromolecules, especially because they are fast and robust and so are often employed in pipelines for high-throughput phasing. This paper could serve as a general literature citation when one or more of the open-source SHELX programs (and the Bruker AXS version SHELXTL) are employed in the course of a crystal-structure determination.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 41114,
          "supporting": 144,
          "contradicting": 2,
          "mentioning": 40783,
          "unclassified": 185,
          "citingPublications": 86469
        }
      },
      {
        "title": "Measuring inconsistency in meta-analyses",
        "doi": "10.1136/bmj.327.7414.557",
        "year": 2003,
        "journal": "BMJ",
        "abstract": "Background: Postoperative delirium is a common complication characterized by confusion, inattentiveness and other mental symptoms. It is still unclear whether the use of electroencephalogram (EEG) monitoring during surgery can decrease the incidence of postoperative delirium. The purpose of this study was to evaluate the effectiveness of EEG guided anesthesia on postoperative delirium (POD) based on randomized controlled trials (RCTs). Methods: The electronic databases of Ovid MEDLINE, PubMed, EMBASE, Cochrane Library database, CNKI and other local databases were systematically searched for RCTs from their inception until October 2019. The odds ratios (ORs) and the mean differences (MDs) with a 95% confidence intervals (CIs) were calculated to evaluate the correlation between EEG and itemized categories and continuous variable, respectively. Results: Seven RCTs with 3859 patients were included in the final analysis. The summary OR indicated that patients receiving EEG monitoring had a lower incidence rate of postoperative delirium (OR: 0.65; 95% CI: 0.46-0.92; P = 0.01). In addition, no significant difference was found between the EEG monitoring group and the routine care group with respects to the length of hospitalization (MD:-0.59; 95%CI:-1.26 to 0.07; P=0.08). Conclusions: The findings of this study indicated that intraoperative use of electroencephalogram monitoring could decrease the risk of postoperative delirium. But for high risk patients, we should take a multi-component strategy to prevent delirium. Further large-scale, randomized controlled trials should be conducted to verify the treatment effect of intraoperative use of electroencephalogram monitoring on patients.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 36427,
          "supporting": 85,
          "contradicting": 15,
          "mentioning": 36083,
          "unclassified": 244,
          "citingPublications": 55602
        }
      },
      {
        "title": "Bias in meta-analysis detected by a simple, graphical <strong class=\"highlight\">test</strong>",
        "doi": "10.1136/bmj.315.7109.629",
        "year": 1997,
        "journal": "BMJ",
        "abstract": "AbstractObjective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple <strong class=\"highlight\">test</strong> of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews. Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38%) journal meta-analyses and 5 (13%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful <strong class=\"highlight\">test</strong> for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 30448,
          "supporting": 69,
          "contradicting": 13,
          "mentioning": 30201,
          "unclassified": 165,
          "citingPublications": 49041
        }
      },
      {
        "title": "Regression Shrinkage and Selection Via the Lasso",
        "doi": "10.1111/j.2517-6161.1996.tb02080.x",
        "year": 1996,
        "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
        "abstract": "We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of <strong class=\"highlight\">squares</strong> subject to the sum of the absolute value of the coefficientsbeing less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 30306,
          "supporting": 84,
          "contradicting": 9,
          "mentioning": 29998,
          "unclassified": 215,
          "citingPublications": 46002
        }
      }
    ]
  },
  "Exponential Smoothing": {
    "query": "Exponential Smoothing",
    "count": 10,
    "papers": [
      {
        "title": "Deep learning",
        "doi": "10.1038/nature14539",
        "year": 2015,
        "journal": "Nature",
        "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 0,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 0,
          "unclassified": 0,
          "citingPublications": 0
        }
      },
      {
        "title": "limma powers differential expression analyses for RNA-sequencing and microarray studies",
        "doi": "10.1093/nar/gkv007",
        "year": 2015,
        "journal": "Nucleic Acids Research",
        "abstract": "limma is an R/Bioconductor software package that provides an integrated solution for analysing data from gene expression experiments. It contains rich features for handling complex experimental designs and for information borrowing to overcome the problem of small sample sizes. Over the past decade, limma has been a popular choice for gene discovery through differential expression analyses of microarray and high-throughput PCR data. The package contains particularly strong facilities for reading, normalizing and exploring such data. Recently, the capabilities of limma have been significantly expanded in two important directions. First, the package can now perform both differential expression and differential splicing analyses of RNA sequencing (RNA-seq) data. All the downstream analysis tools previously restricted to microarray data are now available for RNA-seq as well. These capabilities allow users to analyse both RNA-seq and microarray data with very similar pipelines. Second, the package is now able to go past the traditional gene-wise expression analyses in a variety of ways, analysing expression profiles in terms of co-regulated sets of genes or in terms of higher-order expression signatures. This provides enhanced possibilities for biological interpretation of gene expression differences. This article reviews the philosophy and design of the limma package, summarizing both new and historical features, with an emphasis on recent enhancements and features that have not been previously described.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 28343,
          "supporting": 33,
          "contradicting": 1,
          "mentioning": 28269,
          "unclassified": 40,
          "citingPublications": 33810
        }
      },
      {
        "title": "The electronic properties of graphene",
        "doi": "10.1103/revmodphys.81.109",
        "year": 2009,
        "journal": "Reviews of Modern Physics",
        "abstract": "This article reviews the basic theoretical aspects of graphene, a one atom\nthick allotrope of carbon, with unusual two-dimensional Dirac-like electronic\nexcitations. The Dirac electrons can be controlled by application of external\nelectric and magnetic fields, or by altering sample geometry and/or topology.\nWe show that the Dirac electrons behave in unusual ways in tunneling,\nconfinement, and integer quantum Hall effect. We discuss the electronic\nproperties of graphene stacks and show that they vary with stacking order and\nnumber of layers. Edge (surface) states in graphene are strongly dependent on\nthe edge termination (zigzag or armchair) and affect the physical properties of\nnanoribbons. We also discuss how different types of disorder modify the Dirac\nequation leading to unusual spectroscopic and transport properties. The effects\nof electron-electron and electron-phonon interactions in single layer and\nmultilayer graphene are also presented.Comment: Accepted for publication in Reviews of Modern Physics. Various\n  sessions and figures were modified, and new references adde",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 18350,
          "supporting": 319,
          "contradicting": 12,
          "mentioning": 17837,
          "unclassified": 182,
          "citingPublications": 23990
        }
      },
      {
        "title": "A <strong class=\"highlight\">smooth</strong> particle mesh Ewald method",
        "doi": "10.1063/1.470117",
        "year": 1995,
        "journal": "The Journal of Chemical Physics",
        "abstract": "The previously developed particle mesh Ewald method is reformulated in terms of efficient B-spline interpolation of the structure factors. This reformulation allows a natural extension of the method to potentials of the form 1/r p with pу1. Furthermore, efficient calculation of the virial tensor follows. Use of B-splines in place of Lagrange interpolation leads to analytic gradients as well as a significant improvement in the accuracy. We demonstrate that arbitrary accuracy can be achieved, independent of system size N, at a cost that scales as N log(N). For biomolecular systems with many thousands of atoms this method permits the use of Ewald summation at a computational cost comparable to that of a simple truncation method of 10 Å or less.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16931,
          "supporting": 43,
          "contradicting": 1,
          "mentioning": 16840,
          "unclassified": 47,
          "citingPublications": 21528
        }
      },
      {
        "title": "Maximum entropy modeling of species geographic distributions",
        "doi": "10.1016/j.ecolmodel.2005.03.026",
        "year": 2006,
        "journal": "Ecological Modelling",
        "abstract": "The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of the method, here we perform a continental-scale case study using two Neotropical mammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus. We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC curve (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16098,
          "supporting": 39,
          "contradicting": 2,
          "mentioning": 15346,
          "unclassified": 711,
          "citingPublications": 15801
        }
      },
      {
        "title": "Scalable molecular dynamics with NAMD",
        "doi": "10.1002/jcc.20289",
        "year": 2005,
        "journal": "Journal of Computational Chemistry",
        "abstract": "AbstractNAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NAMD scales to hundreds of processors on high-end parallel platforms, as well as tens of processors on low-cost commodity clusters, and also runs on individual desktop and laptop computers. NAMD works with AMBER and CHARMM potential functions, parameters, and file formats. This paper, directed to novices as well as experts, first introduces concepts and methods used in the NAMD program, describing the classical molecular dynamics force field, equations of motion, and integration methods along with the efficient electrostatics evaluation algorithms employed and temperature and pressure controls used. Features for steering the simulation across barriers and for calculating both alchemical and conformational free energy differences are presented. The motivations for and a roadmap to the internal design of NAMD, implemented in C++ and based on Charm++ parallel objects, are outlined. The factors affecting the serial and parallel performance of a simulation are discussed. Next, typical NAMD use is illustrated with representative applications to a small, a medium, and a large biomolecular system, highlighting particular features of NAMD, e.g., the Tcl scripting language. Finally, the paper provides a list of the key features of NAMD and discusses the benefits of combining NAMD with the molecular graphics/sequence analysis software VMD and the grid computing/collaboratory software BioCoRE. NAMD is distributed free of charge with source code at www.ks.uiuc.edu.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 15009,
          "supporting": 35,
          "contradicting": 2,
          "mentioning": 14899,
          "unclassified": 73,
          "citingPublications": 16703
        }
      },
      {
        "title": "Regression Models and Life-Tables",
        "doi": "10.1007/978-1-4612-4380-9_37",
        "year": 1992,
        "journal": "",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 14256,
          "supporting": 49,
          "contradicting": 2,
          "mentioning": 14002,
          "unclassified": 203,
          "citingPublications": 29006
        }
      },
      {
        "title": "Regression Models and Life-Tables",
        "doi": "10.1111/j.2517-6161.1972.tb00899.x",
        "year": 1972,
        "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
        "abstract": "Summary\nThe analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age‐specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12648,
          "supporting": 30,
          "contradicting": 1,
          "mentioning": 12307,
          "unclassified": 310,
          "citingPublications": 39510
        }
      },
      {
        "title": "Quantum Mechanical Continuum Solvation Models",
        "doi": "10.1021/cr9904009",
        "year": 2005,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12363,
          "supporting": 124,
          "contradicting": 1,
          "mentioning": 12181,
          "unclassified": 57,
          "citingPublications": 15704
        }
      },
      {
        "title": "Integrated analysis of multimodal single-cell data",
        "doi": "10.1016/j.cell.2021.04.048",
        "year": 2021,
        "journal": "Cell",
        "abstract": "Summary\n    \n\n     The simultaneous measurement of multiple modalities represents an exciting frontier for single-cell genomics and necessitates computational methods that can define cellular states based on multimodal data. Here, we introduce “weighted-nearest neighbor” analysis, an unsupervised framework to learn the relative utility of each data type in each cell, enabling an integrative analysis of multiple modalities. We apply our procedure to a CITE-seq dataset of 211,000 human peripheral blood mononuclear cells (PBMCs) with panels extending to 228 antibodies to construct a multimodal reference atlas of the circulating immune system. Multimodal analysis substantially improves our ability to resolve cell states, allowing us to identify and validate previously unreported lymphoid subpopulations. Moreover, we demonstrate how to leverage this reference to rapidly map new datasets and to interpret immune responses to vaccination and coronavirus disease 2019 (COVID-19). Our approach represents a broadly applicable strategy to analyze single-cell multimodal datasets and to look beyond the transcriptome toward a unified and multimodal definition of cellular identity.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 11217,
          "supporting": 66,
          "contradicting": 0,
          "mentioning": 11150,
          "unclassified": 1,
          "citingPublications": 11619
        }
      }
    ]
  },
  "Exponential": {
    "query": "Exponential",
    "count": 10,
    "papers": [
      {
        "title": "Deep Residual Learning for Image Recognition",
        "doi": "10.1109/cvpr.2016.90",
        "year": 2016,
        "journal": "",
        "abstract": "AbstractDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 120349,
          "supporting": 309,
          "contradicting": 22,
          "mentioning": 119363,
          "unclassified": 655,
          "citingPublications": 206992
        }
      },
      {
        "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
        "doi": "10.1037/0021-9010.88.5.879",
        "year": 2003,
        "journal": "Journal of Applied Psychology",
        "abstract": "Interest in the problem of method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist. Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51016,
          "supporting": 569,
          "contradicting": 162,
          "mentioning": 49580,
          "unclassified": 705,
          "citingPublications": 63567
        }
      },
      {
        "title": "The Measurement of Observer Agreement for Categorical Data",
        "doi": "10.2307/2529310",
        "year": 1977,
        "journal": "Biometrics",
        "abstract": "This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 42579,
          "supporting": 791,
          "contradicting": 72,
          "mentioning": 39999,
          "unclassified": 1717,
          "citingPublications": 69054
        }
      },
      {
        "title": "Deep learning",
        "doi": "10.1038/nature14539",
        "year": 2015,
        "journal": "Nature",
        "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 41814,
          "supporting": 87,
          "contradicting": 4,
          "mentioning": 41107,
          "unclassified": 616,
          "citingPublications": 75083
        }
      },
      {
        "title": "A consistent and accurateab initioparametrization of density functional dispersion correction (DFT-D) for the 94 elements H-Pu",
        "doi": "10.1063/1.3382344",
        "year": 2010,
        "journal": "The Journal of Chemical Physics",
        "abstract": "Perspective: Explicitly correlated electronic structure theory for complex systems Perspective: Computing (ro-)vibrational spectra of molecules with more than four atoms A consistent and accurate ab initio parametrization of density functional dispersion correction (DFT-D) for the 94 elements H Today's quantum chemistry methods are extremely powerful but rely upon complex quantities such as the massively multidimensional wavefunction or even the simpler electron density. Consequently, chemical insight and a chemist's intuition are often lost in this complexity leaving the results obtained difficult to rationalize. To handle this overabundance of information, computational chemists have developed tools and methodologies that assist in composing a more intuitive picture that permits better understanding of the intricacies of chemical behavior. In particular, the fundamental comprehension of phenomena governed by non-covalent interactions is not easily achieved in terms of either the total wavefunction or the total electron density, but can be accomplished using more informative quantities. This perspective provides an overview of these tools and methods that have been specifically developed or used to analyze, identify, quantify, and visualize non-covalent interactions. These include the quantitative energy decomposition analysis schemes and the more qualitative class of approaches such as the Non-covalent Interaction index, the Density Overlap Region Indicator, or quantum theory of atoms in molecules. Aside from the enhanced knowledge gained from these schemes, their strengths, limitations, as well as a roadmap for expanding their capabilities are emphasized. ©2017Author(s).All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 38834,
          "supporting": 439,
          "contradicting": 14,
          "mentioning": 38203,
          "unclassified": 178,
          "citingPublications": 49100
        }
      },
      {
        "title": "limma powers differential expression analyses for RNA-sequencing and microarray studies",
        "doi": "10.1093/nar/gkv007",
        "year": 2015,
        "journal": "Nucleic Acids Research",
        "abstract": "limma is an R/Bioconductor software package that provides an integrated solution for analysing data from gene expression experiments. It contains rich features for handling complex experimental designs and for information borrowing to overcome the problem of small sample sizes. Over the past decade, limma has been a popular choice for gene discovery through differential expression analyses of microarray and high-throughput PCR data. The package contains particularly strong facilities for reading, normalizing and exploring such data. Recently, the capabilities of limma have been significantly expanded in two important directions. First, the package can now perform both differential expression and differential splicing analyses of RNA sequencing (RNA-seq) data. All the downstream analysis tools previously restricted to microarray data are now available for RNA-seq as well. These capabilities allow users to analyse both RNA-seq and microarray data with very similar pipelines. Second, the package is now able to go past the traditional gene-wise expression analyses in a variety of ways, analysing expression profiles in terms of co-regulated sets of genes or in terms of higher-order expression signatures. This provides enhanced possibilities for biological interpretation of gene expression differences. This article reviews the philosophy and design of the limma package, summarizing both new and historical features, with an emphasis on recent enhancements and features that have not been previously described.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 28343,
          "supporting": 33,
          "contradicting": 1,
          "mentioning": 28269,
          "unclassified": 40,
          "citingPublications": 33810
        }
      },
      {
        "title": "Adam: A Method for Stochastic Optimization",
        "doi": "10.48550/arxiv.1412.6980",
        "year": 2014,
        "journal": "",
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23561,
          "supporting": 22,
          "contradicting": 0,
          "mentioning": 23465,
          "unclassified": 74,
          "citingPublications": 60969
        }
      },
      {
        "title": "Maximum Likelihood from Incomplete Data Via the EM Algorithm",
        "doi": "10.1111/j.2517-6161.1977.tb01600.x",
        "year": 1977,
        "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
        "abstract": "Read before the ROYAL STATISTICAL SOCIETY at a meeting organized by the RESEARCH SECTION on Wednesday, December 8th, 1976, Professor S. D. SILVEY in the Chair] SUMMARY A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23283,
          "supporting": 27,
          "contradicting": 2,
          "mentioning": 22884,
          "unclassified": 370,
          "citingPublications": 44904
        }
      },
      {
        "title": "A pneumonia outbreak associated with a new coronavirus of probable bat origin",
        "doi": "10.1038/s41586-020-2012-7",
        "year": 2020,
        "journal": "Nature",
        "abstract": "Article Extended Data Fig. 7 | Analysis of 2019-nCoV receptor usage. Determination of virus infectivity in HeLa cells with or without the expression of human APN and DPP4. The expression of ACE2, APN and DPP4 plasmids with S tag were detected using mouse anti-S tag monoclonal antibody. ACE2, APN and DPP4 proteins (green), viral protein (red) and nuclei (blue) are shown. Scale bars, 10 μm.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Addendum",
            "date": "2020-11-17",
            "noticeDoi": "10.1038/s41586-020-2951-z",
            "doi": "10.1038/s41586-020-2012-7",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23058,
          "supporting": 260,
          "contradicting": 15,
          "mentioning": 21997,
          "unclassified": 786,
          "citingPublications": 22452
        }
      },
      {
        "title": "When to use and how to report the results of PLS-SEM",
        "doi": "10.1108/ebr-11-2018-0203",
        "year": 2019,
        "journal": "European Business Review",
        "abstract": "Purpose\nThe purpose of this paper is to provide a comprehensive, yet concise, overview of the considerations and metrics required for partial least squares structural equation modeling (PLS-SEM) analysis and result reporting. Preliminary considerations are summarized first, including reasons for choosing PLS-SEM, recommended sample size in selected contexts, distributional assumptions, use of secondary data, statistical power and the need for goodness-of-fit testing. Next, the metrics as well as the rules of thumb that should be applied to assess the PLS-SEM results are covered. Besides presenting established PLS-SEM evaluation criteria, the overview includes the following new guidelines: PLSpredict (i.e., a novel approach for assessing a model’s out-of-sample prediction), metrics for model comparisons, and several complementary methods for checking the results’ robustness.\n\n\nDesign/methodology/approach\nThis paper provides an overview of previously and recently proposed metrics as well as rules of thumb for evaluating the research results based on the application of PLS-SEM.\n\n\nFindings\nMost of the previously applied metrics for evaluating PLS-SEM results are still relevant. Nevertheless, scholars need to be knowledgeable about recently proposed metrics (e.g. model comparison criteria) and methods (e.g. endogeneity assessment, latent class analysis and PLSpredict), and when and how to apply them to extend their analyses.\n\n\nResearch limitations/implications\nMethodological developments associated with PLS-SEM are rapidly emerging. The metrics reported in this paper are useful for current applications, but must always be up to date with the latest developments in the PLS-SEM method.\n\n\nOriginality/value\nIn light of more recent research and methodological developments in the PLS-SEM domain, guidelines for the method’s use need to be continuously extended and updated. This paper is the most current and comprehensive summary of the PLS-SEM method and the metrics applied to assess its solutions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23047,
          "supporting": 563,
          "contradicting": 27,
          "mentioning": 21451,
          "unclassified": 1006,
          "citingPublications": 17285
        }
      }
    ]
  },
  "Sum of Sines Splines": {
    "query": "Sum of Sines Splines",
    "count": 10,
    "papers": [
      {
        "title": "Moderated estimation <strong class=\"highlight\">of</strong> fold change and dispersion for RNA-seq data with DESeq2",
        "doi": "10.1186/s13059-014-0550-8",
        "year": 2014,
        "journal": "Genome Biology",
        "abstract": "In comparative high-throughput sequencing assays, a fundamental task is the analysis <strong class=\"highlight\">of</strong> count data, such as read counts per gene in RNA-seq, for evidence <strong class=\"highlight\">of</strong> systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence <strong class=\"highlight\">of</strong> outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis <strong class=\"highlight\">of</strong> count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability <strong class=\"highlight\">of</strong> estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence <strong class=\"highlight\">of</strong> differential expression. The DESeq2 package is available at http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html.Electronic supplementary materialThe online version <strong class=\"highlight\">of</strong> this article (doi:10.1186/s13059-014-0550-8) contains supplementary material, which is available to authorized users.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 63870,
          "supporting": 97,
          "contradicting": 8,
          "mentioning": 63699,
          "unclassified": 66,
          "citingPublications": 80597
        }
      },
      {
        "title": "Fitting Linear Mixed-Effects Models Usinglme4",
        "doi": "10.18637/jss.v067.i01",
        "year": 2015,
        "journal": "Journal <strong Class=\"highlight\">of</Strong> Statistical Software",
        "abstract": "AbstractMaximum likelihood or restricted maximum likelihood (REML) estimates <strong class=\"highlight\">of</strong> the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed-and random-effects terms. The formula and data together determine a numerical representation <strong class=\"highlight\">of</strong> the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function <strong class=\"highlight\">of</strong> some <strong class=\"highlight\">of</strong> the model parameters. The appropriate criterion is optimized, using one <strong class=\"highlight\">of</strong> the constrained optimization functions in R, to provide the parameter estimates. We describe the structure <strong class=\"highlight\">of</strong> the model, the steps in evaluating the profiled deviance or REML criterion, and the structure <strong class=\"highlight\">of</strong> classes or types that represents such a model. Sufficient detail is included to allow specialization <strong class=\"highlight\">of</strong> these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing <strong class=\"highlight\">splines</strong>, that are not easily expressible in the formula language used by lmer.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 61400,
          "supporting": 67,
          "contradicting": 8,
          "mentioning": 61004,
          "unclassified": 321,
          "citingPublications": 84077
        }
      },
      {
        "title": "Common method biases in behavioral research: A critical review <strong class=\"highlight\">of</strong> the literature and recommended remedies.",
        "doi": "10.1037/0021-9010.88.5.879",
        "year": 2003,
        "journal": "Journal <strong Class=\"highlight\">of</Strong> Applied Psychology",
        "abstract": "Interest in the problem <strong class=\"highlight\">of</strong> method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary <strong class=\"highlight\">of</strong> the potential sources <strong class=\"highlight\">of</strong> method biases and how to control for them does not exist. Therefore, the purpose <strong class=\"highlight\">of</strong> this article is to examine the extent to which method biases influence behavioral research results, identify potential sources <strong class=\"highlight\">of</strong> method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types <strong class=\"highlight\">of</strong> research settings.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51016,
          "supporting": 569,
          "contradicting": 162,
          "mentioning": 49580,
          "unclassified": 705,
          "citingPublications": 63567
        }
      },
      {
        "title": "Gapped BLAST and PSI-BLAST: a new generation <strong class=\"highlight\">of</strong> protein database search programs",
        "doi": "10.1093/nar/25.17.3389",
        "year": 1997,
        "journal": "Nucleic Acids Research",
        "abstract": "The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety <strong class=\"highlight\">of</strong> definitional, algorithmic and statistical refinements described here permits the execution time <strong class=\"highlight\">of</strong> the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension <strong class=\"highlight\">of</strong> word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed <strong class=\"highlight\">of</strong> the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members <strong class=\"highlight\">of</strong> the BRCT superfamily.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 50326,
          "supporting": 111,
          "contradicting": 12,
          "mentioning": 49877,
          "unclassified": 326,
          "citingPublications": 69682
        }
      },
      {
        "title": "A consistent and accurate<i>ab initio</i>parametrization <strong class=\"highlight\">of</strong> density functional dispersion correction (DFT-D) for the 94 elements H-Pu",
        "doi": "10.1063/1.3382344",
        "year": 2010,
        "journal": "The Journal <strong Class=\"highlight\">of</Strong> Chemical Physics",
        "abstract": "Perspective: Explicitly correlated electronic structure theory for complex systems Perspective: Computing (ro-)vibrational spectra <strong class=\"highlight\">of</strong> molecules with more than four atoms A consistent and accurate ab initio parametrization <strong class=\"highlight\">of</strong> density functional dispersion correction (DFT-D) for the 94 elements H Today's quantum chemistry methods are extremely powerful but rely upon complex quantities such as the massively multidimensional wavefunction or even the simpler electron density. Consequently, chemical insight and a chemist's intuition are often lost in this complexity leaving the results obtained difficult to rationalize. To handle this overabundance <strong class=\"highlight\">of</strong> information, computational chemists have developed tools and methodologies that assist in composing a more intuitive picture that permits better understanding <strong class=\"highlight\">of</strong> the intricacies <strong class=\"highlight\">of</strong> chemical behavior. In particular, the fundamental comprehension <strong class=\"highlight\">of</strong> phenomena governed by non-covalent interactions is not easily achieved in terms <strong class=\"highlight\">of</strong> either the total wavefunction or the total electron density, but can be accomplished using more informative quantities. This perspective provides an overview <strong class=\"highlight\">of</strong> these tools and methods that have been specifically developed or used to analyze, identify, quantify, and visualize non-covalent interactions. These include the quantitative energy decomposition analysis schemes and the more qualitative class <strong class=\"highlight\">of</strong> approaches such as the Non-covalent Interaction index, the Density Overlap Region Indicator, or quantum theory <strong class=\"highlight\">of</strong> atoms in molecules. Aside from the enhanced knowledge gained from these schemes, their strengths, limitations, as well as a roadmap for expanding their capabilities are emphasized. ©2017Author(s).All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 38834,
          "supporting": 439,
          "contradicting": 14,
          "mentioning": 38203,
          "unclassified": 178,
          "citingPublications": 49100
        }
      },
      {
        "title": "Measuring inconsistency in meta-analyses",
        "doi": "10.1136/bmj.327.7414.557",
        "year": 2003,
        "journal": "BMJ",
        "abstract": "Background: Postoperative delirium is a common complication characterized by confusion, inattentiveness and other mental symptoms. It is still unclear whether the use <strong class=\"highlight\">of</strong> electroencephalogram (EEG) monitoring during surgery can decrease the incidence <strong class=\"highlight\">of</strong> postoperative delirium. The purpose <strong class=\"highlight\">of</strong> this study was to evaluate the effectiveness <strong class=\"highlight\">of</strong> EEG guided anesthesia on postoperative delirium (POD) based on randomized controlled trials (RCTs). Methods: The electronic databases <strong class=\"highlight\">of</strong> Ovid MEDLINE, PubMed, EMBASE, Cochrane Library database, CNKI and other local databases were systematically searched for RCTs from their inception until October 2019. The odds ratios (ORs) and the mean differences (MDs) with a 95% confidence intervals (CIs) were calculated to evaluate the correlation between EEG and itemized categories and continuous variable, respectively. Results: Seven RCTs with 3859 patients were included in the final analysis. The summary OR indicated that patients receiving EEG monitoring had a lower incidence rate <strong class=\"highlight\">of</strong> postoperative delirium (OR: 0.65; 95% CI: 0.46-0.92; P = 0.01). In addition, no significant difference was found between the EEG monitoring group and the routine care group with respects to the length <strong class=\"highlight\">of</strong> hospitalization (MD:-0.59; 95%CI:-1.26 to 0.07; P=0.08). Conclusions: The findings <strong class=\"highlight\">of</strong> this study indicated that intraoperative use <strong class=\"highlight\">of</strong> electroencephalogram monitoring could decrease the risk <strong class=\"highlight\">of</strong> postoperative delirium. But for high risk patients, we should take a multi-component strategy to prevent delirium. Further large-scale, randomized controlled trials should be conducted to verify the treatment effect <strong class=\"highlight\">of</strong> intraoperative use <strong class=\"highlight\">of</strong> electroencephalogram monitoring on patients.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 36427,
          "supporting": 85,
          "contradicting": 15,
          "mentioning": 36083,
          "unclassified": 244,
          "citingPublications": 55602
        }
      },
      {
        "title": "Highly accurate protein structure prediction with AlphaFold",
        "doi": "10.1038/s41586-021-03819-2",
        "year": 2021,
        "journal": "Nature",
        "abstract": "R on ne be rg er , K a t hr yn T un ya su vu na ko ol,",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 34072,
          "supporting": 515,
          "contradicting": 15,
          "mentioning": 33472,
          "unclassified": 70,
          "citingPublications": 34053
        }
      },
      {
        "title": "MUSCLE: multiple sequence alignment with high accuracy and high throughput",
        "doi": "10.1093/nar/gkh340",
        "year": 2004,
        "journal": "Nucleic Acids Research",
        "abstract": "We describe MUSCLE, a new computer program for creating multiple alignments <strong class=\"highlight\">of</strong> protein sequences. Elements <strong class=\"highlight\">of</strong> the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy <strong class=\"highlight\">of</strong> MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets <strong class=\"highlight\">of</strong> reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each <strong class=\"highlight\">of</strong> these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest <strong class=\"highlight\">of</strong> the tested methods for large numbers <strong class=\"highlight\">of</strong> sequences, aligning 5000 sequences <strong class=\"highlight\">of</strong> average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5. com/muscle.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 32338,
          "supporting": 41,
          "contradicting": 1,
          "mentioning": 32123,
          "unclassified": 173,
          "citingPublications": 42368
        }
      },
      {
        "title": "Regression Shrinkage and Selection Via the Lasso",
        "doi": "10.1111/j.2517-6161.1996.tb02080.x",
        "year": 1996,
        "journal": "Journal <strong Class=\"highlight\">of</Strong> the Royal Statistical Society Series B (Statistical Methodology)",
        "abstract": "We propose a new method for estimation in linear models. The 'lasso' minimizes the residual <strong class=\"highlight\">sum</strong> <strong class=\"highlight\">of</strong> squares subject to the <strong class=\"highlight\">sum</strong> <strong class=\"highlight\">of</strong> the absolute value <strong class=\"highlight\">of</strong> the coefficientsbeing less than a constant. Because <strong class=\"highlight\">of</strong> the nature <strong class=\"highlight\">of</strong> this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some <strong class=\"highlight\">of</strong> the favourable properties <strong class=\"highlight\">of</strong> both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability <strong class=\"highlight\">of</strong> ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety <strong class=\"highlight\">of</strong> statistical models: extensions to generalized regression models and tree-based models are briefly described.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 30306,
          "supporting": 84,
          "contradicting": 9,
          "mentioning": 29998,
          "unclassified": 215,
          "citingPublications": 46002
        }
      },
      {
        "title": "PLINK: A Tool Set for Whole-Genome Association and Population-Based Linkage Analyses",
        "doi": "10.1086/519795",
        "year": 2007,
        "journal": "The American Journal <strong Class=\"highlight\">of</Strong> Human Genetics",
        "abstract": "Whole-genome association studies (WGAS) bring new computational, as well as analytic, challenges to researchers. Many existing genetic-analysis tools are not designed to handle such large data sets in a convenient manner and do not necessarily exploit the new opportunities that whole-genome data bring. To address these issues, we developed PLINK, an open-source C/C++ WGAS tool set. With PLINK, large data sets comprising hundreds <strong class=\"highlight\">of</strong> thousands <strong class=\"highlight\">of</strong> markers genotyped for thousands <strong class=\"highlight\">of</strong> individuals can be rapidly manipulated and analyzed in their entirety. As well as providing tools to make the basic analytic steps computationally efficient, PLINK also supports some novel approaches to whole-genome data that take advantage <strong class=\"highlight\">of</strong> whole-genome coverage. We introduce PLINK and describe the five main domains <strong class=\"highlight\">of</strong> function: data management, summary statistics, population stratification, association analysis, and identity-by-descent estimation. In particular, we focus on the estimation and use <strong class=\"highlight\">of</strong> identity-by-state and identity-by-descent information in the context <strong class=\"highlight\">of</strong> population-based whole-genome studies. This information can be used to detect and correct for population stratification and to identify extended chromosomal segments that are shared identical by descent between very distantly related individuals. Analysis <strong class=\"highlight\">of</strong> the patterns <strong class=\"highlight\">of</strong> segmental sharing has the potential to map disease loci that contain multiple rare variants in a population-based linkage analysis.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 30262,
          "supporting": 53,
          "contradicting": 2,
          "mentioning": 30094,
          "unclassified": 113,
          "citingPublications": 30846
        }
      }
    ]
  },
  "Weighted Moving Average": {
    "query": "Weighted Moving Average",
    "count": 10,
    "papers": [
      {
        "title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2",
        "doi": "10.1186/s13059-014-0550-8",
        "year": 2014,
        "journal": "Genome Biology",
        "abstract": "In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html.Electronic supplementary materialThe online version of this article (doi:10.1186/s13059-014-0550-8) contains supplementary material, which is available to authorized users.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 63870,
          "supporting": 97,
          "contradicting": 8,
          "mentioning": 63699,
          "unclassified": 66,
          "citingPublications": 80597
        }
      },
      {
        "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
        "doi": "10.1037/0021-9010.88.5.879",
        "year": 2003,
        "journal": "Journal of Applied Psychology",
        "abstract": "Interest in the problem of method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist. Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51016,
          "supporting": 569,
          "contradicting": 162,
          "mentioning": 49580,
          "unclassified": 705,
          "citingPublications": 63567
        }
      },
      {
        "title": "Gradient-based learning applied to document recognition",
        "doi": "10.1109/5.726791",
        "year": 1998,
        "journal": "Proceedings of the Ieee",
        "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradientbased learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques.Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure.Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks.A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 25812,
          "supporting": 80,
          "contradicting": 8,
          "mentioning": 25415,
          "unclassified": 309,
          "citingPublications": 52110
        }
      },
      {
        "title": "Adam: A Method for Stochastic Optimization",
        "doi": "10.48550/arxiv.1412.6980",
        "year": 2014,
        "journal": "",
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. * Equal contribution. Author ordering determined by coin flip over a Google Hangout.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23561,
          "supporting": 22,
          "contradicting": 0,
          "mentioning": 23465,
          "unclassified": 74,
          "citingPublications": 60969
        }
      },
      {
        "title": "When to use and how to report the results of PLS-SEM",
        "doi": "10.1108/ebr-11-2018-0203",
        "year": 2019,
        "journal": "European Business Review",
        "abstract": "Purpose\nThe purpose of this paper is to provide a comprehensive, yet concise, overview of the considerations and metrics required for partial least squares structural equation modeling (PLS-SEM) analysis and result reporting. Preliminary considerations are summarized first, including reasons for choosing PLS-SEM, recommended sample size in selected contexts, distributional assumptions, use of secondary data, statistical power and the need for goodness-of-fit testing. Next, the metrics as well as the rules of thumb that should be applied to assess the PLS-SEM results are covered. Besides presenting established PLS-SEM evaluation criteria, the overview includes the following new guidelines: PLSpredict (i.e., a novel approach for assessing a model’s out-of-sample prediction), metrics for model comparisons, and several complementary methods for checking the results’ robustness.\n\n\nDesign/methodology/approach\nThis paper provides an overview of previously and recently proposed metrics as well as rules of thumb for evaluating the research results based on the application of PLS-SEM.\n\n\nFindings\nMost of the previously applied metrics for evaluating PLS-SEM results are still relevant. Nevertheless, scholars need to be knowledgeable about recently proposed metrics (e.g. model comparison criteria) and methods (e.g. endogeneity assessment, latent class analysis and PLSpredict), and when and how to apply them to extend their analyses.\n\n\nResearch limitations/implications\nMethodological developments associated with PLS-SEM are rapidly emerging. The metrics reported in this paper are useful for current applications, but must always be up to date with the latest developments in the PLS-SEM method.\n\n\nOriginality/value\nIn light of more recent research and methodological developments in the PLS-SEM domain, guidelines for the method’s use need to be continuously extended and updated. This paper is the most current and comprehensive summary of the PLS-SEM method and the metrics applied to assess its solutions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23047,
          "supporting": 563,
          "contradicting": 27,
          "mentioning": 21451,
          "unclassified": 1006,
          "citingPublications": 17285
        }
      },
      {
        "title": "ImageNet Large Scale Visual Recognition Challenge",
        "doi": "10.1007/s11263-015-0816-y",
        "year": 2015,
        "journal": "International Journal of Computer Vision",
        "abstract": "Abstract The ImageNet Large Scale Visual RecognitionChallenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-ofthe-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 22215,
          "supporting": 63,
          "contradicting": 2,
          "mentioning": 21960,
          "unclassified": 190,
          "citingPublications": 38692
        }
      },
      {
        "title": "The electronic properties of graphene",
        "doi": "10.1103/revmodphys.81.109",
        "year": 2009,
        "journal": "Reviews of Modern Physics",
        "abstract": "This article reviews the basic theoretical aspects of graphene, a one atom\nthick allotrope of carbon, with unusual two-dimensional Dirac-like electronic\nexcitations. The Dirac electrons can be controlled by application of external\nelectric and magnetic fields, or by altering sample geometry and/or topology.\nWe show that the Dirac electrons behave in unusual ways in tunneling,\nconfinement, and integer quantum Hall effect. We discuss the electronic\nproperties of graphene stacks and show that they vary with stacking order and\nnumber of layers. Edge (surface) states in graphene are strongly dependent on\nthe edge termination (zigzag or armchair) and affect the physical properties of\nnanoribbons. We also discuss how different types of disorder modify the Dirac\nequation leading to unusual spectroscopic and transport properties. The effects\nof electron-electron and electron-phonon interactions in single layer and\nmultilayer graphene are also presented.Comment: Accepted for publication in Reviews of Modern Physics. Various\n  sessions and figures were modified, and new references adde",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 18350,
          "supporting": 319,
          "contradicting": 12,
          "mentioning": 17837,
          "unclassified": 182,
          "citingPublications": 23990
        }
      },
      {
        "title": "Statistical mechanics of complex networks",
        "doi": "10.1103/revmodphys.74.47",
        "year": 2002,
        "journal": "Reviews of Modern Physics",
        "abstract": "Complex networks describe a wide range of systems in nature and society, much\nquoted examples including the cell, a network of chemicals linked by chemical\nreactions, or the Internet, a network of routers and computers connected by\nphysical links. While traditionally these systems were modeled as random\ngraphs, it is increasingly recognized that the topology and evolution of real\nnetworks is governed by robust organizing principles. Here we review the recent\nadvances in the field of complex networks, focusing on the statistical\nmechanics of network topology and dynamics. After reviewing the empirical data\nthat motivated the recent interest in networks, we discuss the main models and\nanalytical tools, covering random graphs, small-world and scale-free networks,\nas well as the interplay between topology and the network's robustness against\nfailures and attacks.Comment: 54 pages, submitted to Reviews of Modern Physic",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16007,
          "supporting": 231,
          "contradicting": 20,
          "mentioning": 15477,
          "unclassified": 279,
          "citingPublications": 19423
        }
      },
      {
        "title": "Scalable molecular dynamics with NAMD",
        "doi": "10.1002/jcc.20289",
        "year": 2005,
        "journal": "Journal of Computational Chemistry",
        "abstract": "AbstractNAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NAMD scales to hundreds of processors on high-end parallel platforms, as well as tens of processors on low-cost commodity clusters, and also runs on individual desktop and laptop computers. NAMD works with AMBER and CHARMM potential functions, parameters, and file formats. This paper, directed to novices as well as experts, first introduces concepts and methods used in the NAMD program, describing the classical molecular dynamics force field, equations of motion, and integration methods along with the efficient electrostatics evaluation algorithms employed and temperature and pressure controls used. Features for steering the simulation across barriers and for calculating both alchemical and conformational free energy differences are presented. The motivations for and a roadmap to the internal design of NAMD, implemented in C++ and based on Charm++ parallel objects, are outlined. The factors affecting the serial and parallel performance of a simulation are discussed. Next, typical NAMD use is illustrated with representative applications to a small, a medium, and a large biomolecular system, highlighting particular features of NAMD, e.g., the Tcl scripting language. Finally, the paper provides a list of the key features of NAMD and discusses the benefits of combining NAMD with the molecular graphics/sequence analysis software VMD and the grid computing/collaboratory software BioCoRE. NAMD is distributed free of charge with source code at www.ks.uiuc.edu.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 15009,
          "supporting": 35,
          "contradicting": 2,
          "mentioning": 14899,
          "unclassified": 73,
          "citingPublications": 16703
        }
      },
      {
        "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python",
        "doi": "10.1038/s41592-019-0686-2",
        "year": 2020,
        "journal": "Nature Chemical Biology",
        "abstract": "SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Has correction",
            "date": "2020-2-24",
            "noticeDoi": "10.1038/s41592-020-0772-5",
            "doi": "10.1038/s41592-019-0686-2",
            "urls": null
          },
          {
            "status": "Has erratum",
            "date": "2020-2-24",
            "noticeDoi": "10.1038/s41592-020-0772-5",
            "doi": "10.1038/s41592-019-0686-2",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 14943,
          "supporting": 24,
          "contradicting": 1,
          "mentioning": 14752,
          "unclassified": 166,
          "citingPublications": 30852
        }
      }
    ]
  },
  "Bollinger Bands": {
    "query": "Bollinger Bands",
    "count": 10,
    "papers": [
      {
        "title": "Dioxygen Activation at Mononuclear Nonheme Iron Active Sites:  Enzymes, Models, and Intermediates",
        "doi": "10.1021/cr020628n",
        "year": 2004,
        "journal": "Chemical Reviews",
        "abstract": "Abstract\nFor Abstract see ChemInform Abstract in Full Text.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2087,
          "supporting": 37,
          "contradicting": 1,
          "mentioning": 2012,
          "unclassified": 37,
          "citingPublications": 2375
        }
      },
      {
        "title": "Geometric and Electronic Structure/Function Correlations in Non-Heme Iron Enzymes",
        "doi": "10.1021/cr9900275",
        "year": 1999,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1980,
          "supporting": 63,
          "contradicting": 4,
          "mentioning": 1905,
          "unclassified": 8,
          "citingPublications": 1669
        }
      },
      {
        "title": "Optical atomic clocks",
        "doi": "10.1103/revmodphys.87.637",
        "year": 2015,
        "journal": "Reviews of Modern Physics",
        "abstract": "Optical atomic clocks represent the state of the art in the frontier of modern measurement science. In this article a detailed review on the development of optical atomic clocks that are based on trapped single ions and many neutral atoms is provided. Important technical ingredients for optical clocks are discussed and measurement precision and systematic uncertainty associated with some of the best clocks to date are presented. An outlook on the exciting prospect for clock applications is given in conclusion.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1662,
          "supporting": 15,
          "contradicting": 1,
          "mentioning": 1639,
          "unclassified": 7,
          "citingPublications": 2159
        }
      },
      {
        "title": "Quantum properties of atomic-sized conductors",
        "doi": "10.1016/s0370-1573(02)00633-6",
        "year": 2003,
        "journal": "Physics Reports",
        "abstract": "Using remarkably simple experimental techniques it is possible to gently\nbreak a metallic contact and thus form conducting nanowires. During the last\nstages of the pulling a neck-shaped wire connects the two electrodes, the\ndiameter of which is reduced to single atom upon further stretching. For some\nmetals it is even possible to form a chain of individual atoms in this fashion.\nAlthough the atomic structure of contacts can be quite complicated, as soon as\nthe weakest point is reduced to just a single atom the complexity is removed.\nThe properties of the contact are then dominantly determined by the nature of\nthis atom. This has allowed for quantitative comparison of theory and\nexperiment for many properties, and atomic contacts have proven to form a rich\ntest-bed for concepts from mesoscopic physics. Properties investigated include\nmultiple Andreev reflection, shot noise, conductance quantization, conductance\nfluctuations, and dynamical Coulomb blockade. In addition, pronounced quantum\neffects show up in the mechanical properties of the contacts, as seen in the\nforce and cohesion energy of the nanowires. We review this reseach, which has\nbeen performed mainly during the past decade, and we discuss the results in the\ncontext of related developments.Comment: Review, 120 pages, 98 figures. In view of the file size figures have\n  been compressed. A higher-resolution version can be found at:\n  http://lions1.leidenuniv.nl/wwwhome/ruitenbe/review/QPASC-hr-ps-v2.zip (5.6MB\n  zip PostScript",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1280,
          "supporting": 66,
          "contradicting": 1,
          "mentioning": 1202,
          "unclassified": 11,
          "citingPublications": 1475
        }
      },
      {
        "title": "Optimizing performance through intrinsic motivation and attention for learning: The OPTIMAL theory of motor learning",
        "doi": "10.3758/s13423-015-0999-9",
        "year": 2016,
        "journal": "Psychonomic Bulletin & Review",
        "abstract": "Abstract Effective motor performance is important for surviving and thriving, and skilled movement is critical in many activities. Much theorizing over the past few decades has focused on how certain practice conditions affect the processing of task-related information to affect learning. Yet, existing theoretical perspectives do not accommodate significant recent lines of evidence demonstrating motivational and attentional effects on performance and learning. These include research on (a) conditions that enhance expectancies for future performance, (b) variables that influence learners' autonomy, and (c) an external focus of attention on the intended movement effect. We propose the OPTIMAL (Optimizing Performance through Intrinsic Motivation and Attention for Learning) theory of motor learning. We suggest that motivational and attentional factors contribute to performance and learning by strengthening the coupling of goals to actions. We provide explanations for the performance and learning advantages of these variables on psychological and neuroscientific grounds. We describe a plausible mechanism for expectancy effects rooted in responses of dopamine to the anticipation of positive experience and temporally associated with skill practice. Learner autonomy acts perhaps largely through an enhanced expectancy pathway. Furthermore, we consider the influence of an external focus for the establishment of efficient functional connections across brain networks that subserve skilled movement. We speculate that enhanced expectancies and an external focus propel performers' cognitive and motor systems in productive \"forward\" directions and prevent \"backsliding\" into self-and non-task focused states. Expected success presumably breeds further success and helps consolidate memories. We discuss practical implications and future research directions.Keywords Motivation . Attentional focus . Self-efficacy . Positive affect . Dopamine . Motor performance Skilled movement is fundamental to surviving and thriving in the world and the basis as well for many of the highest human endeavors and cultural achievements, from sport to art to music. How people learn and relearn movement skills has been addressed from a number of disparate scientific perspectives and levels of analysis, including behavioral, social cognitive, neurophysiological, and neurocomputational. In part, differences in assumptions, scientific terminology and philosophy, as well as methodological approaches have made scholarly rapprochement challenging, but we see the end goal of optimizing motor learning as important. We suggest that it may be valuable to bring recent insights from various approaches together to identify a coherent way forward that can result in optimized learning from humans' earliest encounters with new motor skills to the lifelong development of motoric expertise.It is typically considered time to look afresh at dominant theories in a field when the accumulating evidence suggests that old frameworks cannot account for substantial new insights an...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1034,
          "supporting": 41,
          "contradicting": 18,
          "mentioning": 935,
          "unclassified": 40,
          "citingPublications": 943
        }
      },
      {
        "title": "The First Direct Characterization of a High-Valent Iron Intermediate in the Reaction of an α-Ketoglutarate-Dependent Dioxygenase:  A High-Spin Fe(IV) Complex in Taurine/α-Ketoglutarate Dioxygenase (TauD) from Escherichia coli",
        "doi": "10.1021/bi030011f",
        "year": 2003,
        "journal": "Biochemistry",
        "abstract": "The Fe(II)- and alpha-ketoglutarate(alphaKG)-dependent dioxygenases have roles in synthesis of collagen and sensing of oxygen in mammals, in acquisition of nutrients and synthesis of antibiotics in microbes, and in repair of alkylated DNA in both. A consensus mechanism for these enzymes, involving (i) addition of O(2) to a five-coordinate, (His)(2)(Asp)-facially coordinated Fe(II) center to which alphaKG is also bound via its C-1 carboxylate and ketone oxygen; (ii) attack of the uncoordinated oxygen of the bound O(2) on the ketone carbonyl of alphaKG to form a bicyclic Fe(IV)-peroxyhemiketal complex; (iii) decarboxylation of this complex concomitantly with formation of an oxo-ferryl (Fe(IV)=O(2)(-)) intermediate; and (iv) hydroxylation of the substrate by the Fe(IV)=O(2)(-) complex via a substrate radical intermediate, has repeatedly been proposed, but none of the postulated intermediates occurring after addition of O(2) has ever been detected. In this work, an oxidized Fe intermediate in the reaction of one of these enzymes, taurine/alpha-ketoglutarate dioxygenase (TauD) from Escherichia coli, has been directly demonstrated by rapid kinetic and spectroscopic methods. Characterization of the intermediate and its one-electron-reduced form (obtained by low-temperature gamma-radiolysis of the trapped intermediate) by Mössbauer and electron paramagnetic resonance spectroscopies establishes that it is a high-spin, formally Fe(IV) complex. Its Mössbauer isomer shift is, however, significantly greater than those of other known Fe(IV) complexes, suggesting that the iron ligands in the TauD intermediate confer significant Fe(III) character to the high-valent site by strong electron donation. The properties of the complex and previous results on related alphaKG-dependent dioxygenases and other non-heme-Fe(II)-dependent, O(2)-activating enzymes suggest that the TauD intermediate is most probably either the Fe(IV)-peroxyhemiketal complex or the taurine-hydroxylating Fe(IV)=O(2)(-) species. The detection of this intermediate sets the stage for a more detailed dissection of the TauD reaction mechanism than has previously been reported for any other member of this important enzyme family.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1004,
          "supporting": 61,
          "contradicting": 0,
          "mentioning": 941,
          "unclassified": 2,
          "citingPublications": 711
        }
      },
      {
        "title": "Non-Heme Fe(IV)–Oxo Intermediates",
        "doi": "10.1021/ar700066p",
        "year": 2007,
        "journal": "Accounts of Chemical Research",
        "abstract": "AbstractHigh-valent non-heme iron-oxo intermediates have been proposed for decades as the key intermediates in numerous biological oxidation reactions. In the last three years, the first direct characterization of such intermediates has been provided by studies of several αKG-dependent oxygenases that catalyze either hydroxylation or halogenation of their substrates. In each case, the Fe(IV)-oxo intermediate is implicated in cleavage of the aliphatic C-H bond to initiate hydroxylation or halogenation. The observation of non-heme Fe(IV)-oxo intermediates and Fe(II)-containing product(s) complexes with almost identical spectroscopic parameters in the reactions of two distantly related αKG-dependent hydroxylases suggests that members of this sub-family follow a conserved mechanism for substrate hydroxylation. In contrast, for the αKG-dependent non-heme-iron halogenase, CytC3, two distinct Fe(IV) complexes form and decay together, suggesting that they are in rapid equilibrium. The existence of two distinct conformers of the Fe site may be the key factor accounting for the divergence of the halogenase reaction from the more usual hydroxylation pathway after C-H cleavage. Distinct transformations catalyzed by other mononuclear non-heme enzymes are likely also to involve initial C-H-cleavage by Fe(IV)-oxo complexes, followed by diverging reactivities of the resulting Fe(III)-hydroxo/substrate radical intermediates.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 917,
          "supporting": 36,
          "contradicting": 0,
          "mentioning": 880,
          "unclassified": 1,
          "citingPublications": 943
        }
      },
      {
        "title": "Oxygen Activation and Radical Transformations in Heme Proteins and Metalloporphyrins",
        "doi": "10.1021/acs.chemrev.7b00373",
        "year": 2017,
        "journal": "Chemical Reviews",
        "abstract": "As\na result of the adaptation of life to an aerobic environment,\nnature has evolved a panoply of metalloproteins for oxidative metabolism\nand protection against reactive oxygen species. Despite the diverse\nstructures and functions of these proteins, they share common mechanistic\ngrounds. An open-shell transition metal like iron or copper is employed\nto interact with O2 and its derived intermediates such\nas hydrogen peroxide to afford a variety of metal–oxygen intermediates.\nThese reactive intermediates, including metal-superoxo, -(hydro)peroxo,\nand high-valent metal–oxo species, are the basis for the various\nbiological functions of O2-utilizing metalloproteins. Collectively,\nthese processes are called oxygen activation. Much of our understanding\nof the reactivity of these reactive intermediates has come from the\nstudy of heme-containing proteins and related metalloporphyrin compounds.\nThese studies not only have deepened our understanding of various\nfunctions of heme proteins, such as O2 storage and transport,\ndegradation of reactive oxygen species, redox signaling, and biological\noxygenation, etc., but also have driven the development of bioinorganic\nchemistry and biomimetic catalysis. In this review, we survey the\nrange of O2 activation processes mediated by heme proteins\nand model compounds with a focus on recent progress in the characterization\nand reactivity of important iron–oxygen intermediates. Representative\nreactions initiated by these reactive intermediates as well as some\ncontext from prior decades will also be presented. We will discuss\nthe fundamental mechanistic features of these transformations and\ndelineate the underlying structural and electronic factors that contribute\nto the spectrum of reactivities that has been observed in nature as\nwell as those that have been invented using these paradigms. Given\nthe recent developments in biocatalysis for non-natural chemistries\nand the renaissance of radical chemistry in organic synthesis, we\nenvision that new enzymatic and synthetic transformations will emerge\nbased on the radical processes mediated by metalloproteins and their\nsynthetic analogs.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 916,
          "supporting": 16,
          "contradicting": 2,
          "mentioning": 871,
          "unclassified": 27,
          "citingPublications": 893
        }
      },
      {
        "title": "Electron tunneling through proteins",
        "doi": "10.1017/s0033583503003913",
        "year": 2003,
        "journal": "Quarterly Reviews of Biophysics",
        "abstract": "Electron transfer processes are vital elements of energy transduction pathways in living cells. More than a half century of research has produced a remarkably detailed understanding of the factors that regulate these 'currents of life'. We review investigations of Ru-modified proteins that have delineated the distance- and driving-force dependences of intra-protein electron-transfer rates. We also discuss electron transfer across protein-protein interfaces that has been probed both in solution and in structurally characterized crystals. It is now clear that electrons tunnel between sites in biological redox chains, and that protein structures tune thermodynamic properties and electronic coupling interactions to facilitate these reactions. Our work has produced an experimentally validated timetable for electron tunneling across specified distances in proteins. Many electron tunneling rates in cytochrome c oxidase and photosynthetic reaction centers agree well with timetable predictions, indicating that the natural reactions are highly optimized, both in terms of thermodynamics and electronic coupling. The rates of some reactions, however, significantly exceed timetable predictions: it is likely that multistep tunneling is responsible for these anomalously rapid charge transfer events.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 869,
          "supporting": 37,
          "contradicting": 0,
          "mentioning": 823,
          "unclassified": 9,
          "citingPublications": 639
        }
      },
      {
        "title": "Mechanochemistry:  The Mechanical Activation of Covalent Bonds",
        "doi": "10.1021/cr030697h",
        "year": 2005,
        "journal": "Chemical Reviews",
        "abstract": "",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 841,
          "supporting": 15,
          "contradicting": 1,
          "mentioning": 813,
          "unclassified": 12,
          "citingPublications": 1221
        }
      }
    ]
  },
  "Markov Chains": {
    "query": "Markov Chains",
    "count": 10,
    "papers": [
      {
        "title": "Maximum Likelihood from Incomplete Data Via the EM Algorithm",
        "doi": "10.1111/j.2517-6161.1977.tb01600.x",
        "year": 1977,
        "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
        "abstract": "Read before the ROYAL STATISTICAL SOCIETY at a meeting organized by the RESEARCH SECTION on Wednesday, December 8th, 1976, Professor S. D. SILVEY in the Chair] SUMMARY A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23283,
          "supporting": 27,
          "contradicting": 2,
          "mentioning": 22884,
          "unclassified": 370,
          "citingPublications": 44904
        }
      },
      {
        "title": "MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space",
        "doi": "10.1093/sysbio/sys029",
        "year": 2012,
        "journal": "Systematic Biology",
        "abstract": "AbstractSince its introduction in 2001, MrBayes has grown in popularity as a software package for Bayesian phylogenetic inference using <strong class=\"highlight\">Markov</strong> <strong class=\"highlight\">chain</strong> Monte Carlo (MCMC) methods. With this note, we announce the release of version 3.2, a major upgrade to the latest official release presented in 2003. The new version provides convergence diagnostics and allows multiple analyses to be run in parallel with convergence progress monitored on the fly. The introduction of new proposals and automatic optimization of tuning parameters has improved convergence for many problems. The new version also sports significantly faster likelihood calculations through streaming single-instruction-multiple-data extensions (SSE) and support of the BEAGLE library, allowing likelihood calculations to be delegated to graphics processing units (GPUs) on compatible hardware. Speedup factors range from around 2 with SSE code to more than 50 with BEAGLE for codon problems. Checkpointing across all models allows long runs to be completed even when an analysis is prematurely terminated. New models include relaxed clocks, dating, model averaging across time-reversible substitution models, and support for hard, negative, and partial (backbone) tree constraints. Inference of species trees from gene trees is supported by full incorporation of the Bayesian estimation of species trees (BEST) algorithms. Marginal model likelihoods for Bayes factor tests can be estimated accurately across the entire model space using the stepping stone method. The new version provides more output options than previously, including samples of ancestral states, site rates, site dN/dS rations, branch rates, and node dates. A wide range of statistics on tree parameters can also be output for visualization in FigTree and compatible software.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 15517,
          "supporting": 30,
          "contradicting": 0,
          "mentioning": 15422,
          "unclassified": 65,
          "citingPublications": 24637
        }
      },
      {
        "title": "Mixed effects models and extensions in ecology with R",
        "doi": "10.1007/978-0-387-87458-6",
        "year": 2009,
        "journal": "",
        "abstract": "No sooner, it seems, had our first book Analysing Ecological Data gone to print, than we embarked on the writing of the nearly 600 page text you are now holding. This proved to be a labour of love of sorts-we felt that there were certain issues sufficiently common in the analysis of ecological data that merited more detailed description and analysis. Thus the present book can be seen as a 'sequel' to Analysing Ecological Data but with much greater emphasis on these very issues so commonly encountered in the collection of, and analysis of, ecological data. In particular, we look at different ways of analysing nested data, heterogeneity of variance, spatial and temporal correlation, and zero-inflated data. The original plan was to write a text of about 350 pages, but to do justice to the sheer range of problems and ideas we have well exceeded that original target (as you can see!). Such is the scope of applied statistics in ecology. In particular, partly on the back of reviewer's comments, we have included a chapter on Bayesian Monte-Carlo <strong class=\"highlight\">Markov</strong>-<strong class=\"highlight\">Chain</strong> applications in generalized linear modelling. We hope this serves as an informative introduction (but no more than an introduction!) to this interesting and increasingly relevant area of statistics. We received lots of positive feedback on the approach and style we used in Analysing Ecological Data, especially the combination of case studies and a theory section. We have therefore followed the same approach with this book. This time, however, we have provided the R code used for the analysis. Most of this R code is included in the text, but where the code was particularly long, it is only available from the book's website at www.highstat.com. In the case studies, we also included advice on what to write in a paper.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 13407,
          "supporting": 35,
          "contradicting": 2,
          "mentioning": 13238,
          "unclassified": 132,
          "citingPublications": 17055
        }
      },
      {
        "title": "Stellar population synthesis at the resolution of 2003",
        "doi": "10.1046/j.1365-8711.2003.06897.x",
        "year": 2003,
        "journal": "Monthly Notices of the Royal Astronomical Society",
        "abstract": "We present a new model for computing the spectral evolution of stellar\npopulations at ages between 100,000 yr and 20 Gyr at a resolution of 3 A across\nthe whole wavelength range from 3200 to 9500 A for a wide range of\nmetallicities. These predictions are based on a newly available library of\nobserved stellar spectra. We also compute the spectral evolution across a\nlarger wavelength range, from 91 A to 160 micron, at lower resolution. The\nmodel incorporates recent progress in stellar evolution theory and an\nobservationally motivated prescription for thermally-pulsing stars on the\nasymptotic giant branch. The latter is supported by observations of surface\nbrightness fluctuations in nearby stellar populations. We show that this model\nreproduces well the observed optical and near-infrared colour-magnitude\ndiagrams of Galactic star clusters of various ages and metallicities.\nStochastic fluctuations in the numbers of stars in different evolutionary\nphases can account for the full range of observed integrated colours of star\nclusters in the Magellanic Clouds. The model reproduces in detail typical\ngalaxy spectra from the Early Data Release (EDR) of the Sloan Digital Sky\nSurvey (SDSS). We exemplify how this type of spectral fit can constrain\nphysical parameters such as the star formation history, metallicity and dust\ncontent of galaxies. Our model is the first to enable accurate studies of\nabsorption-line strengths in galaxies containing stars over the full range of\nages. Using the highest-quality spectra of the SDSS EDR, we show that this\nmodel can reproduce simultaneously the observed strengths of those Lick indices\nthat do not depend strongly on element abundance ratios [abridged].Comment: 35 pages, 22 figures, to appear in MNRAS; version with full\n  resolution figures available at http://www.iap.fr/~charlot/bc2003/pape",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12837,
          "supporting": 255,
          "contradicting": 16,
          "mentioning": 12559,
          "unclassified": 7,
          "citingPublications": 10275
        }
      },
      {
        "title": "A Simple, Fast, and Accurate Algorithm to Estimate Large Phylogenies by Maximum Likelihood",
        "doi": "10.1080/10635150390235520",
        "year": 2003,
        "journal": "Systematic Biology",
        "abstract": "The increase in the number of large data sets and the complexity of current probabilistic sequence evolution models necessitates fast and reliable phylogeny reconstruction methods. We describe a new approach, based on the maximum-likelihood principle, which clearly satisfies these requirements. The core of this method is a simple hill-climbing algorithm that adjusts tree topology and branch lengths simultaneously. This algorithm starts from an initial tree built by a fast distance-based method and modifies this tree to improve its likelihood at each iteration. Due to this simultaneous adjustment of the topology and branch lengths, only a few iterations are sufficient to reach an optimum. We used extensive and realistic computer simulations to show that the topological accuracy of this new method is at least as high as that of the existing maximum-likelihood programs and much higher than the performance of distance-based and parsimony approaches. The reduction of computing time is dramatic in comparison with other maximum-likelihood packages, while the likelihood maximization ability tends to be higher. For example, only 12 min were required on a standard personal computer to analyze a data set consisting of 500 rbcL sequences with 1,428 base pairs from plant plastids, thus reaching a speed of the same order as some popular distance-based and parsimony algorithms. This new method is implemented in the PHYML program, which is freely available on our web page:",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 11613,
          "supporting": 23,
          "contradicting": 1,
          "mentioning": 11526,
          "unclassified": 63,
          "citingPublications": 16147
        }
      },
      {
        "title": "BEAST: Bayesian evolutionary analysis by sampling trees",
        "doi": "10.1186/1471-2148-7-214",
        "year": 2007,
        "journal": "BMC Evolutionary Biology",
        "abstract": "AbstractBackground: The evolutionary analysis of molecular sequence variation is a statistical enterprise. This is reflected in the increased use of probabilistic models for phylogenetic inference, multiple sequence alignment, and molecular population genetics. Here we present BEAST: a fast, flexible software architecture for Bayesian analysis of molecular sequences related by an evolutionary tree. A large number of popular stochastic models of sequence evolution are provided and tree-based models suitable for both within-and between-species sequence data are implemented.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 11154,
          "supporting": 37,
          "contradicting": 1,
          "mentioning": 11057,
          "unclassified": 59,
          "citingPublications": 12317
        }
      },
      {
        "title": "The Elements of Statistical Learning",
        "doi": "10.1007/978-0-387-21606-5",
        "year": 2001,
        "journal": "",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 10684,
          "supporting": 38,
          "contradicting": 2,
          "mentioning": 10469,
          "unclassified": 175,
          "citingPublications": 24472
        }
      },
      {
        "title": "<b>mice</b>: Multivariate Imputation by <strong class=\"highlight\">Chained</strong> Equations in<i>R</i>",
        "doi": "10.18637/jss.v045.i03",
        "year": 2011,
        "journal": "Journal of Statistical Software",
        "abstract": "AbstractThe R package mice imputes incomplete multivariate data by <strong class=\"highlight\">chained</strong> equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice 2.9, which extends the functionality of mice 1.0 in several ways. In mice 2.9, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice 2.9 adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice 2.9 can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 10513,
          "supporting": 23,
          "contradicting": 2,
          "mentioning": 10430,
          "unclassified": 58,
          "citingPublications": 13692
        }
      },
      {
        "title": "Bayesian Measures of Model Complexity and Fit",
        "doi": "10.1111/1467-9868.00353",
        "year": 2002,
        "journal": "Journal of the Royal Statistical Society Series B (Statistical Methodology)",
        "abstract": "Summary.We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure p D for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general p D approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding p D to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a <strong class=\"highlight\">Markov</strong> <strong class=\"highlight\">chain</strong> Monte Carlo analysis.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 9532,
          "supporting": 40,
          "contradicting": 4,
          "mentioning": 9385,
          "unclassified": 103,
          "citingPublications": 11856
        }
      },
      {
        "title": "Bayesian Phylogenetics with BEAUti and the BEAST 1.7",
        "doi": "10.1093/molbev/mss075",
        "year": 2012,
        "journal": "Molecular Biology and Evolution",
        "abstract": "Computational evolutionary biology, statistical phylogenetics and coalescent-based population genetics are becoming increasingly central to the analysis and understanding of molecular sequence data. We present the Bayesian Evolutionary Analysis by Sampling Trees (BEAST) software package version 1.7, which implements a family of <strong class=\"highlight\">Markov</strong> <strong class=\"highlight\">chain</strong> Monte Carlo (MCMC) algorithms for Bayesian phylogenetic inference, divergence time dating, coalescent analysis, phylogeography and related molecular evolutionary analyses. This package includes an enhanced graphical user interface program called Bayesian Evolutionary Analysis Utility (BEAUti) that enables access to advanced models for molecular sequence and phenotypic trait evolution that were previously available to developers only. The package also provides new tools for visualizing and summarizing multispecies coalescent and phylogeographic analyses. BEAUti and BEAST 1.7 are open source under the GNU lesser general public license and available at http://beast-mcmc.googlecode.com and http://beast.bio.ed.ac.uk",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 9109,
          "supporting": 18,
          "contradicting": 1,
          "mentioning": 9046,
          "unclassified": 44,
          "citingPublications": 9891
        }
      }
    ]
  },
  "Bell Curve": {
    "query": "Bell Curve",
    "count": 10,
    "papers": [
      {
        "title": "Maps of Dust Infrared Emission for Use in Estimation of Reddening and Cosmic Microwave Background Radiation Foregrounds",
        "doi": "10.1086/305772",
        "year": 1998,
        "journal": "The Astrophysical Journal",
        "abstract": "ABSTRACTWe present a full-sky 100 km map that is a reprocessed composite of the COBE/DIRBE and IRAS/ ISSA maps, with the zodiacal foreground and conÐrmed point sources removed. Before using the ISSA maps, we remove the remaining artifacts from the IRAS scan pattern. Using the DIRBE 100 and 240 km data, we have constructed a map of the dust temperature so that the 100 km map may be converted to a map proportional to dust column density. The dust temperature varies from 17 to 21 K, which is modest but does modify the estimate of the dust column by a factor of 5. The result of these manipulations is a map with DIRBE quality calibration and IRAS resolution. A wealth of Ðlamentary detail is apparent on many di †erent scales at all Galactic latitudes. In high-latitude regions, the dust map correlates well with maps of H I emission, but deviations are coherent in the sky and are especially conspicuous in regions of saturation of H I emission toward denser clouds and of formation of in molecular clouds. In contrast, H 2 high-velocity H I clouds are deÐcient in dust emission, as expected.To generate the full-sky dust maps, we must Ðrst remove zodiacal light contamination, as well as a possible cosmic infrared background (CIB). This is done via a regression analysis of the 100 km DIRBE map against the Leiden-Dwingeloo map of H I emission, with corrections for the zodiacal light via a suitable expansion of the DIRBE 25 km Ñux. This procedure removes virtually all traces of the zodiacal foreground. For the 100 km map no signiÐcant CIB is detected. At longer wavelengths, where the zodiacal contamination is weaker, we detect the CIB at surprisingly high Ñux levels of 32^13 nW m~2 sr~1 at 140 km and of 17^4 nW m~2 sr~1 at 240 km (95% conÐdence). This integrated Ñux D2 times that extrapolated from optical galaxies in the Hubble Deep Field.The primary use of these maps is likely to be as a new estimator of Galactic extinction. To calibrate our maps, we assume a standard reddening law and use the colors of elliptical galaxies to measure the reddening per unit Ñux density of 100 km emission. We Ðnd consistent calibration using the B[R color distribution of a sample of the 106 brightest cluster ellipticals, as well as a sample of 384 ellipticals with B[V and Mg line strength measurements. For the latter sample, we use the correlation of intrinsic B[V versus index to tighten the power of the test greatly. We demonstrate that the new maps are Mg 2 twice as accurate as the older Burstein-Heiles reddening estimates in regions of low and moderate reddening. The maps are expected to be signiÐcantly more accurate in regions of high reddening. These dust maps will also be useful for estimating millimeter emission that contaminates cosmic microwave background radiation experiments and for estimating soft X-ray absorption. We describe how to access our maps readily for general use.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 17520,
          "supporting": 592,
          "contradicting": 71,
          "mentioning": 16836,
          "unclassified": 21,
          "citingPublications": 14540
        }
      },
      {
        "title": "Maximum entropy modeling of species geographic distributions",
        "doi": "10.1016/j.ecolmodel.2005.03.026",
        "year": 2006,
        "journal": "Ecological Modelling",
        "abstract": "The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of the method, here we perform a continental-scale case study using two Neotropical mammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus. We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC <strong class=\"highlight\">curve</strong> (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16098,
          "supporting": 39,
          "contradicting": 2,
          "mentioning": 15346,
          "unclassified": 711,
          "citingPublications": 15801
        }
      },
      {
        "title": "Stellar population synthesis at the resolution of 2003",
        "doi": "10.1046/j.1365-8711.2003.06897.x",
        "year": 2003,
        "journal": "Monthly Notices of the Royal Astronomical Society",
        "abstract": "We present a new model for computing the spectral evolution of stellar\npopulations at ages between 100,000 yr and 20 Gyr at a resolution of 3 A across\nthe whole wavelength range from 3200 to 9500 A for a wide range of\nmetallicities. These predictions are based on a newly available library of\nobserved stellar spectra. We also compute the spectral evolution across a\nlarger wavelength range, from 91 A to 160 micron, at lower resolution. The\nmodel incorporates recent progress in stellar evolution theory and an\nobservationally motivated prescription for thermally-pulsing stars on the\nasymptotic giant branch. The latter is supported by observations of surface\nbrightness fluctuations in nearby stellar populations. We show that this model\nreproduces well the observed optical and near-infrared colour-magnitude\ndiagrams of Galactic star clusters of various ages and metallicities.\nStochastic fluctuations in the numbers of stars in different evolutionary\nphases can account for the full range of observed integrated colours of star\nclusters in the Magellanic Clouds. The model reproduces in detail typical\ngalaxy spectra from the Early Data Release (EDR) of the Sloan Digital Sky\nSurvey (SDSS). We exemplify how this type of spectral fit can constrain\nphysical parameters such as the star formation history, metallicity and dust\ncontent of galaxies. Our model is the first to enable accurate studies of\nabsorption-line strengths in galaxies containing stars over the full range of\nages. Using the highest-quality spectra of the SDSS EDR, we show that this\nmodel can reproduce simultaneously the observed strengths of those Lick indices\nthat do not depend strongly on element abundance ratios [abridged].Comment: 35 pages, 22 figures, to appear in MNRAS; version with full\n  resolution figures available at http://www.iap.fr/~charlot/bc2003/pape",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12837,
          "supporting": 255,
          "contradicting": 16,
          "mentioning": 12559,
          "unclassified": 7,
          "citingPublications": 10275
        }
      },
      {
        "title": "The ERA5 global reanalysis",
        "doi": "10.1002/qj.3803",
        "year": 2020,
        "journal": "Quarterly Journal of the Royal Meteorological Society",
        "abstract": "Within the Copernicus Climate Change Service (C3S), ECMWF is producing the ERA5 reanalysis which, once completed, will embody a detailed record of the global atmosphere, land surface and ocean waves from 1950 onwards. This new reanalysis replaces the ERA-Interim reanalysis (spanning 1979 onwards) which was started in 2006. ERA5 is based on the Integrated Forecasting System (IFS) Cy41r2 which was operational in 2016. ERA5 thus benefits from a decade of developments in model physics, core dynamics and data assimilation. In addition to a significantly enhanced horizontal resolution of 31 km, compared to 80 km for ERA-Interim, ERA5 has hourly output throughout, and an uncertainty estimate from an ensemble (3-hourly at half the horizontal resolution). This paper describes the general setup of ERA5, as well as a basic evaluation of characteristics and performance, with a focus on the dataset from 1979 onwards which is currently publicly available. Re-forecasts from ERA5 analyses show a gain of up to one day in skill with respect to ERA-Interim. Comparison with radiosonde and PILOT data prior to assimilation shows an improved fit for temperature, wind and humidity in the troposphere, but not the stratosphere. A comparison with independent buoy data shows a much improved fit for ocean wave height. The uncertainty estimate reflects the evolution of the observing systems used in ERA5. The enhanced temporal and spatial resolution allows for a detailed evolution of weather systems. For precipitation, global-mean correlation with monthly-mean GPCP data is increased from 67% This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12144,
          "supporting": 117,
          "contradicting": 6,
          "mentioning": 11971,
          "unclassified": 50,
          "citingPublications": 23614
        }
      },
      {
        "title": "The Chemical Composition of the Sun",
        "doi": "10.1146/annurev.astro.46.060407.145222",
        "year": 2009,
        "journal": "Annual Review of Astronomy and Astrophysics",
        "abstract": "The solar chemical composition is an important ingredient in our understanding of the formation, structure and evolution of both the Sun and our solar system. Furthermore, it is an essential reference standard against which the elemental contents of other astronomical objects are compared. In this review we evaluate the current understanding of the solar photospheric composition. In particular, we present a redetermination of the abundances of nearly all available elements, using a realistic new 3-dimensional (3D), time-dependent hydrodynamical model of the solar atmosphere. We have carefully considered the atomic input data and selection of spectral lines, and accounted for departures from LTE whenever possible. The end result is a comprehensive and homogeneous compilation of the solar elemental abundances. Particularly noteworthy findings are significantly lower abundances of carbon, nitrogen, oxygen and neon compared with the widely-used values of a decade ago. The new solar chemical composition is supported by a high degree of internal consistency between available abundance indicators, and by agreement with values obtained in the solar neighborhood and from the most pristine meteorites. There is, however, a stark conflict with standard models of the solar interior according to helioseismology, a discrepancy that has yet to find a satisfactory resolution.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 10121,
          "supporting": 503,
          "contradicting": 49,
          "mentioning": 9549,
          "unclassified": 20,
          "citingPublications": 9144
        }
      },
      {
        "title": "A Universal Density Profile from Hierarchical Clustering",
        "doi": "10.1086/304888",
        "year": 1997,
        "journal": "The Astrophysical Journal",
        "abstract": "We use high-resolution N-body simulations to study the equilibrium density\nprofiles of dark matter halos in hierarchically clustering universes. We find\nthat all such profiles have the same shape, independent of halo mass, of\ninitial density fluctuation spectrum, and of the values of the cosmological\nparameters. Spherically averaged equilibrium profiles are well fit over two\ndecades in radius by a simple formula originally proposed to describe the\nstructure of galaxy clusters in a cold dark matter universe. In any particular\ncosmology the two scale parameters of the fit, the halo mass and its\ncharacteristic density, are strongly correlated. Low-mass halos are\nsignificantly denser than more massive systems, a correlation which reflects\nthe higher collapse redshift of small halos. The characteristic density of an\nequilibrium halo is proportional to the density of the universe at the time it\nwas assembled. A suitable definition of this assembly time allows the same\nproportionality constant to be used for all the cosmologies that we have\ntested. We compare our results to previous work on halo density profiles and\nshow that there is good agreement. We also provide a step-by-step analytic\nprocedure, based on the Press-Schechter formalism, which allows accurate\nequilibrium profiles to be calculated as a function of mass in any hierarchical\nmodel.Comment: typos in eqs A5 and A14 corrected. In press in the ApJ, Dec 1, 1997.\n  Full PS file with figures can be obtained from\n  http://penedes.as.arizona.edu/~jfn/preprints/dprof.ps.g",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 9518,
          "supporting": 253,
          "contradicting": 24,
          "mentioning": 9219,
          "unclassified": 22,
          "citingPublications": 9947
        }
      },
      {
        "title": "Adolescence-limited and life-course-persistent antisocial behavior: A developmental taxonomy.",
        "doi": "10.1037/0033-295x.100.4.674",
        "year": 1993,
        "journal": "Psychological Review",
        "abstract": "A dual taxonomy is presented to reconcile 2 incongruous facts about antisocial behavior: (a) It shows impressive continuity over age, but (b) its prevalence changes dramatically over age, increasing almost 10-fold temporarily during adolescence. This article suggests that delinquency conceals 2 distinct categories of individuals, each with a unique natural history and etiology: A small group engages in antisocial behavior of 1 sort or another at every life stage, whereas a larger group is antisocial only during adolescence. According to the theory of life-course-persistent antisocial behavior, children's neuropsychological problems interact cumulatively with their criminogenic environments across development, culminating in a pathological personality. According to the theory of adolescence-limited antisocial behavior, a contemporary maturity gap encourages teens to mimic antisocial behavior in ways that are normative and adjustive.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 8073,
          "supporting": 570,
          "contradicting": 49,
          "mentioning": 7126,
          "unclassified": 328,
          "citingPublications": 8702
        }
      },
      {
        "title": "A comparative risk assessment of burden of disease and injury attributable to 67 risk factors and risk factor clusters in 21 regions, 1990–2010: a systematic analysis for the Global Burden of Disease Study 2010",
        "doi": "10.1016/s0140-6736(12)61766-8",
        "year": 2012,
        "journal": "The Lancet",
        "abstract": "Summary\n\nBackground\nQuantification of the disease burden caused by different risks informs prevention by providing an account of health loss different to that provided by a disease-by-disease analysis. No complete revision of global disease burden caused by risk factors has been done since a comparative risk assessment in 2000, and no previous analysis has assessed changes in burden attributable to risk factors over time.\n\n\nMethods\nWe estimated deaths and disability-adjusted life years (DALYs; sum of years lived with disability [YLD] and years of life lost [YLL]) attributable to the independent effects of 67 risk factors and clusters of risk factors for 21 regions in 1990 and 2010. We estimated exposure distributions for each year, region, sex, and age group, and relative risks per unit of exposure by systematically reviewing and synthesising published and unpublished data. We used these estimates, together with estimates of cause-specific deaths and DALYs from the Global Burden of Disease Study 2010, to calculate the burden attributable to each risk factor exposure compared with the theoretical-minimum-risk exposure. We incorporated uncertainty in disease burden, relative risks, and exposures into our estimates of attributable burden.\n\n\nFindings\nIn 2010, the three leading risk factors for global disease burden were high blood pressure (7·0% [95% uncertainty interval 6·2–7·7] of global DALYs), tobacco smoking including second-hand smoke (6·3% [5·5–7·0]), and alcohol use (5·5% [5·0–5·9]). In 1990, the leading risks were childhood underweight (7·9% [6·8–9·4]), household air pollution from solid fuels (HAP; 7·0% [5·6–8·3]), and tobacco smoking including second-hand smoke (6·1% [5·4–6·8]). Dietary risk factors and physical inactivity collectively accounted for 10·0% (95% UI 9·2–10·8) of global DALYs in 2010, with the most prominent dietary risks being diets low in fruits and those high in sodium. Several risks that primarily affect childhood communicable diseases, including unimproved water and sanitation and childhood micronutrient deficiencies, fell in rank between 1990 and 2010, with unimproved water we and sanitation accounting for 0·9% (0·4–1·6) of global DALYs in 2010. However, in most of sub-Saharan Africa childhood underweight, HAP, and non-exclusive and discontinued breastfeeding were the leading risks in 2010, while HAP was the leading risk in south Asia. The leading risk factor in Eastern Europe, most of Latin America, and southern sub-Saharan Africa in 2010 was alcohol use; in most of Asia, North Africa and Middle East, and central Europe it was high blood pressure. Despite declines, tobacco smoking including second-hand smoke remained the leading risk in high-income north America and western Europe. High body-mass index has increased globally and it is the leading risk in Australasia and southern Latin America, and also ranks high in other high-income regions, North Africa and Middle East, and Oceania.\n\n\nInterpretation\nWorldwide, the contribution of different risk factors to disease burden has changed s...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Has erratum",
            "date": "2013-1-8",
            "noticeDoi": "10.1038/nrcardio.2012.194",
            "doi": "10.1016/s0140-6736(12)61766-8",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2013-1-8",
            "noticeDoi": "10.1038/nrcardio.2012.194",
            "doi": "10.1016/s0140-6736(12)61766-8",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2013-3",
            "noticeDoi": "10.1016/s0140-6736(13)60705-9",
            "doi": "10.1016/s0140-6736(12)61766-8",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2013-3",
            "noticeDoi": "10.1016/s0140-6736(13)60706-0",
            "doi": "10.1016/s0140-6736(12)61766-8",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2013-9",
            "noticeDoi": "10.1016/s0140-6736(13)62014-0",
            "doi": "10.1016/s0140-6736(12)61766-8",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2015-4-30",
            "noticeDoi": "10.1111/add.12924",
            "doi": "10.1016/s0140-6736(12)61766-8",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 8025,
          "supporting": 98,
          "contradicting": 9,
          "mentioning": 7641,
          "unclassified": 277,
          "citingPublications": 11340
        }
      },
      {
        "title": "ANFIS: adaptive-network-based fuzzy inference system",
        "doi": "10.1109/21.256541",
        "year": 1993,
        "journal": "Ieee Transactions on Systems Man and Cybernetics",
        "abstract": "This paper presents the architecture and learning procedure underlying ANFIS (Adaptive-Network-based Fuzzy Inference System), a fuzzy inference system implemented in the framework of adaptive networks. By using a hybrid learning procedure, the proposed ANFIS can construct an input-output mapping based on both human knowledge (in the form of fuzzy if-then rules) and stipulated input-output data pairs. In our simulation, we employ the ANFIS architecture to model nonlinear functions, identify nonlinear components on-linely in a control system, and predict a chaotic time series, all yielding remarkable results. Comparisons with artificail neural networks and earlier work on fuzzy modeling are listed and discussed. Other extensions of the proposed ANFIS and promising applications to automatic control and signal processing are also suggested.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 7726,
          "supporting": 4,
          "contradicting": 1,
          "mentioning": 7384,
          "unclassified": 337,
          "citingPublications": 14975
        }
      },
      {
        "title": "Novel methods improve prediction of species’ distributions from occurrence data",
        "doi": "10.1111/j.2006.0906-7590.04596.x",
        "year": 2006,
        "journal": "Ecography",
        "abstract": ". Novel methods improve prediction of species' distributions from occurrence data. Á/ Ecography 29: 129 Á/151.Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.\nJ. Elith",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 7488,
          "supporting": 121,
          "contradicting": 14,
          "mentioning": 7137,
          "unclassified": 216,
          "citingPublications": 8282
        }
      }
    ]
  },
  "SIR": {
    "query": "SIR",
    "count": 10,
    "papers": [
      {
        "title": "Fiji: an open-source platform for biological-image analysis",
        "doi": "10.1038/nmeth.2019",
        "year": 2012,
        "journal": "Nature Chemical Biology",
        "abstract": "Fiji is a distribution of the popular Open Source software ImageJ focused on biological image analysis. Fiji uses modern software engineering practices to combine powerful software libraries with a broad range of scripting languages to enable rapid prototyping of image processing algorithms. Fiji facilitates the transformation of novel algorithms into ImageJ plugins that can be shared with end users through an integrated update system. We propose Fiji as a platform for productive collaboration between computer science and biology research communities.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 50316,
          "supporting": 86,
          "contradicting": 5,
          "mentioning": 50104,
          "unclassified": 121,
          "citingPublications": 60999
        }
      },
      {
        "title": "A short history ofSHELX",
        "doi": "10.1107/s0108767307043930",
        "year": 2007,
        "journal": "Acta Crystallographica Section a Foundations of Crystallography",
        "abstract": "An account is given of the development of the SHELX system of computer programs from SHELX-76 to the present day. In addition to identifying useful innovations that have come into general use through their implementation in SHELX, a critical analysis is presented of the less-successful features, missed opportunities and desirable improvements for future releases of the software. An attempt is made to understand how a program originally designed for photographic intensity data, punched cards and computers over 10 000 times slower than an average modern personal computer has managed to survive for so long. SHELXL is the most widely used program for small-molecule refinement and SHELXS and SHELXD are often employed for structure solution despite the availability of objectively superior programs. SHELXL also finds a niche for the refinement of macromolecules against high-resolution or twinned data; SHELXPRO acts as an interface for macromolecular applications. SHELXC, SHELXD and SHELXE are proving useful for the experimental phasing of macromolecules, especially because they are fast and robust and so are often employed in pipelines for high-throughput phasing. This paper could serve as a general literature citation when one or more of the open-source SHELX programs (and the Bruker AXS version SHELXTL) are employed in the course of a crystal-structure determination.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 41114,
          "supporting": 144,
          "contradicting": 2,
          "mentioning": 40783,
          "unclassified": 185,
          "citingPublications": 86469
        }
      },
      {
        "title": "A pneumonia outbreak associated with a new coronavirus of probable bat origin",
        "doi": "10.1038/s41586-020-2012-7",
        "year": 2020,
        "journal": "Nature",
        "abstract": "Article Extended Data Fig. 7 | Analysis of 2019-nCoV receptor usage. Determination of virus infectivity in HeLa cells with or without the expression of human APN and DPP4. The expression of ACE2, APN and DPP4 plasmids with S tag were detected using mouse anti-S tag monoclonal antibody. ACE2, APN and DPP4 proteins (green), viral protein (red) and nuclei (blue) are shown. Scale bars, 10 μm.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Addendum",
            "date": "2020-11-17",
            "noticeDoi": "10.1038/s41586-020-2951-z",
            "doi": "10.1038/s41586-020-2012-7",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23058,
          "supporting": 260,
          "contradicting": 15,
          "mentioning": 21997,
          "unclassified": 786,
          "citingPublications": 22452
        }
      },
      {
        "title": "The Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3)",
        "doi": "10.1001/jama.2016.0287",
        "year": 2016,
        "journal": "Jama",
        "abstract": "and septic shock were last revised in 2001. Considerable advances have since been made into the pathobiology (changes in organ function, morphology, cell biology, biochemistry, immunology, and circulation), management, and epidemiology of sepsis, suggesting the need for reexamination.OBJECTIVE To evaluate and, as needed, update definitions for sepsis and septic shock.PROCESS A task force (n = 19) with expertise in sepsis pathobiology, clinical trials, and epidemiology was convened by the Society of Critical Care Medicine and the European Society of Intensive Care Medicine. Definitions and clinical criteria were generated through meetings, Delphi processes, analysis of electronic health record databases, and voting, followed by circulation to international professional societies, requesting peer review and endorsement (by 31 societies listed in the Acknowledgment).\nKEY FINDINGS FROM EVIDENCE SYNTHESISLimitations of previous definitions included an excessive focus on inflammation, the misleading model that sepsis follows a continuum through severe sepsis to shock, and inadequate specificity and sensitivity of the systemic inflammatory response syndrome (<strong class=\"highlight\">SIRS</strong>) criteria. Multiple definitions and terminologies are currently in use for sepsis, septic shock, and organ dysfunction, leading to discrepancies in reported incidence and observed mortality. The task force concluded the term severe sepsis was redundant.RECOMMENDATIONS Sepsis should be defined as life-threatening organ dysfunction caused by a dysregulated host response to infection. For clinical operationalization, organ dysfunction can be represented by an increase in the Sequential [Sepsis-related] Organ Failure Assessment (SOFA) score of 2 points or more, which is associated with an in-hospital mortality greater than 10%. Septic shock should be defined as a subset of sepsis in which particularly profound circulatory, cellular, and metabolic abnormalities are associated with a greater risk of mortality than with sepsis alone. Patients with septic shock can be clinically identified by a vasopressor requirement to maintain a mean arterial pressure of 65 mm Hg or greater and serum lactate level greater than 2 mmol/L (>18 mg/dL) in the absence of hypovolemia. This combination is associated with hospital mortality rates greater than 40%. In out-of-hospital, emergency department, or general hospital ward settings, adult patients with suspected infection can be rapidly identified as being more likely to have poor outcomes typical of sepsis if they have at least 2 of the following clinical criteria that together constitute a new bedside clinical score termed quickSOFA (qSOFA): respiratory rate of 22/min or greater, altered mentation, or systolic blood pressure of 100 mm Hg or less.CONCLUSIONS AND RELEVANCE These updated definitions and clinical criteria should replace previous definitions, offer greater consistency for epidemiologic studies and clinical trials, and facilitate earlier recognition and more timely management of patients with sepsis or at risk of developi...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 22332,
          "supporting": 187,
          "contradicting": 22,
          "mentioning": 20960,
          "unclassified": 1163,
          "citingPublications": 22751
        }
      },
      {
        "title": "The Hallmarks of Aging",
        "doi": "10.1016/j.cell.2013.05.039",
        "year": 2013,
        "journal": "Cell",
        "abstract": "Aging is characterized by a progressive loss of physiological integrity, leading to impaired function and increased vulnerability to death. This deterioration is the primary risk factor for major human pathologies, including cancer, diabetes, cardiovascular disorders, and neurodegenerative diseases. Aging research has experienced an unprecedented advance over recent years, particularly with the discovery that the rate of aging is controlled, at least to some extent, by genetic pathways and biochemical processes conserved in evolution. This Review enumerates nine tentative hallmarks that represent common denominators of aging in different organisms, with special emphasis on mammalian aging. These hallmarks are: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication. A major challenge is to dissect the interconnectedness between the candidate hallmarks and their relative contributions to aging, with the final goal of identifying pharmaceutical targets to improve human health during aging, with minimal side effects.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12934,
          "supporting": 178,
          "contradicting": 13,
          "mentioning": 12532,
          "unclassified": 211,
          "citingPublications": 13279
        }
      },
      {
        "title": "The Structure and Function of Complex Networks",
        "doi": "10.1137/s003614450342480",
        "year": 2003,
        "journal": "Siam Review",
        "abstract": "Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12904,
          "supporting": 148,
          "contradicting": 11,
          "mentioning": 12318,
          "unclassified": 427,
          "citingPublications": 17092
        }
      },
      {
        "title": "The Elements of Statistical Learning",
        "doi": "10.1007/978-0-387-21606-5",
        "year": 2001,
        "journal": "",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 10684,
          "supporting": 38,
          "contradicting": 2,
          "mentioning": 10469,
          "unclassified": 175,
          "citingPublications": 24472
        }
      },
      {
        "title": "Overview of theCCP4 suite and current developments",
        "doi": "10.1107/s0907444910045749",
        "year": 2011,
        "journal": "Acta Crystallographica Section D Biological Crystallography",
        "abstract": "The CCP4 (Collaborative Computational Project, Number 4) software suite is a collection of programs and associated data and software libraries which can be used for macromolecular structure determination by X-ray crystallography. The suite is designed to be flexible, allowing users a number of methods of achieving their aims. The programs are from a wide variety of sources but are connected by a common infrastructure provided by standard file formats, data objects and graphical interfaces. Structure solution by macromolecular crystallography is becoming increasingly automated and the CCP4 suite includes several automation pipelines. After giving a brief description of the evolution of CCP4 over the last 30 years, an overview of the current suite is given. While detailed descriptions are given in the accompanying articles, here it is shown how the individual programs contribute to a complete software package.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 10076,
          "supporting": 15,
          "contradicting": 0,
          "mentioning": 10035,
          "unclassified": 26,
          "citingPublications": 12280
        }
      },
      {
        "title": "Chromatin Modifications and Their Function",
        "doi": "10.1016/j.cell.2007.02.005",
        "year": 2007,
        "journal": "Cell",
        "abstract": "The surface of nucleosomes is studded with a multiplicity of modifications. At least eight different classes have been characterized to date and many different sites have been identified for each class. Operationally, modifications function either by disrupting chromatin contacts or by affecting the recruitment of nonhistone proteins to chromatin. Their presence on histones can dictate the higher-order chromatin structure in which DNA is packaged and can orchestrate the ordered recruitment of enzyme complexes to manipulate DNA. In this way, histone modifications have the potential to influence many fundamental biological processes, some of which may be epigenetically inherited.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 9377,
          "supporting": 101,
          "contradicting": 8,
          "mentioning": 9094,
          "unclassified": 174,
          "citingPublications": 9972
        }
      },
      {
        "title": "Development of a new resilience scale: The Connor-Davidson Resilience Scale (CD-RISC)",
        "doi": "10.1002/da.10113",
        "year": 2003,
        "journal": "Depression and Anxiety",
        "abstract": "Resilience may be viewed as a measure of stress coping ability and, as such, could be an important target of treatment in anxiety, depression, and stress reactions. We describe a new rating scale to assess resilience. The Connor-Davidson Resilience scale (CD-RISC) comprises of 25 items, each rated on a 5-point scale (0-4), with higher scores reflecting greater resilience. The scale was administered to subjects in the following groups: community sample, primary care outpatients, general psychiatric outpatients, clinical trial of generalized anxiety disorder, and two clinical trials of PTSD. The reliability, validity, and factor analytic structure of the scale were evaluated, and reference scores for study samples were calculated. Sensitivity to treatment effects was examined in subjects from the PTSD clinical trials. The scale demonstrated good psychometric properties and factor analysis yielded five factors. A repeated measures ANOVA showed that an increase in CD-RISC score was associated with greater improvement during treatment. Improvement in CD-RISC score was noted in proportion to overall clinical global improvement, with greatest increase noted in subjects with the highest global improvement and deterioration in CD-RISC score in those with minimal or no global improvement. The CD-RISC has sound psychometric properties and distinguishes between those with greater and lesser resilience. The scale demonstrates that resilience is modifiable and can improve with treatment, with greater improvement corresponding to higher levels of global improvement.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 8428,
          "supporting": 179,
          "contradicting": 56,
          "mentioning": 7727,
          "unclassified": 466,
          "citingPublications": 9411
        }
      }
    ]
  },
  "Logistic/Population": {
    "query": "Logistic/Population",
    "count": 10,
    "papers": [
      {
        "title": "Deep Residual Learning for Image Recognition",
        "doi": "10.1109/cvpr.2016.90",
        "year": 2016,
        "journal": "",
        "abstract": "AbstractDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 120349,
          "supporting": 309,
          "contradicting": 22,
          "mentioning": 119363,
          "unclassified": 655,
          "citingPublications": 206992
        }
      },
      {
        "title": "Using thematic analysis in psychology",
        "doi": "10.1191/1478088706qp063oa",
        "year": 2006,
        "journal": "Qualitative Research in Psychology",
        "abstract": "Background: Several chemical pollutants can accumulate in closed spaces of transportation and storage of non-dangerous goods, leading to high concentrations. Pollutants are mainly residues of pesticides used to prevent the spread of harmful organisms in intercontinental transportation, volatile organic compounds and components of diesel engine exhaust. The objectives of this cross-sectional qualitative study were to identify and evaluate the regulations in connection with the occupational chemical exposures caused by chemical pollutants in closed environments of transportation and storage, and to survey the knowledge, attitudes and practices (KAP) of occupational health and safety professionals and <strong class=\"highlight\">logistics</strong> managers related to this global workplace issue. Methods: A comprehensive systematic search of legal instruments was carried out in international, European Union and Hungarian legislation databases. Legal documents relevant for occupational chemical exposure at workplaces were included in the study and the legal relationships between selected documents were mapped. The systematic search of legal instruments identi ed 4737 records, of which 16 were included in the in-depth content analysis. According to the second objective, semi-structured face-to-face interviews were carried out with occupational health and safety professionals and warehouse managers at <strong class=\"highlight\">logistics</strong> companies located in Hungary. Results: The analysis of the documents highlighted the lack of explicit regulation on prevention from exposures caused by chemical pollutants in closed environments of transportation and storage of nonhazardous materials. The 21 completed interviews revealed that the professionals had very limited knowledge about the potential presence of chemical residues in closed spaces of transportation and storage. They deemed such chemical exposure rare and the related health effects negligible. Although legislation requires the risk assessment of workplaces, the assessment is misleading if potential hazards are not identi ed. Conclusions: The increasing risk of rapid global spread of harmful organisms due to climate change and dense international tra c results in growing need for fumigation, which, together with increasing temperature and transportation time, generates an emerging occupational and public health challenge. The revealed limitations point out that the chemical safety of transportation and storage of nondangerous goods should be more speci cally regulated and responsible professionals should be better informed about such workplace hazards.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 107327,
          "supporting": 187,
          "contradicting": 5,
          "mentioning": 104576,
          "unclassified": 2559,
          "citingPublications": 141314
        }
      },
      {
        "title": "Global Cancer Statistics 2020: GLOBOCAN Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries",
        "doi": "10.3322/caac.21660",
        "year": 2021,
        "journal": "Ca a Cancer Journal for Clinicians",
        "abstract": "This article provides an update on the global cancer burden using the GLOBOCAN 2020 estimates of cancer incidence and mortality produced by the International Agency for Research on Cancer. Worldwide, an estimated 19.3 million new cancer cases (18.1 million excluding nonmelanoma skin cancer) and almost 10.0 million cancer deaths (9.9 million excluding nonmelanoma skin cancer) occurred in 2020. Female breast cancer has surpassed lung cancer as the most commonly diagnosed cancer, with an estimated 2.3 million new cases (11.7%), followed by lung (11.4%), colorectal (10.0 %), prostate (7.3%), and stomach (5.6%) cancers. Lung cancer remained the leading cause of cancer death, with an estimated 1.8 million deaths (18%), followed by colorectal (9.4%), liver (8.3%), stomach (7.7%), and female breast (6.9%) cancers. Overall incidence was from 2‐fold to 3‐fold higher in transitioned versus transitioning countries for both sexes, whereas mortality varied &lt;2‐fold for men and little for women. Death rates for female breast and cervical cancers, however, were considerably higher in transitioning versus transitioned countries (15.0 vs 12.8 per 100,000 and 12.4 vs 5.2 per 100,000, respectively). The global cancer burden is expected to be 28.4 million cases in 2040, a 47% rise from 2020, with a larger increase in transitioning (64% to 95%) versus transitioned (32% to 56%) countries due to demographic changes, although this may be further exacerbated by increasing risk factors associated with globalization and a growing economy. Efforts to build a sustainable infrastructure for the dissemination of cancer prevention measures and provision of cancer care in transitioning countries is critical for global cancer control.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 68019,
          "supporting": 246,
          "contradicting": 32,
          "mentioning": 66379,
          "unclassified": 1362,
          "citingPublications": 91700
        }
      },
      {
        "title": "Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries",
        "doi": "10.3322/caac.21492",
        "year": 2018,
        "journal": "Ca a Cancer Journal for Clinicians",
        "abstract": "This article provides a status report on the global burden of cancer worldwide using the GLOBOCAN 2018 estimates of cancer incidence and mortality produced by the International Agency for Research on Cancer, with a focus on geographic variability across 20 world regions. There will be an estimated 18.1 million new cancer cases (17.0 million excluding nonmelanoma skin cancer) and 9.6 million cancer deaths (9.5 million excluding nonmelanoma skin cancer) in 2018. In both sexes combined, lung cancer is the most commonly diagnosed cancer (11.6% of the total cases) and the leading cause of cancer death (18.4% of the total cancer deaths), closely followed by female breast cancer (11.6%), prostate cancer (7.1%), and colorectal cancer (6.1%) for incidence and colorectal cancer (9.2%), stomach cancer (8.2%), and liver cancer (8.2%) for mortality. Lung cancer is the most frequent cancer and the leading cause of cancer death among males, followed by prostate and colorectal cancer (for incidence) and liver and stomach cancer (for mortality). Among females, breast cancer is the most commonly diagnosed cancer and the leading cause of cancer death, followed by colorectal and lung cancer (for incidence), and vice versa (for mortality); cervical cancer ranks fourth for both incidence and mortality. The most frequently diagnosed cancer and the leading cause of cancer death, however, substantially vary across countries and within each country depending on the degree of economic development and associated social and life style factors. It is noteworthy that high-quality cancer registry data, the basis for planning and implementing evidence-based cancer control programs, are not available in most low- and middle-income countries. The Global Initiative for Cancer Registry Development is an international partnership that supports better estimation, as well as the collection and use of local data, to prioritize and evaluate national cancer control efforts. CA: A Cancer Journal for Clinicians 2018;0:1-31. © 2018 American Cancer Society.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Has erratum",
            "date": "2020-4-6",
            "noticeDoi": "10.3322/caac.21609",
            "doi": "10.3322/caac.21492",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2019-9-6",
            "noticeDoi": "10.1136/bmj.l5391",
            "doi": "10.3322/caac.21492",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 60748,
          "supporting": 282,
          "contradicting": 29,
          "mentioning": 58657,
          "unclassified": 1780,
          "citingPublications": 80114
        }
      },
      {
        "title": "Hallmarks of Cancer: The Next Generation",
        "doi": "10.1016/j.cell.2011.02.013",
        "year": 2011,
        "journal": "Cell",
        "abstract": "The hallmarks of cancer comprise six biological capabilities acquired during the multistep development of human tumors. The hallmarks constitute an organizing principle for rationalizing the complexities of neoplastic disease. They include sustaining proliferative signaling, evading growth suppressors, resisting cell death, enabling replicative immortality, inducing angiogenesis, and activating invasion and metastasis. Underlying these hallmarks are genome instability, which generates the genetic diversity that expedites their acquisition, and inflammation, which fosters multiple hallmark functions. Conceptual progress in the last decade has added two emerging hallmarks of potential generality to this list-reprogramming of energy metabolism and evading immune destruction. In addition to cancer cells, tumors exhibit another dimension of complexity: they contain a repertoire of recruited, ostensibly normal cells that contribute to the acquisition of hallmark traits by creating the \"tumor microenvironment.\" Recognition of the widespread applicability of these concepts will increasingly affect the development of new means to treat human cancer.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51167,
          "supporting": 583,
          "contradicting": 21,
          "mentioning": 49690,
          "unclassified": 873,
          "citingPublications": 59730
        }
      },
      {
        "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
        "doi": "10.1037/0021-9010.88.5.879",
        "year": 2003,
        "journal": "Journal of Applied Psychology",
        "abstract": "Interest in the problem of method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist. Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51016,
          "supporting": 569,
          "contradicting": 162,
          "mentioning": 49580,
          "unclassified": 705,
          "citingPublications": 63567
        }
      },
      {
        "title": "The Measurement of Observer Agreement for Categorical Data",
        "doi": "10.2307/2529310",
        "year": 1977,
        "journal": "Biometrics",
        "abstract": "This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 42579,
          "supporting": 791,
          "contradicting": 72,
          "mentioning": 39999,
          "unclassified": 1717,
          "citingPublications": 69054
        }
      },
      {
        "title": "G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences",
        "doi": "10.3758/bf03193146",
        "year": 2007,
        "journal": "Behavior Research Methods",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 34746,
          "supporting": 149,
          "contradicting": 17,
          "mentioning": 34085,
          "unclassified": 495,
          "citingPublications": 54836
        }
      },
      {
        "title": "Clinical course and risk factors for mortality of adult inpatients with COVID-19 in Wuhan, China: a retrospective cohort study",
        "doi": "10.1016/s0140-6736(20)30566-3",
        "year": 2020,
        "journal": "The Lancet",
        "abstract": "Background Since December, 2019, Wuhan, China, has experienced an outbreak of coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Epidemiological and clinical characteristics of patients with COVID-19 have been reported but risk factors for mortality and a detailed clinical course of illness, including viral shedding, have not been well described.\nMethodsIn this retrospective, multicentre cohort study, we included all adult inpatients (≥18 years old) with laboratoryconfirmed COVID-19 from Jinyintan Hospital and Wuhan Pulmonary Hospital (Wuhan, China) who had been discharged or had died by Jan 31, 2020. Demographic, clinical, treatment, and laboratory data, including serial samples for viral RNA detection, were extracted from electronic medical records and compared between survivors and non-survivors. We used univariable and multivariable <strong class=\"highlight\">logistic</strong> regression methods to explore the risk factors associated with in-hospital death.\nFindings 191 patients (135 from Jinyintan Hospital and 56 from Wuhan Pulmonary Hospital) were included in this study, of whom 137 were discharged and 54 died in hospital. 91 (48%) patients had a comorbidity, with hypertension being the most common (58 [30%] patients), followed by diabetes (36 [19%] patients) and coronary heart disease (15 [8%] patients).Multivariable regression showed increasing odds of in-hospital death associated with older age (odds ratio 1·10, 95% CI 1·03-1·17, per year increase; p=0·0043), higher Sequential Organ Failure Assessment (SOFA) score (5·65, 2·61-12·23; p<0·0001), and d-dimer greater than 1 µg/mL (18·42, 2·64-128·55; p=0·0033) on admission. Median duration of viral shedding was 20·0 days (IQR 17·0-24·0) in survivors, but SARS-CoV-2 was detectable until death in non-survivors. The longest observed duration of viral shedding in survivors was 37 days.Interpretation The potential risk factors of older age, high SOFA score, and d-dimer greater than 1 µg/mL could help clinicians to identify patients with poor prognosis at an early stage. Prolonged viral shedding provides the rationale for a strategy of isolation of infected patients and optimal antiviral interventions in the future.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Comment",
            "date": "2020-7",
            "noticeDoi": "10.1111/resp.13894",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Has erratum",
            "date": "2020-3",
            "noticeDoi": "10.1016/s0140-6736(20)30606-1",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Has erratum",
            "date": "2020-3",
            "noticeDoi": "10.1016/s0140-6736(20)30638-3",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-3",
            "noticeDoi": "10.1016/s0140-6736(20)30633-4",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-3-27",
            "noticeDoi": "10.1136/bmj.m1185",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-4-15",
            "noticeDoi": "10.1136/annrheumdis-2020-217494",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-4",
            "noticeDoi": "10.1016/s0140-6736(20)30868-0",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-4-22",
            "noticeDoi": "10.1017/ice.2020.156",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-4-28",
            "noticeDoi": "10.1007/s11325-020-02087-0",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-4-30",
            "noticeDoi": "10.1007/s00259-020-04837-4",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-4-30",
            "noticeDoi": "10.1093/ehjcvp/pvaa036",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-5",
            "noticeDoi": "10.1016/j.tmaid.2020.101711",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-5-6",
            "noticeDoi": "10.1007/s11606-020-05862-7",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-5-9",
            "noticeDoi": "10.1093/ibd/izaa110",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-5",
            "noticeDoi": "10.1016/s0140-6736(20)31052-7",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-7",
            "noticeDoi": "10.1016/j.jaci.2020.05.013",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2021-1",
            "noticeDoi": "10.1016/j.jinf.2020.05.045",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-7-30",
            "noticeDoi": "10.1093/jalm/jfaa092",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-7",
            "noticeDoi": "10.1016/j.vaccine.2020.06.058",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-9",
            "noticeDoi": "10.1161/hypertensionaha.120.15725",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-7-28",
            "noticeDoi": "10.1080/03009742.2020.1765413",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-8-25",
            "noticeDoi": "10.1128/mbio.01806-20",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020",
            "noticeDoi": "10.4045/tidsskr.20.0443",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-12",
            "noticeDoi": "10.1148/radiol.2020203481",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2020-10",
            "noticeDoi": "10.1016/j.thromres.2020.08.008",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2021-2",
            "noticeDoi": "10.1002/uog.23584",
            "doi": "10.1016/s0140-6736(20)30566-3",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 34087,
          "supporting": 2196,
          "contradicting": 405,
          "mentioning": 29572,
          "unclassified": 1914,
          "citingPublications": 28372
        }
      },
      {
        "title": "ImageNet: A large-scale hierarchical image database",
        "doi": "10.1109/cvpr.2009.5206848",
        "year": 2009,
        "journal": "",
        "abstract": "AbstractThe explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \"ImageNet\", a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to <strong class=\"highlight\">populate</strong> the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 33724,
          "supporting": 59,
          "contradicting": 2,
          "mentioning": 33465,
          "unclassified": 198,
          "citingPublications": 59781
        }
      }
    ]
  },
  "Queue Model": {
    "query": "Queue Model",
    "count": 10,
    "papers": [
      {
        "title": "MEME SUITE: tools for motif discovery and searching",
        "doi": "10.1093/nar/gkp335",
        "year": 2009,
        "journal": "Nucleic Acids Research",
        "abstract": "The MEME Suite web server provides a unified portal for online discovery and analysis of sequence motifs representing features such as DNA binding sites and protein interaction domains. The popular MEME motif discovery algorithm is now complemented by the GLAM2 algorithm which allows discovery of motifs containing gaps. Three sequence scanning algorithms—MAST, FIMO and GLAM2SCAN—allow scanning numerous DNA and protein sequence databases for motifs discovered by MEME and GLAM2. Transcription factor motifs (including those discovered using MEME) can be compared with motifs in many popular motif databases using the motif database scanning algorithm Tomtom. Transcription factor motifs can be further analyzed for putative function by association with Gene Ontology (GO) terms using the motif-GO term association tool GOMO. MEME output now contains sequence LOGOS for each discovered motif, as well as buttons to allow motifs to be conveniently submitted to the sequence and motif database scanning algorithms (MAST, FIMO and Tomtom), or to GOMO, for further analysis. GLAM2 output similarly contains buttons for further analysis using GLAM2SCAN and for rerunning GLAM2 with different parameters. All of the motif-based tools are now implemented as web services via Opal. Source code, binaries and a web server are freely available for noncommercial use at http://meme.nbcr.net.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 7946,
          "supporting": 41,
          "contradicting": 2,
          "mentioning": 7878,
          "unclassified": 25,
          "citingPublications": 9641
        }
      },
      {
        "title": "Complex networks: Structure and dynamics",
        "doi": "10.1016/j.physrep.2005.10.009",
        "year": 2006,
        "journal": "Physics Reports",
        "abstract": "AbstractCoupled biological and chemical systems, neural networks, social interacting species, the Internet and the World Wide Web, are only a few examples of systems composed by a large number of highly interconnected dynamical units. The first approach to capture the global properties of such systems is to <strong class=\"highlight\">model</strong> them as graphs whose nodes represent the dynamical units, and whose links stand for the interactions between them. On the one hand, scientists have to cope with structural issues, such as characterizing the topology of a complex wiring architecture, revealing the unifying principles that are at the basis of real networks, and developing <strong class=\"highlight\">models</strong> to mimic the growth of a network and reproduce its structural properties. On the other hand, many relevant questions arise when studying complex networks' dynamics, such as learning how a large ensemble of dynamical systems that interact through a complex wiring topology can behave collectively. We review the major concepts and results recently achieved in the study of the structure and dynamics of complex networks, and summarize the relevant applications of these ideas in many different disciplines, ranging from nonlinear science to biology, from statistical mechanics to medicine and engineering.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 7833,
          "supporting": 54,
          "contradicting": 4,
          "mentioning": 7581,
          "unclassified": 194,
          "citingPublications": 10389
        }
      },
      {
        "title": "The Structure of the Potassium Channel: Molecular Basis of K\n            +\n            Conduction and Selectivity",
        "doi": "10.1126/science.280.5360.69",
        "year": 1998,
        "journal": "Science",
        "abstract": "The potassium channel from Streptomyces lividans is an integral membrane protein with sequence similarity to all known K+ channels, particularly in the pore region. X-ray analysis with data to 3.2 angstroms reveals that four identical subunits create an inverted teepee, or cone, cradling the selectivity filter of the pore in its outer end. The narrow selectivity filter is only 12 angstroms long, whereas the remainder of the pore is wider and lined with hydrophobic amino acids. A large water-filled cavity and helix dipoles are positioned so as to overcome electrostatic destabilization of an ion in the pore at the center of the bilayer. Main chain carbonyl oxygen atoms from the K+ channel signature sequence line the selectivity filter, which is held open by structural constraints to coordinate K+ ions but not smaller Na+ ions. The selectivity filter contains two K+ ions about 7.5 angstroms apart. This configuration promotes ion conduction by exploiting electrostatic repulsive forces to overcome attractive forces between K+ ions and the selectivity filter. The architecture of the pore establishes the physical principles underlying selective K+ conduction.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 6459,
          "supporting": 200,
          "contradicting": 16,
          "mentioning": 6190,
          "unclassified": 53,
          "citingPublications": 6561
        }
      },
      {
        "title": "Statistical physics of social dynamics",
        "doi": "10.1103/revmodphys.81.591",
        "year": 2009,
        "journal": "Reviews of Modern Physics",
        "abstract": "Statistical physics has proven to be a very fruitful framework to describe phenomena outside the realm of traditional physics. The last years have witnessed the attempt by physicists to study collective phenomena emerging from the interactions of individuals as elementary units in social structures. Here we review the state of the art by focusing on a wide list of topics ranging from opinion, cultural and language dynamics to crowd behavior, hierarchy formation, human dynamics, social spreading. We highlight the connections between these problems and other, more traditional, topics of statistical physics. We also emphasize the comparison of <strong class=\"highlight\">model</strong> results with empirical data from social systems.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4002,
          "supporting": 43,
          "contradicting": 2,
          "mentioning": 3876,
          "unclassified": 81,
          "citingPublications": 3979
        }
      },
      {
        "title": "Internet of Things: A Survey on Enabling Technologies, Protocols, and Applications",
        "doi": "10.1109/comst.2015.2444095",
        "year": 2015,
        "journal": "Ieee Communications Surveys & Tutorials",
        "abstract": "A Fig. 1. The overall picture of IoT emphasizing the vertical markets and the horizontal integration between them.\nRemarks:The architectures that borrow their layers and concepts from network stacks (like the three-layer <strong class=\"highlight\">model</strong>) do not conform to real IoT environments since, e.g., the ‗Network Layer' does not cover all underlying technologies that transfer data to an IoT platform. In addition, these <strong class=\"highlight\">models</strong> have been designed to address specific types of communication media such as WSNs. More importantly, the layers are supposed to be run on resource-constrained devices while having a layer like ‗Service Composition' in SOA-based architecture takes rather a big fraction of the time and energy of the device to communicate with other devices and integrate the required services.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3711,
          "supporting": 9,
          "contradicting": 1,
          "mentioning": 3486,
          "unclassified": 215,
          "citingPublications": 7647
        }
      },
      {
        "title": "Reinforcement Learning:  A Survey",
        "doi": "10.1613/jair.301",
        "year": 1996,
        "journal": "Journal of Artificial Intelligence Research",
        "abstract": "AbstractThis paper surveys the eld of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the eld and a broad selection of current w ork are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but di ers considerably in the details and in the use of the word reinforcement.\" The paper discusses central issues of reinforcement learning, including trading o exploration and exploitation, establishing the foundations of the eld via Markov decision theory, learning from delayed reinforcement, constructing empirical <strong class=\"highlight\">models</strong> to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3576,
          "supporting": 9,
          "contradicting": 0,
          "mentioning": 3460,
          "unclassified": 107,
          "citingPublications": 7458
        }
      },
      {
        "title": "Understanding normal and impaired word reading: Computational principles in quasi-regular domains.",
        "doi": "10.1037/0033-295x.103.1.56",
        "year": 1996,
        "journal": "Psychological Review",
        "abstract": "AbstractWe develop a connectionist approach to processing in quasi-regular domains, as exemplified by English word reading. A consideration of the shortcomings of a previous implementation (Seidenberg &amp; McClelland, 1989, Psych. Rev.)   in reading nonwords leads to the development of orthographic and phonological representations that capture better the relevant structure among the written and spoken forms of words. In a number of simulation experiments, networks using the new representations learn to read both regular and exception words, including low-frequency exception words, and yet are still able to read pronounceable nonwords as well as skilled readers. A mathematical analysis of the effects of word frequency and spelling-sound consistency in a related but simpler system serves to clarify the close relationship of these factors in influencing naming latencies. These insights are verified in subsequent simulations, including an attractor network that reproduces the naming latency data directly in its time to settle on a response. Further analyses of the network's ability to reproduce data on impaired reading in surface dyslexia support a view of the reading system that incorporates a graded division-of-labor between semantic and phonological processes. Such a view is consistent with the more general Seidenberg and McClelland framework and has some similarities with-but also important differences from-the standard dual-route account.Many aspects of language can be characterized as quasiregular-the relationship between inputs and outputs is systematic but admits many exceptions. One such task is the mapping between the written and spoken forms of English words. Most words are regular (e.g., GAVE, MINT)  in that their pronunciations adhere to standard spelling-sound correspondences. There are, however, many irregular or exception words (e.g., HAVE, PINT)  whose pronunciations violate the standard correspondences. To make matters worse, some spelling patterns have a range of pronunciations with none clearly predominating (e.g., OWN  in DOWN, TOWN, BROWN,    This research was supported financially by the National Institute of Mental Health (Grants MH47566 and MH00385), the National Institute on Aging (Grant Ag10109), the National Science Foundation (Grant ASC-9109215), and the McDonnell-Pew Program in Cognitive Neuroscience (Grant T89-01245-016).We thank Marlene Behrmann, Derek Besner, Max Coltheart, Joe Devlin, Geoff Hinton, and Eamon Strain for helpful discussions and comments. We also acknowledge Derek Besner, Max Coltheart, and Michael McCloskey for directing attention to many of the issues addressed in this paper.Correspondence concerning this paper should be sent to Dr. David C. Plaut, Department of Psychology, Carnegie Mellon University, Pittsburgh, PA 15213-3890, plaut@cmu.edu. CROWN vs. KNOWN, SHOWN, GROWN, THROWN,  or OUGH in COUGH, ROUGH, BOUGH, THOUGH, THROUGH). Nonetheless, in the face of this complexity, skilled readers pronounce written words quickly and accurately, and can also use their knowledge...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2989,
          "supporting": 134,
          "contradicting": 12,
          "mentioning": 2790,
          "unclassified": 53,
          "citingPublications": 2445
        }
      },
      {
        "title": "Watersheds in digital spaces: an efficient algorithm based on immersion simulations",
        "doi": "10.1109/34.87344",
        "year": 1991,
        "journal": "Ieee Transactions on Pattern Analysis and Machine Intelligence",
        "abstract": "In this paper, a fast and flexible algorithm for computing watersheds in digital grayscale images is introduced. A review of watersheds and related notion is first presented, and the major methods to determine watersheds are discussed. The present algorithm is based on an immersion process analogy, in which the flooding of the water in the picture is efficiently simulated using a <strong class=\"highlight\">queue</strong> of pixels. It is described in detail and provided in a pseudo C language. We prove the accuracy of this algorithm is superior to that of the existing implementations. Furthermore, it is shown that its adaptation to any kind of digital grid and its generalization to n-dimensional images and even to graphs are straightforward. In addition, its strongest point is that it is faster than any other watershed algorithm. Applications of this algorithm with regard to picture segmentation are presented for MR imagery and for digital elevation <strong class=\"highlight\">models</strong>. An example of 3-D watershed is also provided. Lastly, some ideas are given on how to solve complex segmentation tasks using watersheds on graphs.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2824,
          "supporting": 4,
          "contradicting": 0,
          "mentioning": 2760,
          "unclassified": 60,
          "citingPublications": 5093
        }
      },
      {
        "title": "Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases",
        "doi": "10.1038/s41586-019-1237-9",
        "year": 2019,
        "journal": "Nature",
        "abstract": "Inflammatory bowel diseases, which include Crohn’s disease and ulcerative colitis, affect several million individuals worldwide. Crohn’s disease and ulcerative colitis are complex diseases that are heterogeneous at the clinical, immunological, molecular, genetic, and microbial levels. Individual contributing factors have been the focus of extensive research. As part of the Integrative Human Microbiome Project (HMP2 or iHMP), we followed 132 subjects for one year each to generate integrated longitudinal molecular profiles of host and microbial activity during disease (up to 24 time points each; in total 2,965 stool, biopsy, and blood specimens). Here we present the results, which provide a comprehensive view of functional dysbiosis in the gut microbiome during inflammatory bowel disease activity. We demonstrate a characteristic increase in facultative anaerobes at the expense of obligate anaerobes, as well as molecular disruptions in microbial transcription (for example, among clostridia), metabolite pools (acylcarnitines, bile acids, and short-chain fatty acids), and levels of antibodies in host serum. Periods of disease activity were also marked by increases in temporal variability, with characteristic taxonomic, functional, and biochemical shifts. Finally, integrative analysis identified microbial, biochemical, and host factors central to this dysregulation. The study’s infrastructure resources, results, and data, which are available through the Inflammatory Bowel Disease Multi’omics Database (\n     \n      http://ibdmdb.org\n     \n     ), provide the most comprehensive description to date of host and microbial activities in inflammatory bowel diseases.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2740,
          "supporting": 163,
          "contradicting": 9,
          "mentioning": 2557,
          "unclassified": 11,
          "citingPublications": 2404
        }
      },
      {
        "title": "An Experimental Comparison of Min-cut/Max-flow Algorithms for Energy Minimization in Vision",
        "doi": "10.1007/3-540-44745-8_24",
        "year": 2001,
        "journal": "",
        "abstract": "Abstract. After [10,15,12,2,4] minimum cut/maximum flow algorithms on graphs emerged as an increasingly useful tool for exact or approximate energy minimization in low-level vision. The combinatorial optimization literature provides many min-cut/max-flow algorithms with different polynomial time complexity. Their practical efficiency, however, has to date been studied mainly outside the scope of computer vision. The goal of this paper is to provide an experimental comparison of the efficiency of min-cut/max flow algorithms for energy minimization in vision. We compare the running times of several standard algorithms, as well as a new algorithm that we have recently developed. The algorithms we study include both Goldberg-style \"push-relabel\" methods and algorithms based on Ford-Fulkerson style augmenting paths. We benchmark these algorithms on a number of typical graphs in the contexts of image restoration, stereo, and interactive segmentation. In many cases our new algorithm works several times faster than any of the other methods making near real-time performance possible.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 2566,
          "supporting": 4,
          "contradicting": 0,
          "mentioning": 2545,
          "unclassified": 17,
          "citingPublications": 1689
        }
      }
    ]
  },
  "Compound Annual Growth Rate": {
    "query": "Compound Annual Growth Rate",
    "count": 10,
    "papers": [
      {
        "title": "Global Cancer Statistics 2020: GLOBOCAN Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries",
        "doi": "10.3322/caac.21660",
        "year": 2021,
        "journal": "Ca a Cancer Journal for Clinicians",
        "abstract": "This article provides an update on the global cancer burden using the GLOBOCAN 2020 estimates of cancer incidence and mortality produced by the International Agency for Research on Cancer. Worldwide, an estimated 19.3 million new cancer cases (18.1 million excluding nonmelanoma skin cancer) and almost 10.0 million cancer deaths (9.9 million excluding nonmelanoma skin cancer) occurred in 2020. Female breast cancer has surpassed lung cancer as the most commonly diagnosed cancer, with an estimated 2.3 million new cases (11.7%), followed by lung (11.4%), colorectal (10.0 %), prostate (7.3%), and stomach (5.6%) cancers. Lung cancer remained the leading cause of cancer death, with an estimated 1.8 million deaths (18%), followed by colorectal (9.4%), liver (8.3%), stomach (7.7%), and female breast (6.9%) cancers. Overall incidence was from 2‐fold to 3‐fold higher in transitioned versus transitioning countries for both sexes, whereas mortality varied <2‐fold for men and little for women. Death <strong class=\"highlight\">rates</strong> for female breast and cervical cancers, however, were considerably higher in transitioning versus transitioned countries (15.0 vs 12.8 per 100,000 and 12.4 vs 5.2 per 100,000, respectively). The global cancer burden is expected to be 28.4 million cases in 2040, a 47% rise from 2020, with a larger increase in transitioning (64% to 95%) versus transitioned (32% to 56%) countries due to demographic changes, although this may be further exacerbated by increasing risk factors associated with globalization and a growing economy. Efforts to build a sustainable infrastructure for the dissemination of cancer prevention measures and provision of cancer care in transitioning countries is critical for global cancer control.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 68019,
          "supporting": 246,
          "contradicting": 32,
          "mentioning": 66379,
          "unclassified": 1362,
          "citingPublications": 91700
        }
      },
      {
        "title": "Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries",
        "doi": "10.3322/caac.21492",
        "year": 2018,
        "journal": "Ca a Cancer Journal for Clinicians",
        "abstract": "This article provides a status report on the global burden of cancer worldwide using the GLOBOCAN 2018 estimates of cancer incidence and mortality produced by the International Agency for Research on Cancer, with a focus on geographic variability across 20 world regions. There will be an estimated 18.1 million new cancer cases (17.0 million excluding nonmelanoma skin cancer) and 9.6 million cancer deaths (9.5 million excluding nonmelanoma skin cancer) in 2018. In both sexes combined, lung cancer is the most commonly diagnosed cancer (11.6% of the total cases) and the leading cause of cancer death (18.4% of the total cancer deaths), closely followed by female breast cancer (11.6%), prostate cancer (7.1%), and colorectal cancer (6.1%) for incidence and colorectal cancer (9.2%), stomach cancer (8.2%), and liver cancer (8.2%) for mortality. Lung cancer is the most frequent cancer and the leading cause of cancer death among males, followed by prostate and colorectal cancer (for incidence) and liver and stomach cancer (for mortality). Among females, breast cancer is the most commonly diagnosed cancer and the leading cause of cancer death, followed by colorectal and lung cancer (for incidence), and vice versa (for mortality); cervical cancer ranks fourth for both incidence and mortality. The most frequently diagnosed cancer and the leading cause of cancer death, however, substantially vary across countries and within each country depending on the degree of economic development and associated social and life style factors. It is noteworthy that high-quality cancer registry data, the basis for planning and implementing evidence-based cancer control programs, are not available in most low- and middle-income countries. The Global Initiative for Cancer Registry Development is an international partnership that supports better estimation, as well as the collection and use of local data, to prioritize and evaluate national cancer control efforts. CA: A Cancer Journal for Clinicians 2018;0:1-31. © 2018 American Cancer Society.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Has erratum",
            "date": "2020-4-6",
            "noticeDoi": "10.3322/caac.21609",
            "doi": "10.3322/caac.21492",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2019-9-6",
            "noticeDoi": "10.1136/bmj.l5391",
            "doi": "10.3322/caac.21492",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 60748,
          "supporting": 282,
          "contradicting": 29,
          "mentioning": 58657,
          "unclassified": 1780,
          "citingPublications": 80114
        }
      },
      {
        "title": "Hallmarks of Cancer: The Next Generation",
        "doi": "10.1016/j.cell.2011.02.013",
        "year": 2011,
        "journal": "Cell",
        "abstract": "The hallmarks of cancer comprise six biological capabilities acquired during the multistep development of human tumors. The hallmarks constitute an organizing principle for rationalizing the complexities of neoplastic disease. They include sustaining proliferative signaling, evading <strong class=\"highlight\">growth</strong> suppressors, resisting cell death, enabling replicative immortality, inducing angiogenesis, and activating invasion and metastasis. Underlying these hallmarks are genome instability, which generates the genetic diversity that expedites their acquisition, and inflammation, which fosters multiple hallmark functions. Conceptual progress in the last decade has added two emerging hallmarks of potential generality to this list-reprogramming of energy metabolism and evading immune destruction. In addition to cancer cells, tumors exhibit another dimension of complexity: they contain a repertoire of recruited, ostensibly normal cells that contribute to the acquisition of hallmark traits by creating the \"tumor microenvironment.\" Recognition of the widespread applicability of these concepts will increasingly affect the development of new means to treat human cancer.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51167,
          "supporting": 583,
          "contradicting": 21,
          "mentioning": 49690,
          "unclassified": 873,
          "citingPublications": 59730
        }
      },
      {
        "title": "Common method biases in behavioral research: A critical review of the literature and recommended remedies.",
        "doi": "10.1037/0021-9010.88.5.879",
        "year": 2003,
        "journal": "Journal of Applied Psychology",
        "abstract": "Interest in the problem of method biases has a long history in the behavioral sciences. Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist. Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51016,
          "supporting": 569,
          "contradicting": 162,
          "mentioning": 49580,
          "unclassified": 705,
          "citingPublications": 63567
        }
      },
      {
        "title": "Cytoscape: A Software Environment for Integrated Models of Biomolecular Interaction Networks",
        "doi": "10.1101/gr.1239303",
        "year": 2003,
        "journal": "Genome Research",
        "abstract": "Cytoscape is an open source software project for integrating biomolecular interaction networks with high-throughput expression data and other molecular states into a unified conceptual framework. Although applicable to any system of molecular components and interactions, Cytoscape is most powerful when used in conjunction with large databases of protein-protein, protein-DNA, and genetic interactions that are increasingly available for humans and model organisms. Cytoscape's software Core provides basic functionality to layout and query the network; to visually integrate the network with expression profiles, phenotypes, and other molecular states; and to link the network to databases of functional annotations. The Core is extensible through a straightforward plug-in architecture, allowing rapid development of additional computational analyses and features. Several case studies of Cytoscape plug-ins are surveyed, including a search for interaction pathways correlating with changes in gene expression, a study of protein complexes involved in cellular recovery to DNA damage, inference of a combined physical/functional interaction network for Halobacterium, and an interface to detailed stochastic/kinetic gene regulatory models.[The Cytoscape v1.1 Core runs on all major operating systems and is freely available for download from http://www.cytoscape.org/ as an open source Java application.] Such models promise to transform biological research by providing a framework to (1) systematically interrogate and experimentally verify knowledge of a pathway; (2) manage the immense complexity of hundreds or potentially thousands of cellular components and interactions; and (3) reveal emergent properties and unanticipated consequences of different pathway configurations.Typically, models are directed toward a cellular process or disease pathway of interest (Gilman and Arkin 2002) and are built by formulating existing literature as a system of differential and/or stochastic equations. However, pathway-specific models are now being supplemented with global data gathered for an entire cell or organism, by use of two complementary approaches. First, recent technological developments have made it feasible to measure pathway structure systematically, using highthroughput screens for protein-protein (Ito et al. 2001;von Mering et al. 2002), protein-DNA (Lee et al. 2002, and genetic interactions (Tong et al. 2001). To complement these data, a second set of high-throughput methods are available to characterize the molecular and cellular states induced by pathway interactions under different experimental conditions. For instance, global changes in gene expression are measured with DNA microarrays (DeRisi et al. 1997), whereas changes in protein abundance (Gygi et al. 1999), protein phosphorylation state (Zhou et al. 2001), and metabolite concentrations (Griffin et al. 2001) may be quantified with mass spectrometry, NMR, and other advanced techniques. High-throughput data pertaining to molecular interactions and states are well matched, in...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 34040,
          "supporting": 38,
          "contradicting": 3,
          "mentioning": 33876,
          "unclassified": 123,
          "citingPublications": 44784
        }
      },
      {
        "title": "Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.",
        "doi": "10.1037/0003-066x.55.1.68",
        "year": 2000,
        "journal": "American Psychologist",
        "abstract": "Human beings can be proactive and engaged or, alternatively, passive and alienated, largely as a function of the social conditions in which they develop and function. Accordingly, research guided by self-determination theo~ has focused on the social-contextual conditions that facilitate versus forestall the natural processes of self-motivation and healthy psychological development. Specifically, factors have been examined that enhance versus undermine intrinsic motivation, self-regulation, and well-being. The findings have led to the postulate of three innate psychological needs--competence, autonomy, and relatedness-which when satisfied yield enhanced self-motivation and mental health and when thwarted lead to diminished motivation and well-being. Also considered is the significance of these psychological needs and processes within domains such as health care, education, work, sport, religion, and psychotherapy.T he fullest representations of humanity show people to be curious, vital, and self-motivated. At their best, they are agentic and inspired, striving to learn; extend themselves; master new skills; and apply their talents responsibly. That most people show considerable effort, agency, and commitment in their lives appears, in fact, to be more normative than exceptional, suggesting some very positive and persistent features of human nature.Yet, it is also clear that the human spirit can be diminished or crushed and that individuals sometimes reject <strong class=\"highlight\">growth</strong> and responsibility. Regardless of social strata or cultural origin, examples of both children and adults who are apathetic, alienated, and irresponsible are abundant. Such non-optimal human functioning can be observed not only in our psychological clinics but also among the millions who, for hours a day, sit passively before their televisions, stare blankly from the back of their classrooms, or wait listlessly for the weekend as they go about their jobs. The persistent, proactive, and positive tendencies of human nature are clearly not invariantly apparent.The fact that human nature, phenotypically expressed, can be either active or passive, constructive or indolent, suggests more than mere dispositional differences and is a function of more than just biological endowments. It also bespeaks a wide range of reactions to social environments that is worthy of our most intense scientific investigation. Specifically, social contexts catalyze both within-and between-person differences in motivation and personal <strong class=\"highlight\">growth</strong>, resulting in people being more self-motivated, energized, and integrated in some situations, domains, and cultures than in others. Research on the conditions that foster versus undermine positive human potentials has both theoretical import and practical significance because it can contribute not only to formal knowledge of the causes of human behavior but also to the design of social environments that optimize people's development, performance, and well-being. Research guided by self-determination theory (SDT) has had an ongoing concern with pre...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 30517,
          "supporting": 1088,
          "contradicting": 80,
          "mentioning": 27611,
          "unclassified": 1738,
          "citingPublications": 32868
        }
      },
      {
        "title": "The Iron Cage Revisited: Institutional Isomorphism and Collective Rationality in Organizational Fields",
        "doi": "10.2307/2095101",
        "year": 1983,
        "journal": "American Sociological Review",
        "abstract": "Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 27169,
          "supporting": 453,
          "contradicting": 31,
          "mentioning": 24877,
          "unclassified": 1808,
          "citingPublications": 33643
        }
      },
      {
        "title": "Global cancer statistics",
        "doi": "10.3322/caac.20107",
        "year": 2011,
        "journal": "Ca a Cancer Journal for Clinicians",
        "abstract": "AbstractThe global burden of cancer continues to increase largely because of the aging and <strong class=\"highlight\">growth</strong> of the world population alongside an increasing adoption of cancer-causing behaviors, particularly smoking, in economically developing countries. Based on the GLOBOCAN 2008 estimates, about 12.7 million cancer cases and 7.6 million cancer deaths are estimated to have occurred in 2008; of these, 56% of the cases and 64% of the deaths occurred in the economically developing world. Breast cancer is the most frequently diagnosed cancer and the leading cause of cancer death among females, accounting for 23% of the total cancer cases and 14% of the cancer deaths. Lung cancer is the leading cancer site in males, comprising 17% of the total new cancer cases and 23% of the total cancer deaths. Breast cancer is now also the leading cause of cancer death among females in economically developing countries, a shift from the previous decade during which the most common cause of cancer death was cervical cancer. Further, the mortality burden for lung cancer among females in developing countries is as high as the burden for cervical cancer, with each accounting for 11% of the total female cancer deaths. Although overall cancer incidence <strong class=\"highlight\">rates</strong> in the developing world are half those seen in the developed world in both sexes, the overall cancer mortality <strong class=\"highlight\">rates</strong> are generally similar. Cancer survival tends to be poorer in developing countries, most likely because of a combination of a late stage at diagnosis and limited access to timely and standard treatment. A substantial proportion of the worldwide burden of cancer could be prevented through the application of existing cancer control knowledge and by implementing programs for tobacco control, vaccination (for liver and cervical cancers), and early detection and treatment, as well as public health campaigns promoting physical activity and a healthier dietary intake. Clinicians, public health professionals, and policy makers can play an active role in accelerating the application of such interventions globally. CA Cancer J Clin 2011;61:69-90.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Has erratum",
            "date": "2011-2-4",
            "noticeDoi": "10.3322/caac.20108",
            "doi": "10.3322/caac.20107",
            "urls": null
          },
          {
            "status": "Comment",
            "date": "2011-2-4",
            "noticeDoi": "10.3322/caac.20108",
            "doi": "10.3322/caac.20107",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23973,
          "supporting": 98,
          "contradicting": 23,
          "mentioning": 23312,
          "unclassified": 540,
          "citingPublications": 51549
        }
      },
      {
        "title": "Cancer incidence and mortality worldwide: Sources, methods and major patterns in GLOBOCAN 2012",
        "doi": "10.1002/ijc.29210",
        "year": 2014,
        "journal": "International Journal of Cancer",
        "abstract": "Estimates of the worldwide incidence and mortality from 27 major cancers and for all cancers combined for 2012 are now available in the GLOBOCAN series of the International Agency for Research on Cancer. We review the sources and methods used in compiling the national cancer incidence and mortality estimates, and briefly describe the key results by cancer site and in 20 large \"areas\" of the world. Overall, there were 14.1 million new cases and 8.2 million deaths in 2012. The most commonly diagnosed cancers were lung (1.82 million), breast (1.67 million), and colorectal (1.36 million); the most common causes of cancer death were lung cancer (1.6 million deaths), liver cancer (745,000 deaths), and stomach cancer (723,000 deaths).According to WHO estimates for 2011, cancer now causes more deaths than all coronary heart disease or all stroke.1 The continuing global demographic and epidemiologic transitions signal an ever-increasing cancer burden over the next decades, particularly in low and middle income countries (LMIC), with over 20 million new cancer cases expected <strong class=\"highlight\">annually</strong> as early as 2025.2 The GLOBOCAN estimates for 2012 3 aim to provide the evidence and impetus for developing resource-contingent strategies to reduce the cancer burden worldwide.We review here the fifth version of GLOBOCAN, the sources and methods used in compiling cancer incidence and mortality estimates for 2012 in 184 countries worldwide, and briefly describe the key results by cancer site. The basic units for estimation are countries, although we present the results globally, by level of development and for aggregated regions, as defined by the United Nations. 4  Such estimates have been prepared for 27 major cancers and for all cancers combined and by sex. While the methods of estimation have been refined over time, they still rely upon the best available data on cancer incidence and mortality at the national level in assembling regional and global profiles. Facilities for the tabulation and graphical visualisation of the full dataset of 184 countries and 30 world regions by sex can be accessed via the GLOBOCAN homepage (http://globocan.iarc.fr).To document the methods used in compiling the estimates and guide users as to their validity, we introduce an alphanumeric scoring system that provides information on the availability and quality of the incidence and mortality sources at the country level.\nDataIncidence data derive from population-based cancer registries (PBCR). Although PBCR may cover national populations, more often they cover smaller, subnational areas, and, particularly in countries undergoing development, only selected urban areas. In 2006, about 21% of the world population was covered by PBCR, with sparse registration in Asia (8% of the total population) and in Africa (11%). 5 In terms of what is considered data of high quality (for example, those included in the latest volume (X) of the IARC Cancer Incidence in Five Continents (CI5) series 6 ), these percentages are even lower: only 14% of the world population is covered by...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23634,
          "supporting": 136,
          "contradicting": 18,
          "mentioning": 22554,
          "unclassified": 926,
          "citingPublications": 29484
        }
      },
      {
        "title": "Intrinsic Motivation and Self-Determination in Human Behavior",
        "doi": "10.1007/978-1-4899-2271-7",
        "year": 1985,
        "journal": "",
        "abstract": "A Continuation Order Plan is available for this series. A continuation order will bring delivery of each new volume immediately upon publication. Volumes are billed only upon actual shipment. For further information please contact the publisher.Originally published by Plenum Press, New York in 1985 Softcover reprint of the hardcover 1st edition 1985\nAll rights reservedNo part of this book may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying, microfilming, recording, or otherwise, without written permission from the Publisher To Our Parents:",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 22936,
          "supporting": 767,
          "contradicting": 52,
          "mentioning": 19636,
          "unclassified": 2481,
          "citingPublications": 24009
        }
      }
    ]
  },
  "Economy curve": {
    "query": "Economy curve",
    "count": 10,
    "papers": [
      {
        "title": "Endogenous Technological Change",
        "doi": "10.1086/261725",
        "year": 1990,
        "journal": "Journal of Political <strong Class=\"highlight\">economy</Strong>",
        "abstract": "Growth in this model is driven by technological change that arises from intentional investment decisions made by profit-maximizing agents. The distinguishing feature of the technology as an input is that it is neither a conventional good nor a public good; it is a nonrival, partially excludable good. Because of the nonconvexity introduced by a nonrival good, price-taking competition cannot be supported. Instead, the equilibrium is one with monopolistic competition. The main conclusions are that the stock of human capital determines the rate of growth, that too little human capital is devoted to research in equilibrium, that integration into world markets will increase growth rates, and that having a large population is not sufficient to generate growth.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 11070,
          "supporting": 165,
          "contradicting": 17,
          "mentioning": 9675,
          "unclassified": 1213,
          "citingPublications": 16514
        }
      },
      {
        "title": "Adolescence-limited and life-course-persistent antisocial behavior: A developmental taxonomy.",
        "doi": "10.1037/0033-295x.100.4.674",
        "year": 1993,
        "journal": "Psychological Review",
        "abstract": "A dual taxonomy is presented to reconcile 2 incongruous facts about antisocial behavior: (a) It shows impressive continuity over age, but (b) its prevalence changes dramatically over age, increasing almost 10-fold temporarily during adolescence. This article suggests that delinquency conceals 2 distinct categories of individuals, each with a unique natural history and etiology: A small group engages in antisocial behavior of 1 sort or another at every life stage, whereas a larger group is antisocial only during adolescence. According to the theory of life-course-persistent antisocial behavior, children's neuropsychological problems interact cumulatively with their criminogenic environments across development, culminating in a pathological personality. According to the theory of adolescence-limited antisocial behavior, a contemporary maturity gap encourages teens to mimic antisocial behavior in ways that are normative and adjustive.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 8073,
          "supporting": 570,
          "contradicting": 49,
          "mentioning": 7126,
          "unclassified": 328,
          "citingPublications": 8702
        }
      },
      {
        "title": "An Introduction to Efficiency and Productivity Analysis",
        "doi": "10.1007/978-1-4615-5493-6",
        "year": 1998,
        "journal": "",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 5800,
          "supporting": 28,
          "contradicting": 1,
          "mentioning": 4910,
          "unclassified": 861,
          "citingPublications": 5808
        }
      },
      {
        "title": "Crime and Punishment: An Economic Approach",
        "doi": "10.1086/259394",
        "year": 1968,
        "journal": "Journal of Political <strong Class=\"highlight\">economy</Strong>",
        "abstract": "Since the turn of the century, legislation in Western countries has expanded rapidly to reverse the brief dominance of laissez faire during the nineteenth century. The state no longer merely protects against violations of person and property through murder, rape, or burglary but also restricts \"discrimination\" against certain minorities, collusive business arrangements, \"jaywalking,\" travel, the materials used in construction, and thousands of other activities. The activities restricted not only are numerous but also range widely, affecting persons in very different pursuits and of diverse social backgrounds, education levels, ages, races, etc. Moreover, the likelihood that an offender will be discovered and con-I would like to thank the Lilly Endowment for financing a very productive summer in 1965 at the University of California at Los Angeles. While there I received very helpful comments on an earlier draft from, among others,",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 5521,
          "supporting": 158,
          "contradicting": 24,
          "mentioning": 4676,
          "unclassified": 663,
          "citingPublications": 11387
        }
      },
      {
        "title": "The socio-economic implications of the coronavirus pandemic (COVID-19): A review",
        "doi": "10.1016/j.ijsu.2020.04.018",
        "year": 2020,
        "journal": "International Journal of Surgery",
        "abstract": "The COVID-19 pandemic has resulted in over 1.4 million confirmed cases and over 83,000 deaths globally. It has also sparked fears of an impending economic crisis and recession. Social distancing, self-isolation and travel restrictions forced a decrease in the workforce across all economic sectors and caused many jobs to be lost. Schools have closed down, and the need of commodities and manufactured products has decreased. In contrast, the need for medical supplies has significantly increased. The food sector has also seen a great demand due to panic-buying and stockpiling of food products. In response to this global outbreak, we summarise the socio-economic effects of COVID-19 on individual aspects of the world <strong class=\"highlight\">economy</strong>.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 5204,
          "supporting": 66,
          "contradicting": 3,
          "mentioning": 4852,
          "unclassified": 283,
          "citingPublications": 6334
        }
      },
      {
        "title": "INTEREST AND PRICES: FOUNDATIONS OF A THEORY OF MONETARY POLICY",
        "doi": "10.1017/s1365100505040253",
        "year": 2005,
        "journal": "Macroeconomic Dynamics",
        "abstract": "The evolution of research on monetary policy over the past two decades has been dramatic. The 1980's through the mid-1990's were dominated by work building on the insights of Kydland and Prescott, employing the notion of dynamic consistency to offer a theory of monetary policy, one in which discretionary policy by central banks led to socially costly inflation. Researchers studied the nature of the time inconsistency of monetary policy and developed solutions, ranging from appointing conservatives, to developing incentive contracts, to imposing inflation targets. Whether central bankers learned, in the words of Ben McCallum, to \"just do it,\" or the nature of the incentives they faced changed, inflation was reduced in the industrialized <strong class=\"highlight\">economies</strong> during the 1980's and early 1990's. In the low-inflation environment since, central banks have, in the eyes of many monetary economists, changed from a source of (often) politically induced inflation to maximizers of social welfare. This shift has opened new and productive collaborations between academic economists and central bank economists interested in issues of monetary policy design. Much of this work builds on the basic foundations provided by the marriage of dynamic stochastic general equilibrium models, with their emphasis on optimizing behavior by economic agents and careful attention to budget constraints and equilibrium conditions, with simple models of price stickiness. Early contributors to these foundations include Yun (1996), Goodfriend and King (1997), and Rotemberg and Woodford (1997). The resulting framework for macroeconomic analysis, in its simplest form, boils down to an expectational IS <strong class=\"highlight\">curve</strong>, an inflation adjustment equation, and a specification of monetary policy in terms of either an objective function or a rule for setting the nominal rate of interest.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4736,
          "supporting": 120,
          "contradicting": 6,
          "mentioning": 4464,
          "unclassified": 146,
          "citingPublications": 5302
        }
      },
      {
        "title": "The Financial Accelerator in a Quantitative Business Cycle Framework",
        "doi": "10.3386/w6455",
        "year": 1998,
        "journal": "",
        "abstract": null,
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 4621,
          "supporting": 111,
          "contradicting": 20,
          "mentioning": 4419,
          "unclassified": 71,
          "citingPublications": 3504
        }
      },
      {
        "title": "Methods for in vitro evaluating antimicrobial activity: A review",
        "doi": "10.1016/j.jpha.2015.11.005",
        "year": 2016,
        "journal": "Journal of Pharmaceutical Analysis",
        "abstract": "In recent years, there has been a growing interest in researching and developing new antimicrobial agents from various sources to combat microbial resistance. Therefore, a greater attention has been paid to antimicrobial activity screening and evaluating methods. Several bioassays such as disk-diffusion, well diffusion and broth or agar dilution are well known and commonly used, but others such as flow cytofluorometric and bioluminescent methods are not widely used because they require specified equipment and further evaluation for reproducibility and standardization, even if they can provide rapid results of the antimicrobial agent's effects and a better understanding of their impact on the viability and cell damage inflicted to the tested microorganism. In this review article, an exhaustive list of in vitro antimicrobial susceptibility testing methods and detailed information on their advantages and limitations are reported.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3825,
          "supporting": 28,
          "contradicting": 2,
          "mentioning": 3664,
          "unclassified": 131,
          "citingPublications": 6007
        }
      },
      {
        "title": "Heart Disease and Stroke Statistics—2022 Update: A Report From the American Heart Association",
        "doi": "10.1161/cir.0000000000001052",
        "year": 2022,
        "journal": "Circulation",
        "abstract": "Background:\n            The American Heart Association, in conjunction with the National Institutes of Health, annually reports the most up-to-date statistics related to heart disease, stroke, and cardiovascular risk factors, including core health behaviors (smoking, physical activity, diet, and weight) and health factors (cholesterol, blood pressure, and glucose control) that contribute to cardiovascular health. The Statistical Update presents the latest data on a range of major clinical heart and circulatory disease conditions (including stroke, congenital heart disease, rhythm disorders, subclinical atherosclerosis, coronary heart disease, heart failure, valvular disease, venous disease, and peripheral artery disease) and the associated outcomes (including quality of care, procedures, and economic costs).\n          \n          \n            Methods:\n            The American Heart Association, through its Statistics Committee, continuously monitors and evaluates sources of data on heart disease and stroke in the United States to provide the most current information available in the annual Statistical Update. The 2022 Statistical Update is the product of a full year’s worth of effort by dedicated volunteer clinicians and scientists, committed government professionals, and American Heart Association staff members. This year’s edition includes data on the monitoring and benefits of cardiovascular health in the population and an enhanced focus on social determinants of health, adverse pregnancy outcomes, vascular contributions to brain health, and the global burden of cardiovascular disease and healthy life expectancy.\n          \n          \n            Results:\n            Each of the chapters in the Statistical Update focuses on a different topic related to heart disease and stroke statistics.\n          \n          \n            Conclusions:\n            The Statistical Update represents a critical resource for the lay public, policymakers, media professionals, clinicians, health care administrators, researchers, health advocates, and others seeking the best available data on these factors and conditions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [
          {
            "status": "Has correction",
            "date": "2022-9-6",
            "noticeDoi": "10.1161/cir.0000000000001074",
            "doi": "10.1161/cir.0000000000001052",
            "urls": null
          },
          {
            "status": "Has erratum",
            "date": "2022-9-6",
            "noticeDoi": "10.1161/cir.0000000000001074",
            "doi": "10.1161/cir.0000000000001052",
            "urls": null
          }
        ],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3483,
          "supporting": 32,
          "contradicting": 7,
          "mentioning": 3391,
          "unclassified": 53,
          "citingPublications": 4701
        }
      },
      {
        "title": "A spreading-activation theory of semantic processing.",
        "doi": "10.1037/0033-295x.82.6.407",
        "year": 1975,
        "journal": "Psychological Review",
        "abstract": "This paper presents a spreading-activation theory of human semantic processing, which can be applied to a wide range of recent experimental results. The theory is based on Quillian's theory of semantic memory search and semantic preparation, or priming. In conjunction with this, several of the miscondeptions concerning Qullian's theory are discussed. A number of additional assumptions are proposed for his theory in order to apply it to recent experiments. The present paper shows how the extended theory can account for results of several production experiments by Loftus, Juola and Atkinson's multiple-category experiment, Conrad's sentence-verification experiments, and several categorization experiments on the effect of semantic relatedness and typicality by Holyoak and Glass, Rips, Shoben, and Smith, and Rosch. The paper also provides a critique of the Smith, Shoben, and Rips model for categorization judgments.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 3477,
          "supporting": 93,
          "contradicting": 5,
          "mentioning": 3285,
          "unclassified": 94,
          "citingPublications": 7042
        }
      }
    ]
  },
  "Monte Carlo Simulation": {
    "query": "Monte Carlo Simulation",
    "count": 10,
    "papers": [
      {
        "title": "Some Tests of Specification for Panel Data: <strong class=\"highlight\">Monte</strong> <strong class=\"highlight\">Carlo</strong> Evidence and an Application to Employment Equations",
        "doi": "10.2307/2297968",
        "year": 1991,
        "journal": "The Review of Economic Studies",
        "abstract": "JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.Oxford University Press and The Review of Economic Studies, Ltd. are collaborating with JSTOR to digitize, preserve and extend access to The Review of Economic Studies. This paper presents specification tests that are applicable after estimating a dynamic model from panel data by the generalized method of moments (GMM), and studies the practical performance of these procedures using both generated and real data. Our GMM estimator optimally exploits all the linear moment restrictions that follow from the assumption of no serial correlation in the errors, in an equation which contains individual effects, lagged dependent variables and no strictly exogenous variables. We propose a test of serial correlation based on the GMM residuals and compare this with Sargan tests of over-identifying restrictions and Hausman specification tests.\nIn this paper we represent this type of matrix by Zi = diag (yil, .Yis), (s = 1 ... i T-2).\nAn alternative choice of AN is (N-1 ZfAlZi)-l withA = N-1 S The resulting estimator does not depend on the data fourth-order moments and is asymptotically equivalent to '2 provided the vi, are serially independent. Note that in this case E(Z!ti3ivZi) = E(Z!fliZi) and limN o0 N-1 , E[Z!(fli -fN)Zi] = O (see White (1982), p. 492).",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 21264,
          "supporting": 212,
          "contradicting": 10,
          "mentioning": 20328,
          "unclassified": 714,
          "citingPublications": 27692
        }
      },
      {
        "title": "QUANTUM ESPRESSO: a modular and open-source software project for quantum <strong class=\"highlight\">simulations</strong> of materials",
        "doi": "10.1088/0953-8984/21/39/395502",
        "year": 2009,
        "journal": "Journal of Physics Condensed Matter",
        "abstract": "QUANTUM ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling, based on density-functional theory, plane waves, and pseudopotentials (norm-conserving, ultrasoft, and projector-augmented wave). The acronym ESPRESSO stands for opEn Source Package for Research in Electronic Structure, <strong class=\"highlight\">Simulation</strong>, and Optimization. It is freely available to researchers around the world under the terms of the GNU General Public License. QUANTUM ESPRESSO builds upon newly-restructured electronic-structure codes that have been developed and tested by some of the original authors of novel electronic-structure algorithms and applied in the last twenty years by some of the leading materials modeling groups worldwide. Innovation and efficiency are still its main focus, with special attention paid to massively parallel architectures, and a great effort being devoted to user friendliness. QUANTUM ESPRESSO is evolving towards a distribution of independent and interoperable codes in the spirit of an open-source project, where researchers active in the field of electronic-structure calculations are encouraged to participate in the project by contributing their own codes or by implementing their own ideas into existing codes.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 0,
          "supporting": 0,
          "contradicting": 0,
          "mentioning": 0,
          "unclassified": 0,
          "citingPublications": 0
        }
      },
      {
        "title": "The electronic properties of graphene",
        "doi": "10.1103/revmodphys.81.109",
        "year": 2009,
        "journal": "Reviews of Modern Physics",
        "abstract": "This article reviews the basic theoretical aspects of graphene, a one atom\nthick allotrope of carbon, with unusual two-dimensional Dirac-like electronic\nexcitations. The Dirac electrons can be controlled by application of external\nelectric and magnetic fields, or by altering sample geometry and/or topology.\nWe show that the Dirac electrons behave in unusual ways in tunneling,\nconfinement, and integer quantum Hall effect. We discuss the electronic\nproperties of graphene stacks and show that they vary with stacking order and\nnumber of layers. Edge (surface) states in graphene are strongly dependent on\nthe edge termination (zigzag or armchair) and affect the physical properties of\nnanoribbons. We also discuss how different types of disorder modify the Dirac\nequation leading to unusual spectroscopic and transport properties. The effects\nof electron-electron and electron-phonon interactions in single layer and\nmultilayer graphene are also presented.Comment: Accepted for publication in Reviews of Modern Physics. Various\n  sessions and figures were modified, and new references adde",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 18350,
          "supporting": 319,
          "contradicting": 12,
          "mentioning": 17837,
          "unclassified": 182,
          "citingPublications": 23990
        }
      },
      {
        "title": "A new criterion for assessing discriminant validity in variance-based structural equation modeling",
        "doi": "10.1007/s11747-014-0403-8",
        "year": 2014,
        "journal": "Journal of the Academy of Marketing Science",
        "abstract": "Abstract Discriminant validity assessment has become a generally accepted prerequisite for analyzing relationships between latent variables. For variance-based structural equation modeling, such as partial least squares, the FornellLarcker criterion and the examination of cross-loadings are the dominant approaches for evaluating discriminant validity. By means of a <strong class=\"highlight\">simulation</strong> study, we show that these approaches do not reliably detect the lack of discriminant validity in common research situations. We therefore propose an alternative approach, based on the multitrait-multimethod matrix, to assess discriminant validity: the heterotrait-monotrait ratio of correlations. We demonstrate its superior performance by means of a <strong class=\"highlight\">Monte</strong> <strong class=\"highlight\">Carlo</strong> <strong class=\"highlight\">simulation</strong> study, in which we compare the new approach to the Fornell-Larcker criterion and the assessment of (partial) cross-loadings. Finally, we provide guidelines on how to handle discriminant validity issues in variance-based structural equation modeling.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 18196,
          "supporting": 612,
          "contradicting": 26,
          "mentioning": 17043,
          "unclassified": 515,
          "citingPublications": 25275
        }
      },
      {
        "title": "A smooth particle mesh Ewald method",
        "doi": "10.1063/1.470117",
        "year": 1995,
        "journal": "The Journal of Chemical Physics",
        "abstract": "The previously developed particle mesh Ewald method is reformulated in terms of efficient B-spline interpolation of the structure factors. This reformulation allows a natural extension of the method to potentials of the form 1/r p with pу1. Furthermore, efficient calculation of the virial tensor follows. Use of B-splines in place of Lagrange interpolation leads to analytic gradients as well as a significant improvement in the accuracy. We demonstrate that arbitrary accuracy can be achieved, independent of system size N, at a cost that scales as N log(N). For biomolecular systems with many thousands of atoms this method permits the use of Ewald summation at a computational cost comparable to that of a simple truncation method of 10 Å or less.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16931,
          "supporting": 43,
          "contradicting": 1,
          "mentioning": 16840,
          "unclassified": 47,
          "citingPublications": 21528
        }
      },
      {
        "title": "MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space",
        "doi": "10.1093/sysbio/sys029",
        "year": 2012,
        "journal": "Systematic Biology",
        "abstract": "AbstractSince its introduction in 2001, MrBayes has grown in popularity as a software package for Bayesian phylogenetic inference using Markov chain <strong class=\"highlight\">Monte</strong> <strong class=\"highlight\">Carlo</strong> (MCMC) methods. With this note, we announce the release of version 3.2, a major upgrade to the latest official release presented in 2003. The new version provides convergence diagnostics and allows multiple analyses to be run in parallel with convergence progress monitored on the fly. The introduction of new proposals and automatic optimization of tuning parameters has improved convergence for many problems. The new version also sports significantly faster likelihood calculations through streaming single-instruction-multiple-data extensions (SSE) and support of the BEAGLE library, allowing likelihood calculations to be delegated to graphics processing units (GPUs) on compatible hardware. Speedup factors range from around 2 with SSE code to more than 50 with BEAGLE for codon problems. Checkpointing across all models allows long runs to be completed even when an analysis is prematurely terminated. New models include relaxed clocks, dating, model averaging across time-reversible substitution models, and support for hard, negative, and partial (backbone) tree constraints. Inference of species trees from gene trees is supported by full incorporation of the Bayesian estimation of species trees (BEST) algorithms. Marginal model likelihoods for Bayes factor tests can be estimated accurately across the entire model space using the stepping stone method. The new version provides more output options than previously, including samples of ancestral states, site rates, site dN/dS rations, branch rates, and node dates. A wide range of statistics on tree parameters can also be output for visualization in FigTree and compatible software.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 15517,
          "supporting": 30,
          "contradicting": 0,
          "mentioning": 15422,
          "unclassified": 65,
          "citingPublications": 24637
        }
      },
      {
        "title": "Co-Integration and Error Correction: Representation, Estimation, and Testing",
        "doi": "10.2307/1913236",
        "year": 1987,
        "journal": "Econometrica",
        "abstract": "The relationship between co-integration and error correction models, first suggested in Granger (1981), is here extended and used to develop estimation procedures, tests, and empirical examples. If each element of a vector of time series x, first achieves stationarity after differencing, but a linear combination a'x, is already stationary, the time series x, are said to be co-integrated with co-integrating vector a. There may be several such co-integrating vectors so that a becomes a matrix. Interpreting a'x, = 0 as a long run equilibrium, co-integration implies that deviations from equilibrium are stationary, with finite variance, even though the series themselves are nonstationary and have infinite variance. The paper presents a representation theorem based on Granger (1983), which connects the moving average, autoregressive, and error correction representations for co-integrated systems. A vector autoregression in differenced variables is incompatible with these representations. Estimation of these models is discussed and a simple but asymptotically efficient two-step estimator is proposed. Testing for co-integration combines the problems of unit root tests and tests with parameters unidentified under the null. Seven statistics are formulated and analyzed. The critical values of these statistics are calculated based on a <strong class=\"highlight\">Monte</strong> <strong class=\"highlight\">Carlo</strong> <strong class=\"highlight\">simulation</strong>. Using these critical values, the power properties of the tests are examined and one test procedure is recommended for application. In a veries of examples it is found that consumption and income are co-integrated, wages and prices are not, short and long interest rates are, and nominal GNP is co-integrated with M2, but not Ml, M3, or aggregate liquid assets.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 14327,
          "supporting": 101,
          "contradicting": 7,
          "mentioning": 13430,
          "unclassified": 789,
          "citingPublications": 26082
        }
      },
      {
        "title": "All-Atom Empirical Potential for Molecular Modeling and Dynamics Studies of Proteins",
        "doi": "10.1021/jp973084f",
        "year": 1998,
        "journal": "The Journal of Physical Chemistry B",
        "abstract": "New protein parameters are reported for the all-atom empirical energy function in the CHARMM program. The parameter evaluation was based on a self-consistent approach designed to achieve a balance between the internal (bonding) and interaction (nonbonding) terms of the force field and among the solvent-solvent, solvent-solute, and solute-solute interactions. Optimization of the internal parameters used experimental gas-phase geometries, vibrational spectra, and torsional energy surfaces supplemented with ab initio results. The peptide backbone bonding parameters were optimized with respect to data for N-methylacetamide and the alanine dipeptide. The interaction parameters, particularly the atomic charges, were determined by fitting ab initio interaction energies and geometries of complexes between water and model compounds that represented the backbone and the various side chains. In addition, dipole moments, experimental heats and free energies of vaporization, solvation and sublimation, molecular volumes, and crystal pressures and structures were used in the optimization. The resulting protein parameters were tested by applying them to noncyclic tripeptide crystals, cyclic peptide crystals, and the proteins crambin, bovine pancreatic trypsin inhibitor, and carbonmonoxy myoglobin in vacuo and in crystals. A detailed analysis of the relationship between the alanine dipeptide potential energy surface and calculated protein φ, χ angles was made and used in optimizing the peptide group torsional parameters. The results demonstrate that use of ab initio structural and energetic data by themselves are not sufficient to obtain an adequate backbone representation for peptides and proteins in solution and in crystals. Extensive comparisons between molecular dynamics <strong class=\"highlight\">simulations</strong> and experimental data for polypeptides and proteins were performed for both structural and dynamic properties. Energy minimization and dynamics <strong class=\"highlight\">simulations</strong> for crystals demonstrate that the latter are needed to obtain meaningful comparisons with experimental crystal structures. The presented parameters, in combination with the previously published CHARMM all-atom parameters for nucleic acids and lipids, provide a consistent set for condensed-phase <strong class=\"highlight\">simulations</strong> of a wide variety of molecules of biological interest.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 13611,
          "supporting": 101,
          "contradicting": 6,
          "mentioning": 13443,
          "unclassified": 61,
          "citingPublications": 14135
        }
      },
      {
        "title": "CHARMM: A program for macromolecular energy, minimization, and dynamics calculations",
        "doi": "10.1002/jcc.540040211",
        "year": 1983,
        "journal": "Journal of Computational Chemistry",
        "abstract": "CHARMM (Chemistry at HARvard Macromolecular Mechanics) is a highly flexible computer program which uses empirical energy functions to model macromolecular systems. The program can read or model build structures, energy minimize them by first-or second-derivative techniques, perform a normal mode or molecular dynamics <strong class=\"highlight\">simulation</strong>, and analyze the structural, equilibrium, and dynamic properties determined in these calculations. The operations that CHARMM can perform are described, and some implementation details are given. A set of parameters for the empirical energy function and a sample run are included.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 13018,
          "supporting": 56,
          "contradicting": 0,
          "mentioning": 12872,
          "unclassified": 90,
          "citingPublications": 15113
        }
      },
      {
        "title": "The Structure and Function of Complex Networks",
        "doi": "10.1137/s003614450342480",
        "year": 2003,
        "journal": "Siam Review",
        "abstract": "Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12904,
          "supporting": 148,
          "contradicting": 11,
          "mentioning": 12318,
          "unclassified": 427,
          "citingPublications": 17092
        }
      }
    ]
  },
  "Graph Theory": {
    "query": "Graph Theory",
    "count": 10,
    "papers": [
      {
        "title": "Gapped BLAST and PSI-BLAST: a new generation of protein database search programs",
        "doi": "10.1093/nar/25.17.3389",
        "year": 1997,
        "journal": "Nucleic Acids Research",
        "abstract": "The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 50326,
          "supporting": 111,
          "contradicting": 12,
          "mentioning": 49877,
          "unclassified": 326,
          "citingPublications": 69682
        }
      },
      {
        "title": "Gradient-based learning applied to document recognition",
        "doi": "10.1109/5.726791",
        "year": 1998,
        "journal": "Proceedings of the Ieee",
        "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradientbased learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques.Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called <strong class=\"highlight\">graph</strong> transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure.Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of <strong class=\"highlight\">graph</strong> transformer networks.A <strong class=\"highlight\">graph</strong> transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 25812,
          "supporting": 80,
          "contradicting": 8,
          "mentioning": 25415,
          "unclassified": 309,
          "citingPublications": 52110
        }
      },
      {
        "title": "When to use and how to report the results of PLS-SEM",
        "doi": "10.1108/ebr-11-2018-0203",
        "year": 2019,
        "journal": "European Business Review",
        "abstract": "Purpose\nThe purpose of this paper is to provide a comprehensive, yet concise, overview of the considerations and metrics required for partial least squares structural equation modeling (PLS-SEM) analysis and result reporting. Preliminary considerations are summarized first, including reasons for choosing PLS-SEM, recommended sample size in selected contexts, distributional assumptions, use of secondary data, statistical power and the need for goodness-of-fit testing. Next, the metrics as well as the rules of thumb that should be applied to assess the PLS-SEM results are covered. Besides presenting established PLS-SEM evaluation criteria, the overview includes the following new guidelines: PLSpredict (i.e., a novel approach for assessing a model’s out-of-sample prediction), metrics for model comparisons, and several complementary methods for checking the results’ robustness.\n\n\nDesign/methodology/approach\nThis paper provides an overview of previously and recently proposed metrics as well as rules of thumb for evaluating the research results based on the application of PLS-SEM.\n\n\nFindings\nMost of the previously applied metrics for evaluating PLS-SEM results are still relevant. Nevertheless, scholars need to be knowledgeable about recently proposed metrics (e.g. model comparison criteria) and methods (e.g. endogeneity assessment, latent class analysis and PLSpredict), and when and how to apply them to extend their analyses.\n\n\nResearch limitations/implications\nMethodological developments associated with PLS-SEM are rapidly emerging. The metrics reported in this paper are useful for current applications, but must always be up to date with the latest developments in the PLS-SEM method.\n\n\nOriginality/value\nIn light of more recent research and methodological developments in the PLS-SEM domain, guidelines for the method’s use need to be continuously extended and updated. This paper is the most current and comprehensive summary of the PLS-SEM method and the metrics applied to assess its solutions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 23047,
          "supporting": 563,
          "contradicting": 27,
          "mentioning": 21451,
          "unclassified": 1006,
          "citingPublications": 17285
        }
      },
      {
        "title": "PRISMA Extension for Scoping Reviews (PRISMA-ScR): Checklist and Explanation",
        "doi": "10.7326/m18-0850",
        "year": 2018,
        "journal": "Annals of Internal Medicine",
        "abstract": "Scoping reviews, a type of knowledge synthesis, follow a systematic approach to map evidence on a topic and identify main concepts, <strong class=\"highlight\">theories</strong>, sources, and knowledge gaps. Although more scoping reviews are being done, their methodological and reporting quality need improvement. This document presents the PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews) checklist and explanation. The checklist was developed by a 24-member expert panel and 2 research leads following published guidance from the EQUATOR (Enhancing the QUAlity and Transparency Of health Research) Network. The final checklist contains 20 essential reporting items and 2 optional items. The authors provide a rationale and an example of good reporting for each item. The intent of the PRISMA-ScR is to help readers (including researchers, publishers, commissioners, policymakers, health care providers, guideline developers, and patients or consumers) develop a greater understanding of relevant terminology, core concepts, and key items to report for scoping reviews.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 21547,
          "supporting": 50,
          "contradicting": 0,
          "mentioning": 20413,
          "unclassified": 1084,
          "citingPublications": 27673
        }
      },
      {
        "title": "Multiwfn: A multifunctional wavefunction analyzer",
        "doi": "10.1002/jcc.22885",
        "year": 2011,
        "journal": "Journal of Computational Chemistry",
        "abstract": "Multiwfn is a multifunctional program for wavefunction analysis. Its main functions are: (1) Calculating and visualizing real space function, such as electrostatic potential and electron localization function at point, in a line, in a plane or in a spatial scope. (2) Population analysis. (3) Bond order analysis. (4) Orbital composition analysis. (5) Plot density-of-states and spectrum. (6) Topology analysis for electron density. Some other useful utilities involved in quantum chemistry studies are also provided. The built-in <strong class=\"highlight\">graph</strong> module enables the results of wavefunction analysis to be plotted directly or exported to high-quality graphic file. The program interface is very user-friendly and suitable for both research and teaching purpose. The code of Multiwfn is substantially optimized and parallelized. Its efficiency is demonstrated to be significantly higher than related programs with the same functions. Five practical examples involving a wide variety of systems and analysis methods are given to illustrate the usefulness of Multiwfn. The program is free of charge and open-source. Its precompiled file and source codes are available from http://multiwfn.codeplex.com.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 19224,
          "supporting": 218,
          "contradicting": 4,
          "mentioning": 18933,
          "unclassified": 69,
          "citingPublications": 33528
        }
      },
      {
        "title": "Statistical mechanics of complex networks",
        "doi": "10.1103/revmodphys.74.47",
        "year": 2002,
        "journal": "Reviews of Modern Physics",
        "abstract": "Complex networks describe a wide range of systems in nature and society, much\nquoted examples including the cell, a network of chemicals linked by chemical\nreactions, or the Internet, a network of routers and computers connected by\nphysical links. While traditionally these systems were modeled as random\n<strong class=\"highlight\">graphs</strong>, it is increasingly recognized that the topology and evolution of real\nnetworks is governed by robust organizing principles. Here we review the recent\nadvances in the field of complex networks, focusing on the statistical\nmechanics of network topology and dynamics. After reviewing the empirical data\nthat motivated the recent interest in networks, we discuss the main models and\nanalytical tools, covering random <strong class=\"highlight\">graphs</strong>, small-world and scale-free networks,\nas well as the interplay between topology and the network's robustness against\nfailures and attacks.Comment: 54 pages, submitted to Reviews of Modern Physic",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16007,
          "supporting": 231,
          "contradicting": 20,
          "mentioning": 15477,
          "unclassified": 279,
          "citingPublications": 19423
        }
      },
      {
        "title": "Fast unfolding of communities in large networks",
        "doi": "10.1088/1742-5468/2008/10/p10008",
        "year": 2008,
        "journal": "Journal of Statistical Mechanics <strong Class=\"highlight\">theory</Strong> and Experiment",
        "abstract": "Abstract. We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web <strong class=\"highlight\">graph</strong> of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 14072,
          "supporting": 61,
          "contradicting": 3,
          "mentioning": 13655,
          "unclassified": 353,
          "citingPublications": 18912
        }
      },
      {
        "title": "MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification",
        "doi": "10.1038/nbt.1511",
        "year": 2008,
        "journal": "Nature Biotechnology",
        "abstract": "Efficient analysis of very large amounts of raw data for peptide identification and protein quantification is a principal challenge in mass spectrometry (MS)-based proteomics. Here we describe MaxQuant, an integrated suite of algorithms specifically developed for high-resolution, quantitative MS data. Using correlation analysis and <strong class=\"highlight\">graph</strong> <strong class=\"highlight\">theory</strong>, MaxQuant detects peaks, isotope clusters and stable amino acid isotope-labeled (SILAC) peptide pairs as three-dimensional objects in m/z, elution time and signal intensity space. By integrating multiple mass measurements and correcting for linear and nonlinear mass offsets, we achieve mass accuracy in the p.p.b. range, a sixfold increase over standard techniques. We increase the proportion of identified fragmentation spectra to 73% for SILAC peptide pairs via unambiguous assignment of isotope and missed-cleavage state and individual mass precision. MaxQuant automatically quantifies several hundred thousand peptides per SILAC-proteome experiment and allows statistically robust identification and quantification of >4,000 proteins in mammalian cell lysates.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 13018,
          "supporting": 23,
          "contradicting": 4,
          "mentioning": 12957,
          "unclassified": 34,
          "citingPublications": 14613
        }
      },
      {
        "title": "The Structure and Function of Complex Networks",
        "doi": "10.1137/s003614450342480",
        "year": 2003,
        "journal": "Siam Review",
        "abstract": "Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random <strong class=\"highlight\">graph</strong> models, models of network growth and preferential attachment, and dynamical processes taking place on networks.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12904,
          "supporting": 148,
          "contradicting": 11,
          "mentioning": 12318,
          "unclassified": 427,
          "citingPublications": 17092
        }
      },
      {
        "title": "Accurate spin-dependent electron liquid correlation energies for local spin density calculations: a critical analysis",
        "doi": "10.1139/p80-159",
        "year": 1980,
        "journal": "Canadian Journal of Physics",
        "abstract": "We assess various approximate forms for the correlation energy per particle of the spinpolarized homogeneous electron gas that have frequently been used in applications of the local spin density approximation to the exchange-correlation energy functional. By accurately recalculating the RPA correlation energy as a function of electron density and spin polarization we demonstrate the inadequacies of the usual approximation for interpolating between the para-and ferro-magnetic states and present an accurate new interpolation formula. A Pade approximant technique is used to accurately interpolate the recent Monte Carlo results (para and ferro) of Ceperley and Alder into the important range of densities for atoms, molecules, and metals. These results can be combined with the RPA spin-dependence so as to produce a correlation energy fora spin-polarized homogeneous electron gas with an estimated maximum error of I mRy and thus should reliably determine the magnitude of non-local corrections to the local spin density approximation in real systems.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12309,
          "supporting": 94,
          "contradicting": 3,
          "mentioning": 12096,
          "unclassified": 116,
          "citingPublications": 20676
        }
      }
    ]
  },
  "Game Theory": {
    "query": "Game Theory",
    "count": 10,
    "papers": [
      {
        "title": "Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology",
        "doi": "10.2307/249008",
        "year": 1989,
        "journal": "Mis Quarterly",
        "abstract": "This paper aims to study the factors that enhance perception of mobile commerce users to promote such form of shopping in Egypt. Researchers explored the factors affecting users of Mobile communication and electronic shopping by identifying all factors considered in technology users' behaviour <strong class=\"highlight\">theories</strong>. Researchers conducted an exploratory research, to examine the most relevant factors for mobile commerce adoption, also, tested the reliable and valid measures extracted, for considering its effect on the user's perception of its usefulness and ease of use. Three factors were considered of main importance: social influence, convenience and hedonic motivations. Researchers tested those factors and they all affected User's perception, further They affected each other. Thus, researchers concluded that, in Mobile commerce, Social influence were fully mediated with hedonic motivation and convenience. As of Convenience, it tends to be the most affecting for perceptions of consumers usefulness and ease of use also, it has a strong mediation effect between social influence and mobile commerce user's perceptions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 51456,
          "supporting": 1707,
          "contradicting": 149,
          "mentioning": 45367,
          "unclassified": 4233,
          "citingPublications": 50563
        }
      },
      {
        "title": "Generative adversarial networks",
        "doi": "10.1145/3422622",
        "year": 2020,
        "journal": "Communications of the Acm",
        "abstract": "Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the\n            generative modeling\n            problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on <strong class=\"highlight\">game</strong> <strong class=\"highlight\">theory</strong> while most other approaches to generative modeling are based on optimization.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 32589,
          "supporting": 44,
          "contradicting": 1,
          "mentioning": 32346,
          "unclassified": 198,
          "citingPublications": 28740
        }
      },
      {
        "title": "Intrinsic Motivation and Self-Determination in Human Behavior",
        "doi": "10.1007/978-1-4899-2271-7",
        "year": 1985,
        "journal": "",
        "abstract": "A Continuation Order Plan is available for this series. A continuation order will bring delivery of each new volume immediately upon publication. Volumes are billed only upon actual shipment. For further information please contact the publisher.Originally published by Plenum Press, New York in 1985 Softcover reprint of the hardcover 1st edition 1985\nAll rights reservedNo part of this book may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying, microfilming, recording, or otherwise, without written permission from the Publisher To Our Parents:",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 22936,
          "supporting": 767,
          "contradicting": 52,
          "mentioning": 19636,
          "unclassified": 2481,
          "citingPublications": 24009
        }
      },
      {
        "title": "Human-level control through deep reinforcement learning",
        "doi": "10.1038/nature14236",
        "year": 2015,
        "journal": "Nature",
        "abstract": "The <strong class=\"highlight\">theory</strong> of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 <strong class=\"highlight\">games</strong>. We demonstrate that the deep Q-network agent, receiving only the pixels and the <strong class=\"highlight\">game</strong> score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human <strong class=\"highlight\">games</strong> tester across a set of 49 <strong class=\"highlight\">games</strong>, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 18351,
          "supporting": 35,
          "contradicting": 4,
          "mentioning": 18196,
          "unclassified": 116,
          "citingPublications": 27157
        }
      },
      {
        "title": "Statistical mechanics of complex networks",
        "doi": "10.1103/revmodphys.74.47",
        "year": 2002,
        "journal": "Reviews of Modern Physics",
        "abstract": "Complex networks describe a wide range of systems in nature and society, much\nquoted examples including the cell, a network of chemicals linked by chemical\nreactions, or the Internet, a network of routers and computers connected by\nphysical links. While traditionally these systems were modeled as random\ngraphs, it is increasingly recognized that the topology and evolution of real\nnetworks is governed by robust organizing principles. Here we review the recent\nadvances in the field of complex networks, focusing on the statistical\nmechanics of network topology and dynamics. After reviewing the empirical data\nthat motivated the recent interest in networks, we discuss the main models and\nanalytical tools, covering random graphs, small-world and scale-free networks,\nas well as the interplay between topology and the network's robustness against\nfailures and attacks.Comment: 54 pages, submitted to Reviews of Modern Physic",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 16007,
          "supporting": 231,
          "contradicting": 20,
          "mentioning": 15477,
          "unclassified": 279,
          "citingPublications": 19423
        }
      },
      {
        "title": "The qualitative content analysis process",
        "doi": "10.1111/j.1365-2648.2007.04569.x",
        "year": 2008,
        "journal": "Journal of Advanced Nursing",
        "abstract": "Inductive content analysis is used in cases where there are no previous studies dealing with the phenomenon or when it is fragmented. A deductive approach is useful if the general aim was to test a previous <strong class=\"highlight\">theory</strong> in a different situation or to compare categories at different time periods.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 14376,
          "supporting": 25,
          "contradicting": 3,
          "mentioning": 13934,
          "unclassified": 414,
          "citingPublications": 18003
        }
      },
      {
        "title": "The need to belong: Desire for interpersonal attachments as a fundamental human motivation.",
        "doi": "10.1037/0033-2909.117.3.497",
        "year": 1995,
        "journal": "Psychological Bulletin",
        "abstract": "A hypothesized need to form and maintain strong, stable interpersonal relationships is evaluated in light of the empirical literature. The need is for frequent, nonaversive interactions within an ongoing relational bond. Consistent with the belongingness hypothesis, people form social attachments readily under most conditions and resist the dissolution of existing bonds. Belongingness appears to have multiple and strong effects on emotional patterns and on cognitive processes. Lack of attachments is linked to a variety of ill effects on health, adjustment, and well-being. Other evidence, such as that concerning satiation, substitution, and behavioral consequences, is likewise consistent with the hypothesized motivation. Several seeming counterexamples turned out not to disconfirm the hypothesis. Existing evidence supports the hypothesis that the need to belong is a powerful, fundamental, and extremely pervasive motivation.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 13585,
          "supporting": 447,
          "contradicting": 19,
          "mentioning": 12681,
          "unclassified": 438,
          "citingPublications": 18235
        }
      },
      {
        "title": "The Structure and Function of Complex Networks",
        "doi": "10.1137/s003614450342480",
        "year": 2003,
        "journal": "Siam Review",
        "abstract": "Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 12904,
          "supporting": 148,
          "contradicting": 11,
          "mentioning": 12318,
          "unclassified": 427,
          "citingPublications": 17092
        }
      },
      {
        "title": "Adaptation in Natural and Artificial Systems",
        "doi": "10.7551/mitpress/1090.001.0001",
        "year": 1992,
        "journal": "",
        "abstract": "Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic <strong class=\"highlight\">theory</strong> to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.\n               In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, <strong class=\"highlight\">game</strong> <strong class=\"highlight\">theory</strong>, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.\n               Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements.\n               Bradford Books imprint",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 11316,
          "supporting": 12,
          "contradicting": 1,
          "mentioning": 10661,
          "unclassified": 642,
          "citingPublications": 20531
        }
      },
      {
        "title": "Executive Functions",
        "doi": "10.1146/annurev-psych-113011-143750",
        "year": 2013,
        "journal": "Annual Review of Psychology",
        "abstract": "Executive functions (EFs) make possible mentally playing with ideas; taking the time to think before acting; meeting novel, unanticipated challenges; resisting temptations; and staying focused. Core EFs are inhibition [response inhibition (self-control—resisting temptations and resisting acting impulsively) and interference control (selective attention and cognitive inhibition)], working memory, and cognitive flexibility (including creatively thinking “outside the box,” seeing anything from different perspectives, and quickly and flexibly adapting to changed circumstances). The developmental progression and representative measures of each are discussed. Controversies are addressed (e.g., the relation between EFs and fluid intelligence, self-regulation, executive attention, and effortful control, and the relation between working memory and inhibition and attention). The importance of social, emotional, and physical health for cognitive health is discussed because stress, lack of sleep, loneliness, or lack of exercise each impair EFs. That EFs are trainable and can be improved with practice is addressed, including diverse methods tried thus far.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 10995,
          "supporting": 213,
          "contradicting": 26,
          "mentioning": 9916,
          "unclassified": 840,
          "citingPublications": 10918
        }
      }
    ]
  },
  "Deseasonalization": {
    "query": "Deseasonalization",
    "count": 10,
    "papers": [
      {
        "title": "ADVANCED SPECTRAL METHODS FOR CLIMATIC TIME SERIES",
        "doi": "10.1029/2000rg000092",
        "year": 2002,
        "journal": "Reviews of Geophysics",
        "abstract": "[1] The analysis of univariate or multivariate time series provides crucial information to describe, understand, and predict climatic variability. The discovery and implementation of a number of novel methods for extracting useful information from time series has recently revitalized this classical field of study. Considerable progress has also been made in interpreting the information so obtained in terms of dynamical systems theory. In this review we describe the connections between time series analysis and nonlinear dynamics, discuss signal-to-noise enhancement, and present some of the novel methods for spectral analysis. The various steps, as well as the advantages and disadvantages of these methods, are illustrated by their application to an important climatic time series, the Southern Oscillation Index. This index captures major features of interannual climate variability and is used extensively in its prediction. Regional and global sea surface temperature data sets are used to illustrate multivariate spectral methods. Open questions and further prospects conclude the review.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1892,
          "supporting": 13,
          "contradicting": 0,
          "mentioning": 1856,
          "unclassified": 23,
          "citingPublications": 2134
        }
      },
      {
        "title": "The Global Methane Budget 2000–2017",
        "doi": "10.5194/essd-12-1561-2020",
        "year": 2020,
        "journal": "Earth System Science Data",
        "abstract": "Abstract. Understanding and quantifying the global methane (CH4) budget\nis important for assessing realistic pathways to mitigate climate change.\nAtmospheric emissions and concentrations of CH4 continue to increase,\nmaking CH4 the second most important human-influenced greenhouse gas in\nterms of climate forcing, after carbon dioxide (CO2). The relative\nimportance of CH4 compared to CO2 depends on its shorter\natmospheric lifetime, stronger warming potential, and variations in\natmospheric growth rate over the past decade, the causes of which are still\ndebated. Two major challenges in reducing uncertainties in the atmospheric\ngrowth rate arise from the variety of geographically overlapping CH4\nsources and from the destruction of CH4 by short-lived hydroxyl\nradicals (OH). To address these challenges, we have established a\nconsortium of multidisciplinary scientists under the umbrella of the Global\nCarbon Project to synthesize and stimulate new research aimed at improving\nand regularly updating the global methane budget. Following Saunois et al. (2016), we present here the second version of the living review paper\ndedicated to the decadal methane budget, integrating results of top-down\nstudies (atmospheric observations within an atmospheric inverse-modelling\nframework) and bottom-up estimates (including process-based models for\nestimating land surface emissions and atmospheric chemistry, inventories of\nanthropogenic emissions, and data-driven extrapolations). For the 2008–2017 decade, global methane emissions are estimated by\natmospheric inversions (a top-down approach) to be 576 Tg CH4 yr−1 (range 550–594, corresponding to the minimum and maximum\nestimates of the model ensemble). Of this total, 359 Tg CH4 yr−1 or\n∼ 60 % is attributed to anthropogenic sources, that is\nemissions caused by direct human activity (i.e. anthropogenic emissions; range 336–376 Tg CH4 yr−1 or 50 %–65 %). The mean annual total emission for the new decade (2008–2017) is\n29 Tg CH4 yr−1 larger than our estimate for the previous decade (2000–2009),\nand 24 Tg CH4 yr−1 larger than the one reported in the previous\nbudget for 2003–2012 (Saunois et al., 2016). Since 2012, global CH4\nemissions have been tracking the warmest scenarios assessed by the\nIntergovernmental Panel on Climate Change. Bottom-up methods suggest almost\n30 % larger global emissions (737 Tg CH4 yr−1, range 594–881)\nthan top-down inversion methods. Indeed, bottom-up estimates for natural\nsources such as natural wetlands, other inland water systems, and geological\nsources are higher than top-down estimates. The atmospheric constraints on\nthe top-down budget suggest that at least some of these bottom-up emissions\nare overestimated. The latitudinal distribution of atmospheric\nobservation-based emissions indicates a predominance of tropical emissions\n(∼ 65 % of the global budget, &lt; 30∘ N)\ncompared to mid-latitudes (∼ 30 %, 30–60∘ N)\nand high northern latitudes (∼ 4 %, 60–90∘ N). The most important source of uncertainty in the methane\nbudget is attributable to natural emissions, especially those from wetlands\nand other inland waters. Some of our global source estimates are smaller than those in previously\npublished budgets (Saunois et al., 2016; Kirschke et al., 2013). In particular wetland emissions are about 35 Tg CH4 yr−1 lower due to\nimproved partition wetlands and other inland waters. Emissions from\ngeological sources and wild animals are also found to be smaller by 7 Tg CH4 yr−1 by 8 Tg CH4 yr−1, respectively. However, the overall\ndiscrepancy between bottom-up and top-down estimates has been reduced by\nonly 5 % compared to Saunois et al. (2016), due to a higher estimate of emissions from inland waters, highlighting the need for more detailed research on emissions factors. Priorities for improving the methane\nbudget include (i) a global, high-resolution map of water-saturated soils\nand inundated areas emitting methane based on a robust classification of\ndifferent types of emitting habitats; (ii) further development of\nprocess-based models for inland-water emissions; (iii) intensification of\nmethane observations at local scales (e.g., FLUXNET-CH4 measurements)\nand urban-scale monitoring to constrain bottom-up land surface models, and\nat regional scales (surface networks and satellites) to constrain\natmospheric inversions; (iv) improvements of transport models and the\nrepresentation of photochemical sinks in top-down inversions; and (v) development of a 3D variational inversion system using isotopic and/or\nco-emitted species such as ethane to improve source partitioning. The data presented here can be downloaded from\nhttps://doi.org/10.18160/GCP-CH4-2019 (Saunois et al., 2020) and from the\nGlobal Carbon Project.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1752,
          "supporting": 59,
          "contradicting": 13,
          "mentioning": 1664,
          "unclassified": 16,
          "citingPublications": 2245
        }
      },
      {
        "title": "On the Relation between the Expected Value and the Volatility of the Nominal Excess Return on Stocks",
        "doi": "10.1111/j.1540-6261.1993.tb05128.x",
        "year": 1993,
        "journal": "The Journal of Finance",
        "abstract": "ABSTRACTWe find support for a negative relation between conditional expected monthly return and conditional variance of monthly return, using a GARCH-M model modified by allowing (1) seasonal patterns in volatility, (2) positive and negative innovations to returns having different impacts on conditional volatility, and (3) nominal interest rates to predict conditional variance. Using the modified GARCH-M model, we also show that monthly conditional volatility may not be as persistent as was thought. Positive unanticipated returns appear to result in a downward revision of the conditional volatility whereas negative unanticipated returns result in an upward revision of conditional volatility. There is general agreement that investors, within a given time period, require a larger expected return from a security that is riskier. However, there is no such agreement about the relation between risk and return across time. Whether or not investors require a larger risk premium on average for investing in a security during times when the security is more risky remains an open question. At first blush, it may appear that rational risk-averse  Most of the support for a zero or positive relation has come from studies that use the standard GARCH-M model of stochastic volatility.2 Other studies, using alternative techniques, have documented a negative relation between expected return and conditional variance. In order to resolve this conflict we examine the possibility that the standard GARCH-M model may not be rich enough to capture the time series properties of the monthly excess return on stocks. We consider a more general specification of the GARCH-M model. In particular, (1) we incorporate dummy variables in the GARCH-M model to capture seasonal effects using the procedure first suggested by Glosten, Jagannathan, and Runkle ( \nTHE TRADEOFF BETWEEN RISK and return has long been an important topic in\nII. Estimating the Model\nA. Econometric IssuesThe parameter p in the model given by (2) cannot be estimated without specifying how variances change over time, since Var(xt + 1? Ft) is not directly observed by the econometrician. To appreciate the difficulties involved, project both sides of (2) on Gt, the econometrician's information set, which is a strict subset of the agents' information set Ft. The term on the left in equation (4) Since the estimated slope coefficient cl is a consistent estimate of /3 bl, and d1 provides a consistent estimate of b1, the ratio of any two corresponding elements of cl and d1 provides a consistent estimate of /8. If zt-, is not a scalar, then we may impose the constraint that the slope coefficients in (5) and the slope coefficients in (6) differ only by the scale factor, /3. Such a restriction also provides a natural test for the validity of the model specification. We call this approach Campbell's Instrumental Variable Model. Another approach, the GARCH-M model, assumes that Var(vtllGt_-) is identically zero, and that zt-1 consists of innovations and variances that, while unobservable, can be...",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 1286,
          "supporting": 36,
          "contradicting": 3,
          "mentioning": 1223,
          "unclassified": 24,
          "citingPublications": 6733
        }
      },
      {
        "title": "Methane emissions from wetlands: biogeochemical, microbial, and modeling perspectives from local to global scales",
        "doi": "10.1111/gcb.12131",
        "year": 2013,
        "journal": "Global Change Biology",
        "abstract": "Understanding the dynamics of methane (CH4 ) emissions is of paramount importance because CH4 has 25 times the global warming potential of carbon dioxide (CO2 ) and is currently the second most important anthropogenic greenhouse gas. Wetlands are the single largest natural CH4 source with median emissions from published studies of 164 Tg yr(-1) , which is about a third of total global emissions. We provide a perspective on important new frontiers in obtaining a better understanding of CH4 dynamics in natural systems, with a focus on wetlands. One of the most exciting recent developments in this field is the attempt to integrate the different methodologies and spatial scales of biogeochemistry, molecular microbiology, and modeling, and thus this is a major focus of this review. Our specific objectives are to provide an up-to-date synthesis of estimates of global CH4 emissions from wetlands and other freshwater aquatic ecosystems, briefly summarize major biogeophysical controls over CH4 emissions from wetlands, suggest new frontiers in CH4 biogeochemistry, examine relationships between methanogen community structure and CH4 dynamics in situ, and to review the current generation of CH4 models. We highlight throughout some of the most pressing issues concerning global change and feedbacks on CH4 emissions from natural ecosystems. Major uncertainties in estimating current and future CH4 emissions from natural ecosystems include the following: (i) A number of important controls over CH4 production, consumption, and transport have not been, or are inadequately, incorporated into existing CH4 biogeochemistry models. (ii) Significant errors in regional and global emission estimates are derived from large spatial-scale extrapolations from highly heterogeneous and often poorly mapped wetland complexes. (iii) The limited number of observations of CH4 fluxes and their associated environmental variables loosely constrains the parameterization of process-based biogeochemistry models.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 996,
          "supporting": 24,
          "contradicting": 4,
          "mentioning": 962,
          "unclassified": 6,
          "citingPublications": 1104
        }
      },
      {
        "title": "A Nonparametric Trend Test for Seasonal Data With Serial Dependence",
        "doi": "10.1029/wr020i006p00727",
        "year": 1984,
        "journal": "Water Resources Research",
        "abstract": "Statistical tests for monotonic trend in seasonal (e.g., monthly) hydrologic time series are commonly confounded by some of the following problems: nonnormal data, missing values, seasonality, censoring (detection limits), and serial dependence. An extension of the Mann-Kendall test for trend (designed for such data) is presented here. Because the test is based entirely on ranks, it is robust against nonnormality and censoring. Seasonality and missing values present no theoretical or computational obstacles to its application. Monte Carlo experiments show that, in terms of type I error, it is robust against serial  correlation except when the data have strong long-term persistence (e.g., ARMA (1, 1) monthly processes with 4• &gt; 0.6) or short records (~ 5 years). When there is no serial correlation, it is less powerful than a related simpler test which is not robust against serial correlation.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 887,
          "supporting": 8,
          "contradicting": 1,
          "mentioning": 863,
          "unclassified": 15,
          "citingPublications": 1536
        }
      },
      {
        "title": "The global methane budget 2000–2012",
        "doi": "10.5194/essd-8-697-2016",
        "year": 2016,
        "journal": "Earth System Science Data",
        "abstract": "Abstract. The global methane (CH 4 ) budget is becoming an increasingly important component for managing realistic pathways to mitigate climate change. This relevance, due to a shorter atmospheric lifetime and a stronger warming potential than carbon dioxide, is challenged by the still unexplained changes of atmospheric CH 4 over the past decade. Emissions and concentrations of CH 4 are continuing to increase, making CH 4 the second most important human-induced greenhouse gas after carbon dioxide. Two major difficulties in reducing uncertainties come from the large variety of diffusive CH 4 sources that overlap geographically, and from the destruction of CH 4 by the very short-lived hydroxyl radical (OH). To address these difficulties, we have established a consortium of multi-disciplinary scientists under the umbrella of the Global Carbon Project to synthesize and stimulate research on the methane cycle, and producing regular (∼ biennial) updates of the global methane budget. This consortium includes atmospheric physicists and chemists, biogeochemists of surface and marine emissions, and socio-economists who study anthropogenic emissions. Following Kirschke et al. (2013), we propose here the first version of a living review paper that integrates results of top-down studies (exploiting atmospheric observations within an atmospheric inverse-modelling framework) and bottom-up models, inventories and data-driven approaches (including process-based models for estimating land surface emissions and atmospheric chemistry, and inventories for anthropogenic emissions, data-driven extrapolations). . Top-down inversions consistently infer lower emissions in China (∼ 58 Tg CH 4 yr −1 , range 51-72, −14 %) and higher emissions in Africa (86 Tg CH 4 yr −1 , range 73-108, +19 %) than bottom-up values used as prior estimates. Overall, uncertainties for anthropogenic emissions appear smaller than those from natural sources, and the uncertainties on source categories appear larger for top-down inversions than for bottom-up inventories and models.The most important source of uncertainty on the methane budget is attributable to emissions from wetland and other inland waters. We show that the wetland extent could contribute 30-40 % on the estimated range for wetland emissions. Other priorities for improving the methane budget include the following: (i) the development of process-based models for inland-water emissions, (ii) the intensification of methane observations at local scale (flux measurements) to constrain bottom-up land surface models, and at regional scale (surface networks and satellites) to constrain top-down inversions, (iii) improvements in the estimation of atmospheric loss by OH, and (iv) improvements of the transport models integrated in top-down inversions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 869,
          "supporting": 40,
          "contradicting": 9,
          "mentioning": 817,
          "unclassified": 3,
          "citingPublications": 1097
        }
      },
      {
        "title": "Modeling and Forecasting Realized Volatility",
        "doi": "10.1111/1468-0262.00418",
        "year": 2003,
        "journal": "Econometrica",
        "abstract": "This paper provides a general framework for integration of high-frequency intraday data into the measurement, modeling, and forecasting of daily and lower frequency volatility and return distributions. Most procedures for modeling and forecasting financial asset return volatilities, correlations, and distributions rely on restrictive and complicated parametric multivariate ARCH or stochastic volatility models, which often perform poorly at intraday frequencies. Use of realized volatility constructed from high-frequency intraday returns, in contrast, permits the use of traditional time series procedures for modeling and forecasting. Building on the theory of continuous-time arbitrage-free price processes and the theory of quadratic variation, we formally develop the links between the conditional covariance matrix and the concept of realized volatility. Next, using continuously recorded observations for the Deutschemark / Dollar and Yen / Dollar spot exchange rates covering more than a decade, we find that forecasts from a simple long-memory Gaussian vector autoregression for the logarithmic daily realized volatilities perform admirably compared to popular daily ARCH and related models. Moreover, the vector autoregressive volatility forecast, coupled with a parametric lognormal-normal mixture distribution implied by the theoretically and empirically grounded assumption of normally distributed standardized returns, gives rise to well-calibrated density forecasts of future returns, and correspondingly accurate quantile estimates. Our results hold promise for practical modeling and forecasting of the large covariance matrices relevant in asset pricing, asset allocation and financial risk management applications.KEYWORDS: Continuous-time methods, quadratic variation, realized volatility, realized correlation, highfrequency data, exchange rates, vector autoregression, long memory, volatility forecasting, correlation forecasting, density forecasting, risk management, value at risk. _________________ * This paper supercedes the earlier manuscript \"Forecasting Volatility: A VAR for VaR.\" The work reported in the paper was supported by the National Science Foundation. We are grateful to Olsen and Associates, who generously made available their intraday exchange rate quotation data. For insightful suggestions and comments we thank Rob Engle, Atsushi Inoue, Neil Shephard, Clara Vega, Sean Campbell, and seminar participants at Chicago, Michigan, Montreal/CIRANO, NYU, Rice, and the June 2000 Meeting of the Western Finance Association. and covariances (or correlations). When important, the precise meaning will be clear from context.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 843,
          "supporting": 14,
          "contradicting": 1,
          "mentioning": 815,
          "unclassified": 13,
          "citingPublications": 3392
        }
      },
      {
        "title": "Recent variability of the global ocean carbon sink",
        "doi": "10.1002/2014gb004853",
        "year": 2014,
        "journal": "Global Biogeochemical Cycles",
        "abstract": "We present a new observation-based estimate of the global oceanic carbon dioxide (CO2) sink and its temporal variation on a monthly basis from 1998 through 2011 and at a spatial resolution of 1×1. This sink estimate rests upon a neural network-based mapping of global surface ocean observations of the partial pressure of CO2 (pCO2) from the Surface Ocean CO2 Atlas database. The resulting pCO2 has small biases when evaluated against independent observations in the different ocean basins, but larger randomly distributed differences exist particularly in high latitudes. The seasonal climatology of our neural network-based product agrees overall well with the Takahashi et al. (2009) climatology, although our product produces a stronger seasonal cycle at high latitudes. From our global pCO2 product, we compute a mean net global ocean (excluding the Arctic Ocean and coastal regions) CO2 uptake flux of −1.42 ± 0.53 Pg C yr−1, which is in good agreement with ocean inversion-based estimates. Our data indicate a moderate level of interannual variability in the ocean carbon sink (±0.12 Pg C yr−1, 1&#x1d70e;) from 1998 through 2011, mostly originating from the equatorial Pacific Ocean, and associated with the El Nino–Southern Oscillation. Accounting for steady state riverine and Arctic Ocean carbon fluxes our estimate further implies a mean anthropogenic CO2 uptake of −1.99 ± 0.59 Pg C yr−1 over the analysis period. From this estimate plus the most recent estimates for fossil fuel emissions and atmospheric CO2 accumulation, we infer a mean global land sink of −2.82 ± 0.85 Pg C yr−1 over the 1998 through 2011 period with strong interannual variation",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 776,
          "supporting": 66,
          "contradicting": 7,
          "mentioning": 700,
          "unclassified": 3,
          "citingPublications": 576
        }
      },
      {
        "title": "Statistical and Machine Learning forecasting methods: Concerns and ways forward",
        "doi": "10.1371/journal.pone.0194889",
        "year": 2018,
        "journal": "Plos One",
        "abstract": "Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 642,
          "supporting": 26,
          "contradicting": 2,
          "mentioning": 598,
          "unclassified": 16,
          "citingPublications": 1147
        }
      },
      {
        "title": "Good Day Sunshine: Stock Returns and the Weather",
        "doi": "10.1111/1540-6261.00556",
        "year": 2003,
        "journal": "The Journal of Finance",
        "abstract": "Psychological evidence and casual intuition predict that sunny weather is associated with upbeat mood. This paper examines the relationship between morning sunshine in the city of a country's leading stock exchange and daily market index returns across 26 countries from 1982 to 1997. Sunshine is strongly signi¢cantly correlated with stock returns. After controlling for sunshine, rain and snow are unrelated to returns. Substantial use of weatherbased strategies was optimal for a trader with very low transactions costs. However, because these strategies involve frequent trades, fairly modest costs eliminate the gains.These ¢ndings are di⁄cult to reconcile with fully rational price setting. SUNSHINE AFFECTS MOOD, as evidenced by song and verse, daily experience, and formal psychological studies. But does sunlight a¡ect the stock market?The traditional e⁄cient markets view says no, with minor quali¢cations. If sunlight a¡ects the weather, it can a¡ect agricultural and perhaps other weatherrelated ¢rms. But in modern economies in which agriculture plays a modest role, it seems unlikely that whether it is cloudy outside the stock exchange today should a¡ect the rational price of the nation's stock market index. (Even in countries where agriculture plays a large role, it is not clear that one day of sunshine versus cloud cover at the stock exchange should be very informative about harvest yield.)An alternative view is that sunlight a¡ects mood, and that people tend to evaluate future prospects more optimistically when they are in a good mood than when they are in a bad mood. A literature in psychology has found that mood a¡ects judgment and behavior. The psychological literature on sunlight, mood, and misattribution of mood is discussed in the next section. An important strand of this literature has provided evidence that mood contains valuable information",
        "keywords": [],
        "topics": [],
        "source": "",
        "scite_score": null,
        "retracted": false,
        "editorialNotices": [],
        "snippets": [],
        "num_snippets": 0,
        "num_keywords": 0,
        "num_topics": 0,
        "citations": {
          "total": 618,
          "supporting": 42,
          "contradicting": 6,
          "mentioning": 562,
          "unclassified": 8,
          "citingPublications": 1572
        }
      }
    ]
  }
}